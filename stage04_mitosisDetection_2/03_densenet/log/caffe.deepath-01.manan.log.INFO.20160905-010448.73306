Log file created at: 2016/09/05 01:04:48
Running on machine: deepath-01
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0905 01:04:48.848081 73306 caffe.cpp:217] Using GPUs 3
I0905 01:04:48.871949 73306 caffe.cpp:222] GPU 3: Tesla K80
I0905 01:04:49.244349 73306 solver.cpp:48] Initializing solver from parameters: 
test_iter: 200
test_interval: 800
base_lr: 0.1
display: 10
max_iter: 100000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
solver_mode: GPU
device_id: 3
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 50000
stepvalue: 75000
type: "Nesterov"
I0905 01:04:49.244473 73306 solver.cpp:91] Creating training net from net file: train_val.prototxt
I0905 01:04:49.247905 73306 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer Data1
I0905 01:04:49.248023 73306 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer Accuracy1
I0905 01:04:49.248863 73306 net.cpp:58] Initializing net from parameters: 
name: "DenseNN"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "ImageData"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 150
    mean_value: 150
    mean_value: 150
  }
  image_data_param {
    source: "../list_bias-1_tr.lst"
    batch_size: 16
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout1"
  type: "Dropout"
  bottom: "Convolution1"
  top: "Dropout1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Dropout1"
  top: "BatchNorm1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "BatchNorm1"
  top: "Convolution2"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout2"
  type: "Dropout"
  bottom: "Convolution2"
  top: "Dropout2"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "Dropout1"
  bottom: "Dropout2"
  top: "Concat1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Concat1"
  top: "BatchNorm2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "BatchNorm2"
  top: "Convolution3"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout3"
  type: "Dropout"
  bottom: "Convolution3"
  top: "Dropout3"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat2"
  type: "Concat"
  bottom: "Concat1"
  bottom: "Dropout3"
  top: "Concat2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Concat2"
  top: "BatchNorm3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "BatchNorm3"
  top: "Convolution4"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout4"
  type: "Dropout"
  bottom: "Convolution4"
  top: "Dropout4"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat3"
  type: "Concat"
  bottom: "Concat2"
  bottom: "Dropout4"
  top: "Concat3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Concat3"
  top: "BatchNorm4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "BatchNorm4"
  top: "BatchNorm4"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "BatchNorm4"
  top: "BatchNorm4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "BatchNorm4"
  top: "Convolution5"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout5"
  type: "Dropout"
  bottom: "Convolution5"
  top: "Dropout5"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat4"
  type: "Concat"
  bottom: "Concat3"
  bottom: "Dropout5"
  top: "Concat4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Concat4"
  top: "BatchNorm5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "BatchNorm5"
  top: "BatchNorm5"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "BatchNorm5"
  top: "BatchNorm5"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "BatchNorm5"
  top: "Convolution6"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout6"
  type: "Dropout"
  bottom: "Convolution6"
  top: "Dropout6"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat5"
  type: "Concat"
  bottom: "Concat4"
  bottom: "Dropout6"
  top: "Concat5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Concat5"
  top: "BatchNorm6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "BatchNorm6"
  top: "BatchNorm6"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "BatchNorm6"
  top: "BatchNorm6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "BatchNorm6"
  top: "Convolution7"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout7"
  type: "Dropout"
  bottom: "Convolution7"
  top: "Dropout7"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat6"
  type: "Concat"
  bottom: "Concat5"
  bottom: "Dropout7"
  top: "Concat6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Concat6"
  top: "BatchNorm7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "BatchNorm7"
  top: "BatchNorm7"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "BatchNorm7"
  top: "BatchNorm7"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "BatchNorm7"
  top: "Convolution8"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout8"
  type: "Dropout"
  bottom: "Convolution8"
  top: "Dropout8"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat7"
  type: "Concat"
  bottom: "Concat6"
  bottom: "Dropout8"
  top: "Concat7"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Concat7"
  top: "BatchNorm8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "BatchNorm8"
  top: "BatchNorm8"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "BatchNorm8"
  top: "BatchNorm8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "BatchNorm8"
  top: "Convolution9"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout9"
  type: "Dropout"
  bottom: "Convolution9"
  top: "Dropout9"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat8"
  type: "Concat"
  bottom: "Concat7"
  bottom: "Dropout9"
  top: "Concat8"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Concat8"
  top: "BatchNorm9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "BatchNorm9"
  top: "BatchNorm9"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "BatchNorm9"
  top: "BatchNorm9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "BatchNorm9"
  top: "Convolution10"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout10"
  type: "Dropout"
  bottom: "Convolution10"
  top: "Dropout10"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat9"
  type: "Concat"
  bottom: "Concat8"
  bottom: "Dropout10"
  top: "Concat9"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Concat9"
  top: "BatchNorm10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "BatchNorm10"
  top: "BatchNorm10"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "BatchNorm10"
  top: "BatchNorm10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "BatchNorm10"
  top: "Convolution11"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout11"
  type: "Dropout"
  bottom: "Convolution11"
  top: "Dropout11"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat10"
  type: "Concat"
  bottom: "Concat9"
  bottom: "Dropout11"
  top: "Concat10"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Concat10"
  top: "BatchNorm11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "BatchNorm11"
  top: "BatchNorm11"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "BatchNorm11"
  top: "BatchNorm11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "BatchNorm11"
  top: "Convolution12"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout12"
  type: "Dropout"
  bottom: "Convolution12"
  top: "Dropout12"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat11"
  type: "Concat"
  bottom: "Concat10"
  bottom: "Dropout12"
  top: "Concat11"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Concat11"
  top: "BatchNorm12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "BatchNorm12"
  top: "BatchNorm12"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "BatchNorm12"
  top: "BatchNorm12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "BatchNorm12"
  top: "Convolution13"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout13"
  type: "Dropout"
  bottom: "Convolution13"
  top: "Dropout13"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat12"
  type: "Concat"
  bottom: "Concat11"
  bottom: "Dropout13"
  top: "Concat12"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Concat12"
  top: "BatchNorm13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "BatchNorm13"
  top: "BatchNorm13"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "BatchNorm13"
  top: "BatchNorm13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "BatchNorm13"
  top: "Convolution14"
  convolution_param {
    num_output: 160
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout14"
  type: "Dropout"
  bottom: "Convolution14"
  top: "Dropout14"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Dropout14"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Pooling1"
  top: "BatchNorm14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "BatchNorm14"
  top: "BatchNorm14"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "BatchNorm14"
  top: "BatchNorm14"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "BatchNorm14"
  top: "Convolution15"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout15"
  type: "Dropout"
  bottom: "Convolution15"
  top: "Dropout15"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat13"
  type: "Concat"
  bottom: "Pooling1"
  bottom: "Dropout15"
  top: "Concat13"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Concat13"
  top: "BatchNorm15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "BatchNorm15"
  top: "BatchNorm15"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "BatchNorm15"
  top: "BatchNorm15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "BatchNorm15"
  top: "Convolution16"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout16"
  type: "Dropout"
  bottom: "Convolution16"
  top: "Dropout16"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat14"
  type: "Concat"
  bottom: "Concat13"
  bottom: "Dropout16"
  top: "Concat14"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Concat14"
  top: "BatchNorm16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "BatchNorm16"
  top: "BatchNorm16"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "BatchNorm16"
  top: "BatchNorm16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "BatchNorm16"
  top: "Convolution17"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout17"
  type: "Dropout"
  bottom: "Convolution17"
  top: "Dropout17"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat15"
  type: "Concat"
  bottom: "Concat14"
  bottom: "Dropout17"
  top: "Concat15"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Concat15"
  top: "BatchNorm17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "BatchNorm17"
  top: "BatchNorm17"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "BatchNorm17"
  top: "BatchNorm17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "BatchNorm17"
  top: "Convolution18"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout18"
  type: "Dropout"
  bottom: "Convolution18"
  top: "Dropout18"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat16"
  type: "Concat"
  bottom: "Concat15"
  bottom: "Dropout18"
  top: "Concat16"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Concat16"
  top: "BatchNorm18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "BatchNorm18"
  top: "BatchNorm18"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "BatchNorm18"
  top: "BatchNorm18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "BatchNorm18"
  top: "Convolution19"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout19"
  type: "Dropout"
  bottom: "Convolution19"
  top: "Dropout19"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat17"
  type: "Concat"
  bottom: "Concat16"
  bottom: "Dropout19"
  top: "Concat17"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Concat17"
  top: "BatchNorm19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "BatchNorm19"
  top: "BatchNorm19"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "BatchNorm19"
  top: "BatchNorm19"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "BatchNorm19"
  top: "Convolution20"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout20"
  type: "Dropout"
  bottom: "Convolution20"
  top: "Dropout20"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat18"
  type: "Concat"
  bottom: "Concat17"
  bottom: "Dropout20"
  top: "Concat18"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Concat18"
  top: "BatchNorm20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "BatchNorm20"
  top: "BatchNorm20"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "BatchNorm20"
  top: "BatchNorm20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "BatchNorm20"
  top: "Convolution21"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout21"
  type: "Dropout"
  bottom: "Convolution21"
  top: "Dropout21"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat19"
  type: "Concat"
  bottom: "Concat18"
  bottom: "Dropout21"
  top: "Concat19"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Concat19"
  top: "BatchNorm21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "BatchNorm21"
  top: "BatchNorm21"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "BatchNorm21"
  top: "BatchNorm21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "BatchNorm21"
  top: "Convolution22"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout22"
  type: "Dropout"
  bottom: "Convolution22"
  top: "Dropout22"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat20"
  type: "Concat"
  bottom: "Concat19"
  bottom: "Dropout22"
  top: "Concat20"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Concat20"
  top: "BatchNorm22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "BatchNorm22"
  top: "BatchNorm22"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "BatchNorm22"
  top: "BatchNorm22"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "BatchNorm22"
  top: "Convolution23"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout23"
  type: "Dropout"
  bottom: "Convolution23"
  top: "Dropout23"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat21"
  type: "Concat"
  bottom: "Concat20"
  bottom: "Dropout23"
  top: "Concat21"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Concat21"
  top: "BatchNorm23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "BatchNorm23"
  top: "BatchNorm23"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "BatchNorm23"
  top: "BatchNorm23"
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "BatchNorm23"
  top: "Convolution24"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout24"
  type: "Dropout"
  bottom: "Convolution24"
  top: "Dropout24"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat22"
  type: "Concat"
  bottom: "Concat21"
  bottom: "Dropout24"
  top: "Concat22"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Concat22"
  top: "BatchNorm24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "BatchNorm24"
  top: "BatchNorm24"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "BatchNorm24"
  top: "BatchNorm24"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "BatchNorm24"
  top: "Convolution25"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout25"
  type: "Dropout"
  bottom: "Convolution25"
  top: "Dropout25"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat23"
  type: "Concat"
  bottom: "Concat22"
  bottom: "Dropout25"
  top: "Concat23"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Concat23"
  top: "BatchNorm25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "BatchNorm25"
  top: "BatchNorm25"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "BatchNorm25"
  top: "BatchNorm25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "BatchNorm25"
  top: "Convolution26"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout26"
  type: "Dropout"
  bottom: "Convolution26"
  top: "Dropout26"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat24"
  type: "Concat"
  bottom: "Concat23"
  bottom: "Dropout26"
  top: "Concat24"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Concat24"
  top: "BatchNorm26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "BatchNorm26"
  top: "BatchNorm26"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "BatchNorm26"
  top: "BatchNorm26"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "BatchNorm26"
  top: "Convolution27"
  convolution_param {
    num_output: 304
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout27"
  type: "Dropout"
  bottom: "Convolution27"
  top: "Dropout27"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Dropout27"
  top: "Pooling2"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Pooling2"
  top: "BatchNorm27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "BatchNorm27"
  top: "BatchNorm27"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU27"
  type
I0905 01:04:49.254493 73306 layer_factory.hpp:77] Creating layer Data1
I0905 01:04:49.254544 73306 net.cpp:100] Creating Layer Data1
I0905 01:04:49.254559 73306 net.cpp:408] Data1 -> Data1
I0905 01:04:49.254595 73306 net.cpp:408] Data1 -> Data2
I0905 01:04:49.255015 73306 image_data_layer.cpp:38] Opening file ../list_bias-1_tr.lst
I0905 01:04:49.776764 73306 image_data_layer.cpp:53] Shuffling data
I0905 01:04:50.014055 73306 image_data_layer.cpp:58] A total of 1056600 images.
I0905 01:04:50.110035 73306 image_data_layer.cpp:85] output data size: 16,3,64,64
I0905 01:04:50.120687 73306 net.cpp:150] Setting up Data1
I0905 01:04:50.120825 73306 net.cpp:157] Top shape: 16 3 64 64 (196608)
I0905 01:04:50.120862 73306 net.cpp:157] Top shape: 16 (16)
I0905 01:04:50.120867 73306 net.cpp:165] Memory required for data: 786496
I0905 01:04:50.120898 73306 layer_factory.hpp:77] Creating layer Convolution1
I0905 01:04:50.120959 73306 net.cpp:100] Creating Layer Convolution1
I0905 01:04:50.120970 73306 net.cpp:434] Convolution1 <- Data1
I0905 01:04:50.121032 73306 net.cpp:408] Convolution1 -> Convolution1
I0905 01:04:50.331912 73306 net.cpp:150] Setting up Convolution1
I0905 01:04:50.331965 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.331979 73306 net.cpp:165] Memory required for data: 4980800
I0905 01:04:50.332008 73306 layer_factory.hpp:77] Creating layer Dropout1
I0905 01:04:50.332037 73306 net.cpp:100] Creating Layer Dropout1
I0905 01:04:50.332048 73306 net.cpp:434] Dropout1 <- Convolution1
I0905 01:04:50.332062 73306 net.cpp:408] Dropout1 -> Dropout1
I0905 01:04:50.332120 73306 net.cpp:150] Setting up Dropout1
I0905 01:04:50.332134 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.332142 73306 net.cpp:165] Memory required for data: 9175104
I0905 01:04:50.332149 73306 layer_factory.hpp:77] Creating layer Dropout1_Dropout1_0_split
I0905 01:04:50.332162 73306 net.cpp:100] Creating Layer Dropout1_Dropout1_0_split
I0905 01:04:50.332170 73306 net.cpp:434] Dropout1_Dropout1_0_split <- Dropout1
I0905 01:04:50.332213 73306 net.cpp:408] Dropout1_Dropout1_0_split -> Dropout1_Dropout1_0_split_0
I0905 01:04:50.332226 73306 net.cpp:408] Dropout1_Dropout1_0_split -> Dropout1_Dropout1_0_split_1
I0905 01:04:50.332262 73306 net.cpp:150] Setting up Dropout1_Dropout1_0_split
I0905 01:04:50.332279 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.332288 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.332295 73306 net.cpp:165] Memory required for data: 17563712
I0905 01:04:50.332304 73306 layer_factory.hpp:77] Creating layer BatchNorm1
I0905 01:04:50.332324 73306 net.cpp:100] Creating Layer BatchNorm1
I0905 01:04:50.332332 73306 net.cpp:434] BatchNorm1 <- Dropout1_Dropout1_0_split_0
I0905 01:04:50.332341 73306 net.cpp:408] BatchNorm1 -> BatchNorm1
I0905 01:04:50.332500 73306 net.cpp:150] Setting up BatchNorm1
I0905 01:04:50.332516 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.332525 73306 net.cpp:165] Memory required for data: 21758016
I0905 01:04:50.332538 73306 layer_factory.hpp:77] Creating layer Scale1
I0905 01:04:50.332562 73306 net.cpp:100] Creating Layer Scale1
I0905 01:04:50.332571 73306 net.cpp:434] Scale1 <- BatchNorm1
I0905 01:04:50.332579 73306 net.cpp:395] Scale1 -> BatchNorm1 (in-place)
I0905 01:04:50.332664 73306 net.cpp:150] Setting up Scale1
I0905 01:04:50.332679 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.332690 73306 net.cpp:165] Memory required for data: 25952320
I0905 01:04:50.332698 73306 layer_factory.hpp:77] Creating layer ReLU1
I0905 01:04:50.332715 73306 net.cpp:100] Creating Layer ReLU1
I0905 01:04:50.332722 73306 net.cpp:434] ReLU1 <- BatchNorm1
I0905 01:04:50.332731 73306 net.cpp:395] ReLU1 -> BatchNorm1 (in-place)
I0905 01:04:50.332885 73306 net.cpp:150] Setting up ReLU1
I0905 01:04:50.332901 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.332908 73306 net.cpp:165] Memory required for data: 30146624
I0905 01:04:50.332916 73306 layer_factory.hpp:77] Creating layer Convolution2
I0905 01:04:50.332937 73306 net.cpp:100] Creating Layer Convolution2
I0905 01:04:50.332944 73306 net.cpp:434] Convolution2 <- BatchNorm1
I0905 01:04:50.332954 73306 net.cpp:408] Convolution2 -> Convolution2
I0905 01:04:50.333807 73306 net.cpp:150] Setting up Convolution2
I0905 01:04:50.333827 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.333837 73306 net.cpp:165] Memory required for data: 33292352
I0905 01:04:50.333847 73306 layer_factory.hpp:77] Creating layer Dropout2
I0905 01:04:50.333861 73306 net.cpp:100] Creating Layer Dropout2
I0905 01:04:50.333869 73306 net.cpp:434] Dropout2 <- Convolution2
I0905 01:04:50.333878 73306 net.cpp:408] Dropout2 -> Dropout2
I0905 01:04:50.333925 73306 net.cpp:150] Setting up Dropout2
I0905 01:04:50.333936 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.333943 73306 net.cpp:165] Memory required for data: 36438080
I0905 01:04:50.333950 73306 layer_factory.hpp:77] Creating layer Concat1
I0905 01:04:50.333966 73306 net.cpp:100] Creating Layer Concat1
I0905 01:04:50.333973 73306 net.cpp:434] Concat1 <- Dropout1_Dropout1_0_split_1
I0905 01:04:50.333981 73306 net.cpp:434] Concat1 <- Dropout2
I0905 01:04:50.333989 73306 net.cpp:408] Concat1 -> Concat1
I0905 01:04:50.334020 73306 net.cpp:150] Setting up Concat1
I0905 01:04:50.334033 73306 net.cpp:157] Top shape: 16 28 64 64 (1835008)
I0905 01:04:50.334038 73306 net.cpp:165] Memory required for data: 43778112
I0905 01:04:50.334045 73306 layer_factory.hpp:77] Creating layer Concat1_Concat1_0_split
I0905 01:04:50.334056 73306 net.cpp:100] Creating Layer Concat1_Concat1_0_split
I0905 01:04:50.334064 73306 net.cpp:434] Concat1_Concat1_0_split <- Concat1
I0905 01:04:50.334072 73306 net.cpp:408] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_0
I0905 01:04:50.334084 73306 net.cpp:408] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_1
I0905 01:04:50.334115 73306 net.cpp:150] Setting up Concat1_Concat1_0_split
I0905 01:04:50.334127 73306 net.cpp:157] Top shape: 16 28 64 64 (1835008)
I0905 01:04:50.334147 73306 net.cpp:157] Top shape: 16 28 64 64 (1835008)
I0905 01:04:50.334154 73306 net.cpp:165] Memory required for data: 58458176
I0905 01:04:50.334162 73306 layer_factory.hpp:77] Creating layer BatchNorm2
I0905 01:04:50.334172 73306 net.cpp:100] Creating Layer BatchNorm2
I0905 01:04:50.334185 73306 net.cpp:434] BatchNorm2 <- Concat1_Concat1_0_split_0
I0905 01:04:50.334194 73306 net.cpp:408] BatchNorm2 -> BatchNorm2
I0905 01:04:50.334347 73306 net.cpp:150] Setting up BatchNorm2
I0905 01:04:50.334362 73306 net.cpp:157] Top shape: 16 28 64 64 (1835008)
I0905 01:04:50.334369 73306 net.cpp:165] Memory required for data: 65798208
I0905 01:04:50.334381 73306 layer_factory.hpp:77] Creating layer Scale2
I0905 01:04:50.334393 73306 net.cpp:100] Creating Layer Scale2
I0905 01:04:50.334405 73306 net.cpp:434] Scale2 <- BatchNorm2
I0905 01:04:50.334415 73306 net.cpp:395] Scale2 -> BatchNorm2 (in-place)
I0905 01:04:50.334492 73306 net.cpp:150] Setting up Scale2
I0905 01:04:50.334506 73306 net.cpp:157] Top shape: 16 28 64 64 (1835008)
I0905 01:04:50.334513 73306 net.cpp:165] Memory required for data: 73138240
I0905 01:04:50.334522 73306 layer_factory.hpp:77] Creating layer ReLU2
I0905 01:04:50.334532 73306 net.cpp:100] Creating Layer ReLU2
I0905 01:04:50.334540 73306 net.cpp:434] ReLU2 <- BatchNorm2
I0905 01:04:50.334547 73306 net.cpp:395] ReLU2 -> BatchNorm2 (in-place)
I0905 01:04:50.334867 73306 net.cpp:150] Setting up ReLU2
I0905 01:04:50.334885 73306 net.cpp:157] Top shape: 16 28 64 64 (1835008)
I0905 01:04:50.334893 73306 net.cpp:165] Memory required for data: 80478272
I0905 01:04:50.334900 73306 layer_factory.hpp:77] Creating layer Convolution3
I0905 01:04:50.334915 73306 net.cpp:100] Creating Layer Convolution3
I0905 01:04:50.334923 73306 net.cpp:434] Convolution3 <- BatchNorm2
I0905 01:04:50.334934 73306 net.cpp:408] Convolution3 -> Convolution3
I0905 01:04:50.335891 73306 net.cpp:150] Setting up Convolution3
I0905 01:04:50.335911 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.335919 73306 net.cpp:165] Memory required for data: 83624000
I0905 01:04:50.335929 73306 layer_factory.hpp:77] Creating layer Dropout3
I0905 01:04:50.335943 73306 net.cpp:100] Creating Layer Dropout3
I0905 01:04:50.335950 73306 net.cpp:434] Dropout3 <- Convolution3
I0905 01:04:50.335959 73306 net.cpp:408] Dropout3 -> Dropout3
I0905 01:04:50.336006 73306 net.cpp:150] Setting up Dropout3
I0905 01:04:50.336021 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.336028 73306 net.cpp:165] Memory required for data: 86769728
I0905 01:04:50.336035 73306 layer_factory.hpp:77] Creating layer Concat2
I0905 01:04:50.336051 73306 net.cpp:100] Creating Layer Concat2
I0905 01:04:50.336060 73306 net.cpp:434] Concat2 <- Concat1_Concat1_0_split_1
I0905 01:04:50.336068 73306 net.cpp:434] Concat2 <- Dropout3
I0905 01:04:50.336078 73306 net.cpp:408] Concat2 -> Concat2
I0905 01:04:50.336100 73306 net.cpp:150] Setting up Concat2
I0905 01:04:50.336112 73306 net.cpp:157] Top shape: 16 40 64 64 (2621440)
I0905 01:04:50.336117 73306 net.cpp:165] Memory required for data: 97255488
I0905 01:04:50.336124 73306 layer_factory.hpp:77] Creating layer Concat2_Concat2_0_split
I0905 01:04:50.336135 73306 net.cpp:100] Creating Layer Concat2_Concat2_0_split
I0905 01:04:50.336143 73306 net.cpp:434] Concat2_Concat2_0_split <- Concat2
I0905 01:04:50.336150 73306 net.cpp:408] Concat2_Concat2_0_split -> Concat2_Concat2_0_split_0
I0905 01:04:50.336163 73306 net.cpp:408] Concat2_Concat2_0_split -> Concat2_Concat2_0_split_1
I0905 01:04:50.336195 73306 net.cpp:150] Setting up Concat2_Concat2_0_split
I0905 01:04:50.336206 73306 net.cpp:157] Top shape: 16 40 64 64 (2621440)
I0905 01:04:50.336215 73306 net.cpp:157] Top shape: 16 40 64 64 (2621440)
I0905 01:04:50.336220 73306 net.cpp:165] Memory required for data: 118227008
I0905 01:04:50.336227 73306 layer_factory.hpp:77] Creating layer BatchNorm3
I0905 01:04:50.336236 73306 net.cpp:100] Creating Layer BatchNorm3
I0905 01:04:50.336243 73306 net.cpp:434] BatchNorm3 <- Concat2_Concat2_0_split_0
I0905 01:04:50.336266 73306 net.cpp:408] BatchNorm3 -> BatchNorm3
I0905 01:04:50.336426 73306 net.cpp:150] Setting up BatchNorm3
I0905 01:04:50.336438 73306 net.cpp:157] Top shape: 16 40 64 64 (2621440)
I0905 01:04:50.336448 73306 net.cpp:165] Memory required for data: 128712768
I0905 01:04:50.336463 73306 layer_factory.hpp:77] Creating layer Scale3
I0905 01:04:50.336474 73306 net.cpp:100] Creating Layer Scale3
I0905 01:04:50.336482 73306 net.cpp:434] Scale3 <- BatchNorm3
I0905 01:04:50.336489 73306 net.cpp:395] Scale3 -> BatchNorm3 (in-place)
I0905 01:04:50.336565 73306 net.cpp:150] Setting up Scale3
I0905 01:04:50.336580 73306 net.cpp:157] Top shape: 16 40 64 64 (2621440)
I0905 01:04:50.336590 73306 net.cpp:165] Memory required for data: 139198528
I0905 01:04:50.336597 73306 layer_factory.hpp:77] Creating layer ReLU3
I0905 01:04:50.336611 73306 net.cpp:100] Creating Layer ReLU3
I0905 01:04:50.336617 73306 net.cpp:434] ReLU3 <- BatchNorm3
I0905 01:04:50.336627 73306 net.cpp:395] ReLU3 -> BatchNorm3 (in-place)
I0905 01:04:50.336776 73306 net.cpp:150] Setting up ReLU3
I0905 01:04:50.336791 73306 net.cpp:157] Top shape: 16 40 64 64 (2621440)
I0905 01:04:50.336807 73306 net.cpp:165] Memory required for data: 149684288
I0905 01:04:50.336814 73306 layer_factory.hpp:77] Creating layer Convolution4
I0905 01:04:50.336827 73306 net.cpp:100] Creating Layer Convolution4
I0905 01:04:50.336835 73306 net.cpp:434] Convolution4 <- BatchNorm3
I0905 01:04:50.336846 73306 net.cpp:408] Convolution4 -> Convolution4
I0905 01:04:50.338429 73306 net.cpp:150] Setting up Convolution4
I0905 01:04:50.338451 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.338459 73306 net.cpp:165] Memory required for data: 152830016
I0905 01:04:50.338470 73306 layer_factory.hpp:77] Creating layer Dropout4
I0905 01:04:50.338484 73306 net.cpp:100] Creating Layer Dropout4
I0905 01:04:50.338491 73306 net.cpp:434] Dropout4 <- Convolution4
I0905 01:04:50.338501 73306 net.cpp:408] Dropout4 -> Dropout4
I0905 01:04:50.338547 73306 net.cpp:150] Setting up Dropout4
I0905 01:04:50.338558 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.338565 73306 net.cpp:165] Memory required for data: 155975744
I0905 01:04:50.338572 73306 layer_factory.hpp:77] Creating layer Concat3
I0905 01:04:50.338585 73306 net.cpp:100] Creating Layer Concat3
I0905 01:04:50.338593 73306 net.cpp:434] Concat3 <- Concat2_Concat2_0_split_1
I0905 01:04:50.338600 73306 net.cpp:434] Concat3 <- Dropout4
I0905 01:04:50.338608 73306 net.cpp:408] Concat3 -> Concat3
I0905 01:04:50.338639 73306 net.cpp:150] Setting up Concat3
I0905 01:04:50.338650 73306 net.cpp:157] Top shape: 16 52 64 64 (3407872)
I0905 01:04:50.338656 73306 net.cpp:165] Memory required for data: 169607232
I0905 01:04:50.338663 73306 layer_factory.hpp:77] Creating layer Concat3_Concat3_0_split
I0905 01:04:50.338671 73306 net.cpp:100] Creating Layer Concat3_Concat3_0_split
I0905 01:04:50.338678 73306 net.cpp:434] Concat3_Concat3_0_split <- Concat3
I0905 01:04:50.338688 73306 net.cpp:408] Concat3_Concat3_0_split -> Concat3_Concat3_0_split_0
I0905 01:04:50.338698 73306 net.cpp:408] Concat3_Concat3_0_split -> Concat3_Concat3_0_split_1
I0905 01:04:50.338739 73306 net.cpp:150] Setting up Concat3_Concat3_0_split
I0905 01:04:50.338752 73306 net.cpp:157] Top shape: 16 52 64 64 (3407872)
I0905 01:04:50.338759 73306 net.cpp:157] Top shape: 16 52 64 64 (3407872)
I0905 01:04:50.338765 73306 net.cpp:165] Memory required for data: 196870208
I0905 01:04:50.338773 73306 layer_factory.hpp:77] Creating layer BatchNorm4
I0905 01:04:50.338780 73306 net.cpp:100] Creating Layer BatchNorm4
I0905 01:04:50.338788 73306 net.cpp:434] BatchNorm4 <- Concat3_Concat3_0_split_0
I0905 01:04:50.338800 73306 net.cpp:408] BatchNorm4 -> BatchNorm4
I0905 01:04:50.338968 73306 net.cpp:150] Setting up BatchNorm4
I0905 01:04:50.338980 73306 net.cpp:157] Top shape: 16 52 64 64 (3407872)
I0905 01:04:50.338987 73306 net.cpp:165] Memory required for data: 210501696
I0905 01:04:50.338999 73306 layer_factory.hpp:77] Creating layer Scale4
I0905 01:04:50.339023 73306 net.cpp:100] Creating Layer Scale4
I0905 01:04:50.339032 73306 net.cpp:434] Scale4 <- BatchNorm4
I0905 01:04:50.339041 73306 net.cpp:395] Scale4 -> BatchNorm4 (in-place)
I0905 01:04:50.339123 73306 net.cpp:150] Setting up Scale4
I0905 01:04:50.339138 73306 net.cpp:157] Top shape: 16 52 64 64 (3407872)
I0905 01:04:50.339148 73306 net.cpp:165] Memory required for data: 224133184
I0905 01:04:50.339156 73306 layer_factory.hpp:77] Creating layer ReLU4
I0905 01:04:50.339169 73306 net.cpp:100] Creating Layer ReLU4
I0905 01:04:50.339175 73306 net.cpp:434] ReLU4 <- BatchNorm4
I0905 01:04:50.339182 73306 net.cpp:395] ReLU4 -> BatchNorm4 (in-place)
I0905 01:04:50.339334 73306 net.cpp:150] Setting up ReLU4
I0905 01:04:50.339350 73306 net.cpp:157] Top shape: 16 52 64 64 (3407872)
I0905 01:04:50.339365 73306 net.cpp:165] Memory required for data: 237764672
I0905 01:04:50.339373 73306 layer_factory.hpp:77] Creating layer Convolution5
I0905 01:04:50.339387 73306 net.cpp:100] Creating Layer Convolution5
I0905 01:04:50.339396 73306 net.cpp:434] Convolution5 <- BatchNorm4
I0905 01:04:50.339406 73306 net.cpp:408] Convolution5 -> Convolution5
I0905 01:04:50.340435 73306 net.cpp:150] Setting up Convolution5
I0905 01:04:50.340456 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.340464 73306 net.cpp:165] Memory required for data: 240910400
I0905 01:04:50.340474 73306 layer_factory.hpp:77] Creating layer Dropout5
I0905 01:04:50.340486 73306 net.cpp:100] Creating Layer Dropout5
I0905 01:04:50.340492 73306 net.cpp:434] Dropout5 <- Convolution5
I0905 01:04:50.340503 73306 net.cpp:408] Dropout5 -> Dropout5
I0905 01:04:50.340553 73306 net.cpp:150] Setting up Dropout5
I0905 01:04:50.340564 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.340570 73306 net.cpp:165] Memory required for data: 244056128
I0905 01:04:50.340576 73306 layer_factory.hpp:77] Creating layer Concat4
I0905 01:04:50.340590 73306 net.cpp:100] Creating Layer Concat4
I0905 01:04:50.340596 73306 net.cpp:434] Concat4 <- Concat3_Concat3_0_split_1
I0905 01:04:50.340605 73306 net.cpp:434] Concat4 <- Dropout5
I0905 01:04:50.340612 73306 net.cpp:408] Concat4 -> Concat4
I0905 01:04:50.340638 73306 net.cpp:150] Setting up Concat4
I0905 01:04:50.340649 73306 net.cpp:157] Top shape: 16 64 64 64 (4194304)
I0905 01:04:50.340656 73306 net.cpp:165] Memory required for data: 260833344
I0905 01:04:50.340662 73306 layer_factory.hpp:77] Creating layer Concat4_Concat4_0_split
I0905 01:04:50.340670 73306 net.cpp:100] Creating Layer Concat4_Concat4_0_split
I0905 01:04:50.340677 73306 net.cpp:434] Concat4_Concat4_0_split <- Concat4
I0905 01:04:50.340687 73306 net.cpp:408] Concat4_Concat4_0_split -> Concat4_Concat4_0_split_0
I0905 01:04:50.340700 73306 net.cpp:408] Concat4_Concat4_0_split -> Concat4_Concat4_0_split_1
I0905 01:04:50.340736 73306 net.cpp:150] Setting up Concat4_Concat4_0_split
I0905 01:04:50.340747 73306 net.cpp:157] Top shape: 16 64 64 64 (4194304)
I0905 01:04:50.340754 73306 net.cpp:157] Top shape: 16 64 64 64 (4194304)
I0905 01:04:50.340760 73306 net.cpp:165] Memory required for data: 294387776
I0905 01:04:50.340767 73306 layer_factory.hpp:77] Creating layer BatchNorm5
I0905 01:04:50.340781 73306 net.cpp:100] Creating Layer BatchNorm5
I0905 01:04:50.340790 73306 net.cpp:434] BatchNorm5 <- Concat4_Concat4_0_split_0
I0905 01:04:50.340797 73306 net.cpp:408] BatchNorm5 -> BatchNorm5
I0905 01:04:50.340961 73306 net.cpp:150] Setting up BatchNorm5
I0905 01:04:50.340973 73306 net.cpp:157] Top shape: 16 64 64 64 (4194304)
I0905 01:04:50.340981 73306 net.cpp:165] Memory required for data: 311164992
I0905 01:04:50.340991 73306 layer_factory.hpp:77] Creating layer Scale5
I0905 01:04:50.341006 73306 net.cpp:100] Creating Layer Scale5
I0905 01:04:50.341014 73306 net.cpp:434] Scale5 <- BatchNorm5
I0905 01:04:50.341023 73306 net.cpp:395] Scale5 -> BatchNorm5 (in-place)
I0905 01:04:50.341101 73306 net.cpp:150] Setting up Scale5
I0905 01:04:50.341112 73306 net.cpp:157] Top shape: 16 64 64 64 (4194304)
I0905 01:04:50.341120 73306 net.cpp:165] Memory required for data: 327942208
I0905 01:04:50.341145 73306 layer_factory.hpp:77] Creating layer ReLU5
I0905 01:04:50.341156 73306 net.cpp:100] Creating Layer ReLU5
I0905 01:04:50.341163 73306 net.cpp:434] ReLU5 <- BatchNorm5
I0905 01:04:50.341172 73306 net.cpp:395] ReLU5 -> BatchNorm5 (in-place)
I0905 01:04:50.341449 73306 net.cpp:150] Setting up ReLU5
I0905 01:04:50.341467 73306 net.cpp:157] Top shape: 16 64 64 64 (4194304)
I0905 01:04:50.341475 73306 net.cpp:165] Memory required for data: 344719424
I0905 01:04:50.341482 73306 layer_factory.hpp:77] Creating layer Convolution6
I0905 01:04:50.341497 73306 net.cpp:100] Creating Layer Convolution6
I0905 01:04:50.341505 73306 net.cpp:434] Convolution6 <- BatchNorm5
I0905 01:04:50.341516 73306 net.cpp:408] Convolution6 -> Convolution6
I0905 01:04:50.342600 73306 net.cpp:150] Setting up Convolution6
I0905 01:04:50.342620 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.342633 73306 net.cpp:165] Memory required for data: 347865152
I0905 01:04:50.342660 73306 layer_factory.hpp:77] Creating layer Dropout6
I0905 01:04:50.342675 73306 net.cpp:100] Creating Layer Dropout6
I0905 01:04:50.342684 73306 net.cpp:434] Dropout6 <- Convolution6
I0905 01:04:50.342700 73306 net.cpp:408] Dropout6 -> Dropout6
I0905 01:04:50.342756 73306 net.cpp:150] Setting up Dropout6
I0905 01:04:50.342772 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.342782 73306 net.cpp:165] Memory required for data: 351010880
I0905 01:04:50.342790 73306 layer_factory.hpp:77] Creating layer Concat5
I0905 01:04:50.342803 73306 net.cpp:100] Creating Layer Concat5
I0905 01:04:50.342810 73306 net.cpp:434] Concat5 <- Concat4_Concat4_0_split_1
I0905 01:04:50.342818 73306 net.cpp:434] Concat5 <- Dropout6
I0905 01:04:50.342828 73306 net.cpp:408] Concat5 -> Concat5
I0905 01:04:50.342855 73306 net.cpp:150] Setting up Concat5
I0905 01:04:50.342871 73306 net.cpp:157] Top shape: 16 76 64 64 (4980736)
I0905 01:04:50.342878 73306 net.cpp:165] Memory required for data: 370933824
I0905 01:04:50.342885 73306 layer_factory.hpp:77] Creating layer Concat5_Concat5_0_split
I0905 01:04:50.342895 73306 net.cpp:100] Creating Layer Concat5_Concat5_0_split
I0905 01:04:50.342902 73306 net.cpp:434] Concat5_Concat5_0_split <- Concat5
I0905 01:04:50.342912 73306 net.cpp:408] Concat5_Concat5_0_split -> Concat5_Concat5_0_split_0
I0905 01:04:50.342923 73306 net.cpp:408] Concat5_Concat5_0_split -> Concat5_Concat5_0_split_1
I0905 01:04:50.342957 73306 net.cpp:150] Setting up Concat5_Concat5_0_split
I0905 01:04:50.342967 73306 net.cpp:157] Top shape: 16 76 64 64 (4980736)
I0905 01:04:50.342974 73306 net.cpp:157] Top shape: 16 76 64 64 (4980736)
I0905 01:04:50.342980 73306 net.cpp:165] Memory required for data: 410779712
I0905 01:04:50.342986 73306 layer_factory.hpp:77] Creating layer BatchNorm6
I0905 01:04:50.343004 73306 net.cpp:100] Creating Layer BatchNorm6
I0905 01:04:50.343011 73306 net.cpp:434] BatchNorm6 <- Concat5_Concat5_0_split_0
I0905 01:04:50.343019 73306 net.cpp:408] BatchNorm6 -> BatchNorm6
I0905 01:04:50.343184 73306 net.cpp:150] Setting up BatchNorm6
I0905 01:04:50.343204 73306 net.cpp:157] Top shape: 16 76 64 64 (4980736)
I0905 01:04:50.343211 73306 net.cpp:165] Memory required for data: 430702656
I0905 01:04:50.343230 73306 layer_factory.hpp:77] Creating layer Scale6
I0905 01:04:50.343242 73306 net.cpp:100] Creating Layer Scale6
I0905 01:04:50.343251 73306 net.cpp:434] Scale6 <- BatchNorm6
I0905 01:04:50.343261 73306 net.cpp:395] Scale6 -> BatchNorm6 (in-place)
I0905 01:04:50.343348 73306 net.cpp:150] Setting up Scale6
I0905 01:04:50.343364 73306 net.cpp:157] Top shape: 16 76 64 64 (4980736)
I0905 01:04:50.343374 73306 net.cpp:165] Memory required for data: 450625600
I0905 01:04:50.343382 73306 layer_factory.hpp:77] Creating layer ReLU6
I0905 01:04:50.343392 73306 net.cpp:100] Creating Layer ReLU6
I0905 01:04:50.343400 73306 net.cpp:434] ReLU6 <- BatchNorm6
I0905 01:04:50.343410 73306 net.cpp:395] ReLU6 -> BatchNorm6 (in-place)
I0905 01:04:50.343562 73306 net.cpp:150] Setting up ReLU6
I0905 01:04:50.343578 73306 net.cpp:157] Top shape: 16 76 64 64 (4980736)
I0905 01:04:50.343598 73306 net.cpp:165] Memory required for data: 470548544
I0905 01:04:50.343606 73306 layer_factory.hpp:77] Creating layer Convolution7
I0905 01:04:50.343622 73306 net.cpp:100] Creating Layer Convolution7
I0905 01:04:50.343631 73306 net.cpp:434] Convolution7 <- BatchNorm6
I0905 01:04:50.343641 73306 net.cpp:408] Convolution7 -> Convolution7
I0905 01:04:50.344743 73306 net.cpp:150] Setting up Convolution7
I0905 01:04:50.344763 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.344770 73306 net.cpp:165] Memory required for data: 473694272
I0905 01:04:50.344780 73306 layer_factory.hpp:77] Creating layer Dropout7
I0905 01:04:50.344794 73306 net.cpp:100] Creating Layer Dropout7
I0905 01:04:50.344801 73306 net.cpp:434] Dropout7 <- Convolution7
I0905 01:04:50.344810 73306 net.cpp:408] Dropout7 -> Dropout7
I0905 01:04:50.344852 73306 net.cpp:150] Setting up Dropout7
I0905 01:04:50.344863 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.344871 73306 net.cpp:165] Memory required for data: 476840000
I0905 01:04:50.344882 73306 layer_factory.hpp:77] Creating layer Concat6
I0905 01:04:50.344892 73306 net.cpp:100] Creating Layer Concat6
I0905 01:04:50.344898 73306 net.cpp:434] Concat6 <- Concat5_Concat5_0_split_1
I0905 01:04:50.344907 73306 net.cpp:434] Concat6 <- Dropout7
I0905 01:04:50.344918 73306 net.cpp:408] Concat6 -> Concat6
I0905 01:04:50.344941 73306 net.cpp:150] Setting up Concat6
I0905 01:04:50.344951 73306 net.cpp:157] Top shape: 16 88 64 64 (5767168)
I0905 01:04:50.344957 73306 net.cpp:165] Memory required for data: 499908672
I0905 01:04:50.344964 73306 layer_factory.hpp:77] Creating layer Concat6_Concat6_0_split
I0905 01:04:50.344974 73306 net.cpp:100] Creating Layer Concat6_Concat6_0_split
I0905 01:04:50.344982 73306 net.cpp:434] Concat6_Concat6_0_split <- Concat6
I0905 01:04:50.344993 73306 net.cpp:408] Concat6_Concat6_0_split -> Concat6_Concat6_0_split_0
I0905 01:04:50.345010 73306 net.cpp:408] Concat6_Concat6_0_split -> Concat6_Concat6_0_split_1
I0905 01:04:50.345042 73306 net.cpp:150] Setting up Concat6_Concat6_0_split
I0905 01:04:50.345052 73306 net.cpp:157] Top shape: 16 88 64 64 (5767168)
I0905 01:04:50.345060 73306 net.cpp:157] Top shape: 16 88 64 64 (5767168)
I0905 01:04:50.345067 73306 net.cpp:165] Memory required for data: 546046016
I0905 01:04:50.345072 73306 layer_factory.hpp:77] Creating layer BatchNorm7
I0905 01:04:50.345084 73306 net.cpp:100] Creating Layer BatchNorm7
I0905 01:04:50.345098 73306 net.cpp:434] BatchNorm7 <- Concat6_Concat6_0_split_0
I0905 01:04:50.345108 73306 net.cpp:408] BatchNorm7 -> BatchNorm7
I0905 01:04:50.345276 73306 net.cpp:150] Setting up BatchNorm7
I0905 01:04:50.345288 73306 net.cpp:157] Top shape: 16 88 64 64 (5767168)
I0905 01:04:50.345296 73306 net.cpp:165] Memory required for data: 569114688
I0905 01:04:50.345306 73306 layer_factory.hpp:77] Creating layer Scale7
I0905 01:04:50.345319 73306 net.cpp:100] Creating Layer Scale7
I0905 01:04:50.345327 73306 net.cpp:434] Scale7 <- BatchNorm7
I0905 01:04:50.345335 73306 net.cpp:395] Scale7 -> BatchNorm7 (in-place)
I0905 01:04:50.345414 73306 net.cpp:150] Setting up Scale7
I0905 01:04:50.345427 73306 net.cpp:157] Top shape: 16 88 64 64 (5767168)
I0905 01:04:50.345437 73306 net.cpp:165] Memory required for data: 592183360
I0905 01:04:50.345445 73306 layer_factory.hpp:77] Creating layer ReLU7
I0905 01:04:50.345456 73306 net.cpp:100] Creating Layer ReLU7
I0905 01:04:50.345463 73306 net.cpp:434] ReLU7 <- BatchNorm7
I0905 01:04:50.345474 73306 net.cpp:395] ReLU7 -> BatchNorm7 (in-place)
I0905 01:04:50.345625 73306 net.cpp:150] Setting up ReLU7
I0905 01:04:50.345643 73306 net.cpp:157] Top shape: 16 88 64 64 (5767168)
I0905 01:04:50.345649 73306 net.cpp:165] Memory required for data: 615252032
I0905 01:04:50.345656 73306 layer_factory.hpp:77] Creating layer Convolution8
I0905 01:04:50.345670 73306 net.cpp:100] Creating Layer Convolution8
I0905 01:04:50.345679 73306 net.cpp:434] Convolution8 <- BatchNorm7
I0905 01:04:50.345687 73306 net.cpp:408] Convolution8 -> Convolution8
I0905 01:04:50.347028 73306 net.cpp:150] Setting up Convolution8
I0905 01:04:50.347048 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.347055 73306 net.cpp:165] Memory required for data: 618397760
I0905 01:04:50.347066 73306 layer_factory.hpp:77] Creating layer Dropout8
I0905 01:04:50.347079 73306 net.cpp:100] Creating Layer Dropout8
I0905 01:04:50.347086 73306 net.cpp:434] Dropout8 <- Convolution8
I0905 01:04:50.347095 73306 net.cpp:408] Dropout8 -> Dropout8
I0905 01:04:50.347138 73306 net.cpp:150] Setting up Dropout8
I0905 01:04:50.347151 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.347157 73306 net.cpp:165] Memory required for data: 621543488
I0905 01:04:50.347163 73306 layer_factory.hpp:77] Creating layer Concat7
I0905 01:04:50.347174 73306 net.cpp:100] Creating Layer Concat7
I0905 01:04:50.347182 73306 net.cpp:434] Concat7 <- Concat6_Concat6_0_split_1
I0905 01:04:50.347189 73306 net.cpp:434] Concat7 <- Dropout8
I0905 01:04:50.347199 73306 net.cpp:408] Concat7 -> Concat7
I0905 01:04:50.347223 73306 net.cpp:150] Setting up Concat7
I0905 01:04:50.347234 73306 net.cpp:157] Top shape: 16 100 64 64 (6553600)
I0905 01:04:50.347240 73306 net.cpp:165] Memory required for data: 647757888
I0905 01:04:50.347249 73306 layer_factory.hpp:77] Creating layer Concat7_Concat7_0_split
I0905 01:04:50.347259 73306 net.cpp:100] Creating Layer Concat7_Concat7_0_split
I0905 01:04:50.347265 73306 net.cpp:434] Concat7_Concat7_0_split <- Concat7
I0905 01:04:50.347275 73306 net.cpp:408] Concat7_Concat7_0_split -> Concat7_Concat7_0_split_0
I0905 01:04:50.347285 73306 net.cpp:408] Concat7_Concat7_0_split -> Concat7_Concat7_0_split_1
I0905 01:04:50.347321 73306 net.cpp:150] Setting up Concat7_Concat7_0_split
I0905 01:04:50.347329 73306 net.cpp:157] Top shape: 16 100 64 64 (6553600)
I0905 01:04:50.347337 73306 net.cpp:157] Top shape: 16 100 64 64 (6553600)
I0905 01:04:50.347343 73306 net.cpp:165] Memory required for data: 700186688
I0905 01:04:50.347350 73306 layer_factory.hpp:77] Creating layer BatchNorm8
I0905 01:04:50.347360 73306 net.cpp:100] Creating Layer BatchNorm8
I0905 01:04:50.347368 73306 net.cpp:434] BatchNorm8 <- Concat7_Concat7_0_split_0
I0905 01:04:50.347375 73306 net.cpp:408] BatchNorm8 -> BatchNorm8
I0905 01:04:50.347538 73306 net.cpp:150] Setting up BatchNorm8
I0905 01:04:50.347551 73306 net.cpp:157] Top shape: 16 100 64 64 (6553600)
I0905 01:04:50.347563 73306 net.cpp:165] Memory required for data: 726401088
I0905 01:04:50.347573 73306 layer_factory.hpp:77] Creating layer Scale8
I0905 01:04:50.347585 73306 net.cpp:100] Creating Layer Scale8
I0905 01:04:50.347594 73306 net.cpp:434] Scale8 <- BatchNorm8
I0905 01:04:50.347602 73306 net.cpp:395] Scale8 -> BatchNorm8 (in-place)
I0905 01:04:50.347688 73306 net.cpp:150] Setting up Scale8
I0905 01:04:50.347708 73306 net.cpp:157] Top shape: 16 100 64 64 (6553600)
I0905 01:04:50.347718 73306 net.cpp:165] Memory required for data: 752615488
I0905 01:04:50.347726 73306 layer_factory.hpp:77] Creating layer ReLU8
I0905 01:04:50.347739 73306 net.cpp:100] Creating Layer ReLU8
I0905 01:04:50.347746 73306 net.cpp:434] ReLU8 <- BatchNorm8
I0905 01:04:50.347756 73306 net.cpp:395] ReLU8 -> BatchNorm8 (in-place)
I0905 01:04:50.348047 73306 net.cpp:150] Setting up ReLU8
I0905 01:04:50.348065 73306 net.cpp:157] Top shape: 16 100 64 64 (6553600)
I0905 01:04:50.348073 73306 net.cpp:165] Memory required for data: 778829888
I0905 01:04:50.348081 73306 layer_factory.hpp:77] Creating layer Convolution9
I0905 01:04:50.348096 73306 net.cpp:100] Creating Layer Convolution9
I0905 01:04:50.348104 73306 net.cpp:434] Convolution9 <- BatchNorm8
I0905 01:04:50.348115 73306 net.cpp:408] Convolution9 -> Convolution9
I0905 01:04:50.349344 73306 net.cpp:150] Setting up Convolution9
I0905 01:04:50.349364 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.349371 73306 net.cpp:165] Memory required for data: 781975616
I0905 01:04:50.349382 73306 layer_factory.hpp:77] Creating layer Dropout9
I0905 01:04:50.349392 73306 net.cpp:100] Creating Layer Dropout9
I0905 01:04:50.349412 73306 net.cpp:434] Dropout9 <- Convolution9
I0905 01:04:50.349432 73306 net.cpp:408] Dropout9 -> Dropout9
I0905 01:04:50.349473 73306 net.cpp:150] Setting up Dropout9
I0905 01:04:50.349484 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.349493 73306 net.cpp:165] Memory required for data: 785121344
I0905 01:04:50.349499 73306 layer_factory.hpp:77] Creating layer Concat8
I0905 01:04:50.349511 73306 net.cpp:100] Creating Layer Concat8
I0905 01:04:50.349519 73306 net.cpp:434] Concat8 <- Concat7_Concat7_0_split_1
I0905 01:04:50.349527 73306 net.cpp:434] Concat8 <- Dropout9
I0905 01:04:50.349535 73306 net.cpp:408] Concat8 -> Concat8
I0905 01:04:50.349561 73306 net.cpp:150] Setting up Concat8
I0905 01:04:50.349572 73306 net.cpp:157] Top shape: 16 112 64 64 (7340032)
I0905 01:04:50.349582 73306 net.cpp:165] Memory required for data: 814481472
I0905 01:04:50.349589 73306 layer_factory.hpp:77] Creating layer Concat8_Concat8_0_split
I0905 01:04:50.349597 73306 net.cpp:100] Creating Layer Concat8_Concat8_0_split
I0905 01:04:50.349604 73306 net.cpp:434] Concat8_Concat8_0_split <- Concat8
I0905 01:04:50.349614 73306 net.cpp:408] Concat8_Concat8_0_split -> Concat8_Concat8_0_split_0
I0905 01:04:50.349632 73306 net.cpp:408] Concat8_Concat8_0_split -> Concat8_Concat8_0_split_1
I0905 01:04:50.349665 73306 net.cpp:150] Setting up Concat8_Concat8_0_split
I0905 01:04:50.349676 73306 net.cpp:157] Top shape: 16 112 64 64 (7340032)
I0905 01:04:50.349684 73306 net.cpp:157] Top shape: 16 112 64 64 (7340032)
I0905 01:04:50.349690 73306 net.cpp:165] Memory required for data: 873201728
I0905 01:04:50.349697 73306 layer_factory.hpp:77] Creating layer BatchNorm9
I0905 01:04:50.349714 73306 net.cpp:100] Creating Layer BatchNorm9
I0905 01:04:50.349721 73306 net.cpp:434] BatchNorm9 <- Concat8_Concat8_0_split_0
I0905 01:04:50.349740 73306 net.cpp:408] BatchNorm9 -> BatchNorm9
I0905 01:04:50.349912 73306 net.cpp:150] Setting up BatchNorm9
I0905 01:04:50.349925 73306 net.cpp:157] Top shape: 16 112 64 64 (7340032)
I0905 01:04:50.349932 73306 net.cpp:165] Memory required for data: 902561856
I0905 01:04:50.349942 73306 layer_factory.hpp:77] Creating layer Scale9
I0905 01:04:50.349953 73306 net.cpp:100] Creating Layer Scale9
I0905 01:04:50.349961 73306 net.cpp:434] Scale9 <- BatchNorm9
I0905 01:04:50.349970 73306 net.cpp:395] Scale9 -> BatchNorm9 (in-place)
I0905 01:04:50.350054 73306 net.cpp:150] Setting up Scale9
I0905 01:04:50.350066 73306 net.cpp:157] Top shape: 16 112 64 64 (7340032)
I0905 01:04:50.350075 73306 net.cpp:165] Memory required for data: 931921984
I0905 01:04:50.350083 73306 layer_factory.hpp:77] Creating layer ReLU9
I0905 01:04:50.350095 73306 net.cpp:100] Creating Layer ReLU9
I0905 01:04:50.350101 73306 net.cpp:434] ReLU9 <- BatchNorm9
I0905 01:04:50.350111 73306 net.cpp:395] ReLU9 -> BatchNorm9 (in-place)
I0905 01:04:50.350273 73306 net.cpp:150] Setting up ReLU9
I0905 01:04:50.350288 73306 net.cpp:157] Top shape: 16 112 64 64 (7340032)
I0905 01:04:50.350296 73306 net.cpp:165] Memory required for data: 961282112
I0905 01:04:50.350303 73306 layer_factory.hpp:77] Creating layer Convolution10
I0905 01:04:50.350320 73306 net.cpp:100] Creating Layer Convolution10
I0905 01:04:50.350327 73306 net.cpp:434] Convolution10 <- BatchNorm9
I0905 01:04:50.350339 73306 net.cpp:408] Convolution10 -> Convolution10
I0905 01:04:50.351600 73306 net.cpp:150] Setting up Convolution10
I0905 01:04:50.351620 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.351629 73306 net.cpp:165] Memory required for data: 964427840
I0905 01:04:50.351639 73306 layer_factory.hpp:77] Creating layer Dropout10
I0905 01:04:50.351657 73306 net.cpp:100] Creating Layer Dropout10
I0905 01:04:50.351666 73306 net.cpp:434] Dropout10 <- Convolution10
I0905 01:04:50.351675 73306 net.cpp:408] Dropout10 -> Dropout10
I0905 01:04:50.351717 73306 net.cpp:150] Setting up Dropout10
I0905 01:04:50.351729 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.351735 73306 net.cpp:165] Memory required for data: 967573568
I0905 01:04:50.351747 73306 layer_factory.hpp:77] Creating layer Concat9
I0905 01:04:50.351770 73306 net.cpp:100] Creating Layer Concat9
I0905 01:04:50.351779 73306 net.cpp:434] Concat9 <- Concat8_Concat8_0_split_1
I0905 01:04:50.351788 73306 net.cpp:434] Concat9 <- Dropout10
I0905 01:04:50.351799 73306 net.cpp:408] Concat9 -> Concat9
I0905 01:04:50.351824 73306 net.cpp:150] Setting up Concat9
I0905 01:04:50.351835 73306 net.cpp:157] Top shape: 16 124 64 64 (8126464)
I0905 01:04:50.351841 73306 net.cpp:165] Memory required for data: 1000079424
I0905 01:04:50.351853 73306 layer_factory.hpp:77] Creating layer Concat9_Concat9_0_split
I0905 01:04:50.351861 73306 net.cpp:100] Creating Layer Concat9_Concat9_0_split
I0905 01:04:50.351868 73306 net.cpp:434] Concat9_Concat9_0_split <- Concat9
I0905 01:04:50.351891 73306 net.cpp:408] Concat9_Concat9_0_split -> Concat9_Concat9_0_split_0
I0905 01:04:50.351902 73306 net.cpp:408] Concat9_Concat9_0_split -> Concat9_Concat9_0_split_1
I0905 01:04:50.351938 73306 net.cpp:150] Setting up Concat9_Concat9_0_split
I0905 01:04:50.351948 73306 net.cpp:157] Top shape: 16 124 64 64 (8126464)
I0905 01:04:50.351955 73306 net.cpp:157] Top shape: 16 124 64 64 (8126464)
I0905 01:04:50.351961 73306 net.cpp:165] Memory required for data: 1065091136
I0905 01:04:50.351974 73306 layer_factory.hpp:77] Creating layer BatchNorm10
I0905 01:04:50.351984 73306 net.cpp:100] Creating Layer BatchNorm10
I0905 01:04:50.351997 73306 net.cpp:434] BatchNorm10 <- Concat9_Concat9_0_split_0
I0905 01:04:50.352010 73306 net.cpp:408] BatchNorm10 -> BatchNorm10
I0905 01:04:50.352174 73306 net.cpp:150] Setting up BatchNorm10
I0905 01:04:50.352185 73306 net.cpp:157] Top shape: 16 124 64 64 (8126464)
I0905 01:04:50.352192 73306 net.cpp:165] Memory required for data: 1097596992
I0905 01:04:50.352203 73306 layer_factory.hpp:77] Creating layer Scale10
I0905 01:04:50.352217 73306 net.cpp:100] Creating Layer Scale10
I0905 01:04:50.352226 73306 net.cpp:434] Scale10 <- BatchNorm10
I0905 01:04:50.352236 73306 net.cpp:395] Scale10 -> BatchNorm10 (in-place)
I0905 01:04:50.352320 73306 net.cpp:150] Setting up Scale10
I0905 01:04:50.352334 73306 net.cpp:157] Top shape: 16 124 64 64 (8126464)
I0905 01:04:50.352344 73306 net.cpp:165] Memory required for data: 1130102848
I0905 01:04:50.352352 73306 layer_factory.hpp:77] Creating layer ReLU10
I0905 01:04:50.352365 73306 net.cpp:100] Creating Layer ReLU10
I0905 01:04:50.352372 73306 net.cpp:434] ReLU10 <- BatchNorm10
I0905 01:04:50.352380 73306 net.cpp:395] ReLU10 -> BatchNorm10 (in-place)
I0905 01:04:50.352530 73306 net.cpp:150] Setting up ReLU10
I0905 01:04:50.352545 73306 net.cpp:157] Top shape: 16 124 64 64 (8126464)
I0905 01:04:50.352552 73306 net.cpp:165] Memory required for data: 1162608704
I0905 01:04:50.352560 73306 layer_factory.hpp:77] Creating layer Convolution11
I0905 01:04:50.352579 73306 net.cpp:100] Creating Layer Convolution11
I0905 01:04:50.352587 73306 net.cpp:434] Convolution11 <- BatchNorm10
I0905 01:04:50.352598 73306 net.cpp:408] Convolution11 -> Convolution11
I0905 01:04:50.353852 73306 net.cpp:150] Setting up Convolution11
I0905 01:04:50.353874 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.353883 73306 net.cpp:165] Memory required for data: 1165754432
I0905 01:04:50.353893 73306 layer_factory.hpp:77] Creating layer Dropout11
I0905 01:04:50.353904 73306 net.cpp:100] Creating Layer Dropout11
I0905 01:04:50.353911 73306 net.cpp:434] Dropout11 <- Convolution11
I0905 01:04:50.353920 73306 net.cpp:408] Dropout11 -> Dropout11
I0905 01:04:50.353965 73306 net.cpp:150] Setting up Dropout11
I0905 01:04:50.353976 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.353982 73306 net.cpp:165] Memory required for data: 1168900160
I0905 01:04:50.353989 73306 layer_factory.hpp:77] Creating layer Concat10
I0905 01:04:50.354006 73306 net.cpp:100] Creating Layer Concat10
I0905 01:04:50.354012 73306 net.cpp:434] Concat10 <- Concat9_Concat9_0_split_1
I0905 01:04:50.354020 73306 net.cpp:434] Concat10 <- Dropout11
I0905 01:04:50.354030 73306 net.cpp:408] Concat10 -> Concat10
I0905 01:04:50.354073 73306 net.cpp:150] Setting up Concat10
I0905 01:04:50.354084 73306 net.cpp:157] Top shape: 16 136 64 64 (8912896)
I0905 01:04:50.354091 73306 net.cpp:165] Memory required for data: 1204551744
I0905 01:04:50.354099 73306 layer_factory.hpp:77] Creating layer Concat10_Concat10_0_split
I0905 01:04:50.354107 73306 net.cpp:100] Creating Layer Concat10_Concat10_0_split
I0905 01:04:50.354115 73306 net.cpp:434] Concat10_Concat10_0_split <- Concat10
I0905 01:04:50.354122 73306 net.cpp:408] Concat10_Concat10_0_split -> Concat10_Concat10_0_split_0
I0905 01:04:50.354137 73306 net.cpp:408] Concat10_Concat10_0_split -> Concat10_Concat10_0_split_1
I0905 01:04:50.354176 73306 net.cpp:150] Setting up Concat10_Concat10_0_split
I0905 01:04:50.354187 73306 net.cpp:157] Top shape: 16 136 64 64 (8912896)
I0905 01:04:50.354193 73306 net.cpp:157] Top shape: 16 136 64 64 (8912896)
I0905 01:04:50.354199 73306 net.cpp:165] Memory required for data: 1275854912
I0905 01:04:50.354207 73306 layer_factory.hpp:77] Creating layer BatchNorm11
I0905 01:04:50.354218 73306 net.cpp:100] Creating Layer BatchNorm11
I0905 01:04:50.354224 73306 net.cpp:434] BatchNorm11 <- Concat10_Concat10_0_split_0
I0905 01:04:50.354233 73306 net.cpp:408] BatchNorm11 -> BatchNorm11
I0905 01:04:50.354400 73306 net.cpp:150] Setting up BatchNorm11
I0905 01:04:50.354413 73306 net.cpp:157] Top shape: 16 136 64 64 (8912896)
I0905 01:04:50.354420 73306 net.cpp:165] Memory required for data: 1311506496
I0905 01:04:50.354440 73306 layer_factory.hpp:77] Creating layer Scale11
I0905 01:04:50.354454 73306 net.cpp:100] Creating Layer Scale11
I0905 01:04:50.354461 73306 net.cpp:434] Scale11 <- BatchNorm11
I0905 01:04:50.354473 73306 net.cpp:395] Scale11 -> BatchNorm11 (in-place)
I0905 01:04:50.354558 73306 net.cpp:150] Setting up Scale11
I0905 01:04:50.354571 73306 net.cpp:157] Top shape: 16 136 64 64 (8912896)
I0905 01:04:50.354580 73306 net.cpp:165] Memory required for data: 1347158080
I0905 01:04:50.354588 73306 layer_factory.hpp:77] Creating layer ReLU11
I0905 01:04:50.354599 73306 net.cpp:100] Creating Layer ReLU11
I0905 01:04:50.354606 73306 net.cpp:434] ReLU11 <- BatchNorm11
I0905 01:04:50.354614 73306 net.cpp:395] ReLU11 -> BatchNorm11 (in-place)
I0905 01:04:50.354900 73306 net.cpp:150] Setting up ReLU11
I0905 01:04:50.354918 73306 net.cpp:157] Top shape: 16 136 64 64 (8912896)
I0905 01:04:50.354926 73306 net.cpp:165] Memory required for data: 1382809664
I0905 01:04:50.354934 73306 layer_factory.hpp:77] Creating layer Convolution12
I0905 01:04:50.354954 73306 net.cpp:100] Creating Layer Convolution12
I0905 01:04:50.354961 73306 net.cpp:434] Convolution12 <- BatchNorm11
I0905 01:04:50.354974 73306 net.cpp:408] Convolution12 -> Convolution12
I0905 01:04:50.356277 73306 net.cpp:150] Setting up Convolution12
I0905 01:04:50.356297 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.356305 73306 net.cpp:165] Memory required for data: 1385955392
I0905 01:04:50.356317 73306 layer_factory.hpp:77] Creating layer Dropout12
I0905 01:04:50.356326 73306 net.cpp:100] Creating Layer Dropout12
I0905 01:04:50.356333 73306 net.cpp:434] Dropout12 <- Convolution12
I0905 01:04:50.356343 73306 net.cpp:408] Dropout12 -> Dropout12
I0905 01:04:50.356386 73306 net.cpp:150] Setting up Dropout12
I0905 01:04:50.356397 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.356405 73306 net.cpp:165] Memory required for data: 1389101120
I0905 01:04:50.356411 73306 layer_factory.hpp:77] Creating layer Concat11
I0905 01:04:50.356420 73306 net.cpp:100] Creating Layer Concat11
I0905 01:04:50.356428 73306 net.cpp:434] Concat11 <- Concat10_Concat10_0_split_1
I0905 01:04:50.356437 73306 net.cpp:434] Concat11 <- Dropout12
I0905 01:04:50.356446 73306 net.cpp:408] Concat11 -> Concat11
I0905 01:04:50.356472 73306 net.cpp:150] Setting up Concat11
I0905 01:04:50.356483 73306 net.cpp:157] Top shape: 16 148 64 64 (9699328)
I0905 01:04:50.356489 73306 net.cpp:165] Memory required for data: 1427898432
I0905 01:04:50.356497 73306 layer_factory.hpp:77] Creating layer Concat11_Concat11_0_split
I0905 01:04:50.356516 73306 net.cpp:100] Creating Layer Concat11_Concat11_0_split
I0905 01:04:50.356524 73306 net.cpp:434] Concat11_Concat11_0_split <- Concat11
I0905 01:04:50.356539 73306 net.cpp:408] Concat11_Concat11_0_split -> Concat11_Concat11_0_split_0
I0905 01:04:50.356559 73306 net.cpp:408] Concat11_Concat11_0_split -> Concat11_Concat11_0_split_1
I0905 01:04:50.356596 73306 net.cpp:150] Setting up Concat11_Concat11_0_split
I0905 01:04:50.356607 73306 net.cpp:157] Top shape: 16 148 64 64 (9699328)
I0905 01:04:50.356614 73306 net.cpp:157] Top shape: 16 148 64 64 (9699328)
I0905 01:04:50.356621 73306 net.cpp:165] Memory required for data: 1505493056
I0905 01:04:50.356628 73306 layer_factory.hpp:77] Creating layer BatchNorm12
I0905 01:04:50.356643 73306 net.cpp:100] Creating Layer BatchNorm12
I0905 01:04:50.356650 73306 net.cpp:434] BatchNorm12 <- Concat11_Concat11_0_split_0
I0905 01:04:50.356662 73306 net.cpp:408] BatchNorm12 -> BatchNorm12
I0905 01:04:50.356829 73306 net.cpp:150] Setting up BatchNorm12
I0905 01:04:50.356845 73306 net.cpp:157] Top shape: 16 148 64 64 (9699328)
I0905 01:04:50.356853 73306 net.cpp:165] Memory required for data: 1544290368
I0905 01:04:50.356864 73306 layer_factory.hpp:77] Creating layer Scale12
I0905 01:04:50.356875 73306 net.cpp:100] Creating Layer Scale12
I0905 01:04:50.356883 73306 net.cpp:434] Scale12 <- BatchNorm12
I0905 01:04:50.356891 73306 net.cpp:395] Scale12 -> BatchNorm12 (in-place)
I0905 01:04:50.356971 73306 net.cpp:150] Setting up Scale12
I0905 01:04:50.356988 73306 net.cpp:157] Top shape: 16 148 64 64 (9699328)
I0905 01:04:50.356995 73306 net.cpp:165] Memory required for data: 1583087680
I0905 01:04:50.357004 73306 layer_factory.hpp:77] Creating layer ReLU12
I0905 01:04:50.357015 73306 net.cpp:100] Creating Layer ReLU12
I0905 01:04:50.357023 73306 net.cpp:434] ReLU12 <- BatchNorm12
I0905 01:04:50.357030 73306 net.cpp:395] ReLU12 -> BatchNorm12 (in-place)
I0905 01:04:50.357187 73306 net.cpp:150] Setting up ReLU12
I0905 01:04:50.357203 73306 net.cpp:157] Top shape: 16 148 64 64 (9699328)
I0905 01:04:50.357210 73306 net.cpp:165] Memory required for data: 1621884992
I0905 01:04:50.357218 73306 layer_factory.hpp:77] Creating layer Convolution13
I0905 01:04:50.357236 73306 net.cpp:100] Creating Layer Convolution13
I0905 01:04:50.357244 73306 net.cpp:434] Convolution13 <- BatchNorm12
I0905 01:04:50.357254 73306 net.cpp:408] Convolution13 -> Convolution13
I0905 01:04:50.358587 73306 net.cpp:150] Setting up Convolution13
I0905 01:04:50.358605 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.358614 73306 net.cpp:165] Memory required for data: 1625030720
I0905 01:04:50.358624 73306 layer_factory.hpp:77] Creating layer Dropout13
I0905 01:04:50.358656 73306 net.cpp:100] Creating Layer Dropout13
I0905 01:04:50.358666 73306 net.cpp:434] Dropout13 <- Convolution13
I0905 01:04:50.358677 73306 net.cpp:408] Dropout13 -> Dropout13
I0905 01:04:50.358736 73306 net.cpp:150] Setting up Dropout13
I0905 01:04:50.358750 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.358759 73306 net.cpp:165] Memory required for data: 1628176448
I0905 01:04:50.358767 73306 layer_factory.hpp:77] Creating layer Concat12
I0905 01:04:50.358775 73306 net.cpp:100] Creating Layer Concat12
I0905 01:04:50.358783 73306 net.cpp:434] Concat12 <- Concat11_Concat11_0_split_1
I0905 01:04:50.358790 73306 net.cpp:434] Concat12 <- Dropout13
I0905 01:04:50.358799 73306 net.cpp:408] Concat12 -> Concat12
I0905 01:04:50.358831 73306 net.cpp:150] Setting up Concat12
I0905 01:04:50.358845 73306 net.cpp:157] Top shape: 16 160 64 64 (10485760)
I0905 01:04:50.358852 73306 net.cpp:165] Memory required for data: 1670119488
I0905 01:04:50.358860 73306 layer_factory.hpp:77] Creating layer BatchNorm13
I0905 01:04:50.358870 73306 net.cpp:100] Creating Layer BatchNorm13
I0905 01:04:50.358877 73306 net.cpp:434] BatchNorm13 <- Concat12
I0905 01:04:50.358886 73306 net.cpp:408] BatchNorm13 -> BatchNorm13
I0905 01:04:50.359055 73306 net.cpp:150] Setting up BatchNorm13
I0905 01:04:50.359068 73306 net.cpp:157] Top shape: 16 160 64 64 (10485760)
I0905 01:04:50.359087 73306 net.cpp:165] Memory required for data: 1712062528
I0905 01:04:50.359099 73306 layer_factory.hpp:77] Creating layer Scale13
I0905 01:04:50.359112 73306 net.cpp:100] Creating Layer Scale13
I0905 01:04:50.359122 73306 net.cpp:434] Scale13 <- BatchNorm13
I0905 01:04:50.359129 73306 net.cpp:395] Scale13 -> BatchNorm13 (in-place)
I0905 01:04:50.359215 73306 net.cpp:150] Setting up Scale13
I0905 01:04:50.359227 73306 net.cpp:157] Top shape: 16 160 64 64 (10485760)
I0905 01:04:50.359236 73306 net.cpp:165] Memory required for data: 1754005568
I0905 01:04:50.359244 73306 layer_factory.hpp:77] Creating layer ReLU13
I0905 01:04:50.359257 73306 net.cpp:100] Creating Layer ReLU13
I0905 01:04:50.359264 73306 net.cpp:434] ReLU13 <- BatchNorm13
I0905 01:04:50.359274 73306 net.cpp:395] ReLU13 -> BatchNorm13 (in-place)
I0905 01:04:50.359422 73306 net.cpp:150] Setting up ReLU13
I0905 01:04:50.359441 73306 net.cpp:157] Top shape: 16 160 64 64 (10485760)
I0905 01:04:50.359448 73306 net.cpp:165] Memory required for data: 1795948608
I0905 01:04:50.359457 73306 layer_factory.hpp:77] Creating layer Convolution14
I0905 01:04:50.359470 73306 net.cpp:100] Creating Layer Convolution14
I0905 01:04:50.359477 73306 net.cpp:434] Convolution14 <- BatchNorm13
I0905 01:04:50.359488 73306 net.cpp:408] Convolution14 -> Convolution14
I0905 01:04:50.361822 73306 net.cpp:150] Setting up Convolution14
I0905 01:04:50.361843 73306 net.cpp:157] Top shape: 16 160 64 64 (10485760)
I0905 01:04:50.361852 73306 net.cpp:165] Memory required for data: 1837891648
I0905 01:04:50.361865 73306 layer_factory.hpp:77] Creating layer Dropout14
I0905 01:04:50.361876 73306 net.cpp:100] Creating Layer Dropout14
I0905 01:04:50.361883 73306 net.cpp:434] Dropout14 <- Convolution14
I0905 01:04:50.361892 73306 net.cpp:408] Dropout14 -> Dropout14
I0905 01:04:50.361939 73306 net.cpp:150] Setting up Dropout14
I0905 01:04:50.361950 73306 net.cpp:157] Top shape: 16 160 64 64 (10485760)
I0905 01:04:50.361958 73306 net.cpp:165] Memory required for data: 1879834688
I0905 01:04:50.361965 73306 layer_factory.hpp:77] Creating layer Pooling1
I0905 01:04:50.361976 73306 net.cpp:100] Creating Layer Pooling1
I0905 01:04:50.361984 73306 net.cpp:434] Pooling1 <- Dropout14
I0905 01:04:50.361994 73306 net.cpp:408] Pooling1 -> Pooling1
I0905 01:04:50.362416 73306 net.cpp:150] Setting up Pooling1
I0905 01:04:50.362434 73306 net.cpp:157] Top shape: 16 160 32 32 (2621440)
I0905 01:04:50.362442 73306 net.cpp:165] Memory required for data: 1890320448
I0905 01:04:50.362450 73306 layer_factory.hpp:77] Creating layer Pooling1_Pooling1_0_split
I0905 01:04:50.362462 73306 net.cpp:100] Creating Layer Pooling1_Pooling1_0_split
I0905 01:04:50.362470 73306 net.cpp:434] Pooling1_Pooling1_0_split <- Pooling1
I0905 01:04:50.362478 73306 net.cpp:408] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_0
I0905 01:04:50.362488 73306 net.cpp:408] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_1
I0905 01:04:50.362530 73306 net.cpp:150] Setting up Pooling1_Pooling1_0_split
I0905 01:04:50.362541 73306 net.cpp:157] Top shape: 16 160 32 32 (2621440)
I0905 01:04:50.362548 73306 net.cpp:157] Top shape: 16 160 32 32 (2621440)
I0905 01:04:50.362562 73306 net.cpp:165] Memory required for data: 1911291968
I0905 01:04:50.362570 73306 layer_factory.hpp:77] Creating layer BatchNorm14
I0905 01:04:50.362586 73306 net.cpp:100] Creating Layer BatchNorm14
I0905 01:04:50.362593 73306 net.cpp:434] BatchNorm14 <- Pooling1_Pooling1_0_split_0
I0905 01:04:50.362604 73306 net.cpp:408] BatchNorm14 -> BatchNorm14
I0905 01:04:50.362797 73306 net.cpp:150] Setting up BatchNorm14
I0905 01:04:50.362812 73306 net.cpp:157] Top shape: 16 160 32 32 (2621440)
I0905 01:04:50.362818 73306 net.cpp:165] Memory required for data: 1921777728
I0905 01:04:50.362829 73306 layer_factory.hpp:77] Creating layer Scale14
I0905 01:04:50.362839 73306 net.cpp:100] Creating Layer Scale14
I0905 01:04:50.362848 73306 net.cpp:434] Scale14 <- BatchNorm14
I0905 01:04:50.362855 73306 net.cpp:395] Scale14 -> BatchNorm14 (in-place)
I0905 01:04:50.362965 73306 net.cpp:150] Setting up Scale14
I0905 01:04:50.362979 73306 net.cpp:157] Top shape: 16 160 32 32 (2621440)
I0905 01:04:50.362987 73306 net.cpp:165] Memory required for data: 1932263488
I0905 01:04:50.362994 73306 layer_factory.hpp:77] Creating layer ReLU14
I0905 01:04:50.363004 73306 net.cpp:100] Creating Layer ReLU14
I0905 01:04:50.363011 73306 net.cpp:434] ReLU14 <- BatchNorm14
I0905 01:04:50.363023 73306 net.cpp:395] ReLU14 -> BatchNorm14 (in-place)
I0905 01:04:50.363176 73306 net.cpp:150] Setting up ReLU14
I0905 01:04:50.363193 73306 net.cpp:157] Top shape: 16 160 32 32 (2621440)
I0905 01:04:50.363199 73306 net.cpp:165] Memory required for data: 1942749248
I0905 01:04:50.363207 73306 layer_factory.hpp:77] Creating layer Convolution15
I0905 01:04:50.363225 73306 net.cpp:100] Creating Layer Convolution15
I0905 01:04:50.363234 73306 net.cpp:434] Convolution15 <- BatchNorm14
I0905 01:04:50.363243 73306 net.cpp:408] Convolution15 -> Convolution15
I0905 01:04:50.364648 73306 net.cpp:150] Setting up Convolution15
I0905 01:04:50.364670 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.364677 73306 net.cpp:165] Memory required for data: 1943535680
I0905 01:04:50.364687 73306 layer_factory.hpp:77] Creating layer Dropout15
I0905 01:04:50.364698 73306 net.cpp:100] Creating Layer Dropout15
I0905 01:04:50.364706 73306 net.cpp:434] Dropout15 <- Convolution15
I0905 01:04:50.364717 73306 net.cpp:408] Dropout15 -> Dropout15
I0905 01:04:50.364759 73306 net.cpp:150] Setting up Dropout15
I0905 01:04:50.364773 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.364780 73306 net.cpp:165] Memory required for data: 1944322112
I0905 01:04:50.364787 73306 layer_factory.hpp:77] Creating layer Concat13
I0905 01:04:50.364804 73306 net.cpp:100] Creating Layer Concat13
I0905 01:04:50.364810 73306 net.cpp:434] Concat13 <- Pooling1_Pooling1_0_split_1
I0905 01:04:50.364820 73306 net.cpp:434] Concat13 <- Dropout15
I0905 01:04:50.364827 73306 net.cpp:408] Concat13 -> Concat13
I0905 01:04:50.364856 73306 net.cpp:150] Setting up Concat13
I0905 01:04:50.364866 73306 net.cpp:157] Top shape: 16 172 32 32 (2818048)
I0905 01:04:50.364872 73306 net.cpp:165] Memory required for data: 1955594304
I0905 01:04:50.364879 73306 layer_factory.hpp:77] Creating layer Concat13_Concat13_0_split
I0905 01:04:50.364887 73306 net.cpp:100] Creating Layer Concat13_Concat13_0_split
I0905 01:04:50.364894 73306 net.cpp:434] Concat13_Concat13_0_split <- Concat13
I0905 01:04:50.364904 73306 net.cpp:408] Concat13_Concat13_0_split -> Concat13_Concat13_0_split_0
I0905 01:04:50.364913 73306 net.cpp:408] Concat13_Concat13_0_split -> Concat13_Concat13_0_split_1
I0905 01:04:50.364951 73306 net.cpp:150] Setting up Concat13_Concat13_0_split
I0905 01:04:50.364962 73306 net.cpp:157] Top shape: 16 172 32 32 (2818048)
I0905 01:04:50.364969 73306 net.cpp:157] Top shape: 16 172 32 32 (2818048)
I0905 01:04:50.364976 73306 net.cpp:165] Memory required for data: 1978138688
I0905 01:04:50.364984 73306 layer_factory.hpp:77] Creating layer BatchNorm15
I0905 01:04:50.364992 73306 net.cpp:100] Creating Layer BatchNorm15
I0905 01:04:50.365000 73306 net.cpp:434] BatchNorm15 <- Concat13_Concat13_0_split_0
I0905 01:04:50.365010 73306 net.cpp:408] BatchNorm15 -> BatchNorm15
I0905 01:04:50.365191 73306 net.cpp:150] Setting up BatchNorm15
I0905 01:04:50.365203 73306 net.cpp:157] Top shape: 16 172 32 32 (2818048)
I0905 01:04:50.365211 73306 net.cpp:165] Memory required for data: 1989410880
I0905 01:04:50.365221 73306 layer_factory.hpp:77] Creating layer Scale15
I0905 01:04:50.365237 73306 net.cpp:100] Creating Layer Scale15
I0905 01:04:50.365247 73306 net.cpp:434] Scale15 <- BatchNorm15
I0905 01:04:50.365255 73306 net.cpp:395] Scale15 -> BatchNorm15 (in-place)
I0905 01:04:50.365342 73306 net.cpp:150] Setting up Scale15
I0905 01:04:50.365355 73306 net.cpp:157] Top shape: 16 172 32 32 (2818048)
I0905 01:04:50.365361 73306 net.cpp:165] Memory required for data: 2000683072
I0905 01:04:50.365370 73306 layer_factory.hpp:77] Creating layer ReLU15
I0905 01:04:50.365393 73306 net.cpp:100] Creating Layer ReLU15
I0905 01:04:50.365401 73306 net.cpp:434] ReLU15 <- BatchNorm15
I0905 01:04:50.365408 73306 net.cpp:395] ReLU15 -> BatchNorm15 (in-place)
I0905 01:04:50.365701 73306 net.cpp:150] Setting up ReLU15
I0905 01:04:50.365720 73306 net.cpp:157] Top shape: 16 172 32 32 (2818048)
I0905 01:04:50.365727 73306 net.cpp:165] Memory required for data: 2011955264
I0905 01:04:50.365734 73306 layer_factory.hpp:77] Creating layer Convolution16
I0905 01:04:50.365751 73306 net.cpp:100] Creating Layer Convolution16
I0905 01:04:50.365758 73306 net.cpp:434] Convolution16 <- BatchNorm15
I0905 01:04:50.365768 73306 net.cpp:408] Convolution16 -> Convolution16
I0905 01:04:50.367246 73306 net.cpp:150] Setting up Convolution16
I0905 01:04:50.367269 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.367276 73306 net.cpp:165] Memory required for data: 2012741696
I0905 01:04:50.367290 73306 layer_factory.hpp:77] Creating layer Dropout16
I0905 01:04:50.367300 73306 net.cpp:100] Creating Layer Dropout16
I0905 01:04:50.367307 73306 net.cpp:434] Dropout16 <- Convolution16
I0905 01:04:50.367316 73306 net.cpp:408] Dropout16 -> Dropout16
I0905 01:04:50.367364 73306 net.cpp:150] Setting up Dropout16
I0905 01:04:50.367377 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.367383 73306 net.cpp:165] Memory required for data: 2013528128
I0905 01:04:50.367390 73306 layer_factory.hpp:77] Creating layer Concat14
I0905 01:04:50.367400 73306 net.cpp:100] Creating Layer Concat14
I0905 01:04:50.367408 73306 net.cpp:434] Concat14 <- Concat13_Concat13_0_split_1
I0905 01:04:50.367415 73306 net.cpp:434] Concat14 <- Dropout16
I0905 01:04:50.367425 73306 net.cpp:408] Concat14 -> Concat14
I0905 01:04:50.367452 73306 net.cpp:150] Setting up Concat14
I0905 01:04:50.367463 73306 net.cpp:157] Top shape: 16 184 32 32 (3014656)
I0905 01:04:50.367470 73306 net.cpp:165] Memory required for data: 2025586752
I0905 01:04:50.367476 73306 layer_factory.hpp:77] Creating layer Concat14_Concat14_0_split
I0905 01:04:50.367485 73306 net.cpp:100] Creating Layer Concat14_Concat14_0_split
I0905 01:04:50.367491 73306 net.cpp:434] Concat14_Concat14_0_split <- Concat14
I0905 01:04:50.367499 73306 net.cpp:408] Concat14_Concat14_0_split -> Concat14_Concat14_0_split_0
I0905 01:04:50.367508 73306 net.cpp:408] Concat14_Concat14_0_split -> Concat14_Concat14_0_split_1
I0905 01:04:50.367547 73306 net.cpp:150] Setting up Concat14_Concat14_0_split
I0905 01:04:50.367558 73306 net.cpp:157] Top shape: 16 184 32 32 (3014656)
I0905 01:04:50.367564 73306 net.cpp:157] Top shape: 16 184 32 32 (3014656)
I0905 01:04:50.367570 73306 net.cpp:165] Memory required for data: 2049704000
I0905 01:04:50.367578 73306 layer_factory.hpp:77] Creating layer BatchNorm16
I0905 01:04:50.367588 73306 net.cpp:100] Creating Layer BatchNorm16
I0905 01:04:50.367594 73306 net.cpp:434] BatchNorm16 <- Concat14_Concat14_0_split_0
I0905 01:04:50.367605 73306 net.cpp:408] BatchNorm16 -> BatchNorm16
I0905 01:04:50.367779 73306 net.cpp:150] Setting up BatchNorm16
I0905 01:04:50.367791 73306 net.cpp:157] Top shape: 16 184 32 32 (3014656)
I0905 01:04:50.367799 73306 net.cpp:165] Memory required for data: 2061762624
I0905 01:04:50.367810 73306 layer_factory.hpp:77] Creating layer Scale16
I0905 01:04:50.367822 73306 net.cpp:100] Creating Layer Scale16
I0905 01:04:50.367830 73306 net.cpp:434] Scale16 <- BatchNorm16
I0905 01:04:50.367838 73306 net.cpp:395] Scale16 -> BatchNorm16 (in-place)
I0905 01:04:50.367925 73306 net.cpp:150] Setting up Scale16
I0905 01:04:50.367939 73306 net.cpp:157] Top shape: 16 184 32 32 (3014656)
I0905 01:04:50.367949 73306 net.cpp:165] Memory required for data: 2073821248
I0905 01:04:50.367956 73306 layer_factory.hpp:77] Creating layer ReLU16
I0905 01:04:50.367970 73306 net.cpp:100] Creating Layer ReLU16
I0905 01:04:50.367977 73306 net.cpp:434] ReLU16 <- BatchNorm16
I0905 01:04:50.367985 73306 net.cpp:395] ReLU16 -> BatchNorm16 (in-place)
I0905 01:04:50.368275 73306 net.cpp:150] Setting up ReLU16
I0905 01:04:50.368296 73306 net.cpp:157] Top shape: 16 184 32 32 (3014656)
I0905 01:04:50.368315 73306 net.cpp:165] Memory required for data: 2085879872
I0905 01:04:50.368326 73306 layer_factory.hpp:77] Creating layer Convolution17
I0905 01:04:50.368342 73306 net.cpp:100] Creating Layer Convolution17
I0905 01:04:50.368350 73306 net.cpp:434] Convolution17 <- BatchNorm16
I0905 01:04:50.368360 73306 net.cpp:408] Convolution17 -> Convolution17
I0905 01:04:50.369846 73306 net.cpp:150] Setting up Convolution17
I0905 01:04:50.369865 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.369874 73306 net.cpp:165] Memory required for data: 2086666304
I0905 01:04:50.369884 73306 layer_factory.hpp:77] Creating layer Dropout17
I0905 01:04:50.369894 73306 net.cpp:100] Creating Layer Dropout17
I0905 01:04:50.369902 73306 net.cpp:434] Dropout17 <- Convolution17
I0905 01:04:50.369910 73306 net.cpp:408] Dropout17 -> Dropout17
I0905 01:04:50.369957 73306 net.cpp:150] Setting up Dropout17
I0905 01:04:50.369969 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.369976 73306 net.cpp:165] Memory required for data: 2087452736
I0905 01:04:50.369988 73306 layer_factory.hpp:77] Creating layer Concat15
I0905 01:04:50.369998 73306 net.cpp:100] Creating Layer Concat15
I0905 01:04:50.370005 73306 net.cpp:434] Concat15 <- Concat14_Concat14_0_split_1
I0905 01:04:50.370015 73306 net.cpp:434] Concat15 <- Dropout17
I0905 01:04:50.370024 73306 net.cpp:408] Concat15 -> Concat15
I0905 01:04:50.370054 73306 net.cpp:150] Setting up Concat15
I0905 01:04:50.370064 73306 net.cpp:157] Top shape: 16 196 32 32 (3211264)
I0905 01:04:50.370071 73306 net.cpp:165] Memory required for data: 2100297792
I0905 01:04:50.370077 73306 layer_factory.hpp:77] Creating layer Concat15_Concat15_0_split
I0905 01:04:50.370086 73306 net.cpp:100] Creating Layer Concat15_Concat15_0_split
I0905 01:04:50.370092 73306 net.cpp:434] Concat15_Concat15_0_split <- Concat15
I0905 01:04:50.370103 73306 net.cpp:408] Concat15_Concat15_0_split -> Concat15_Concat15_0_split_0
I0905 01:04:50.370113 73306 net.cpp:408] Concat15_Concat15_0_split -> Concat15_Concat15_0_split_1
I0905 01:04:50.370149 73306 net.cpp:150] Setting up Concat15_Concat15_0_split
I0905 01:04:50.370159 73306 net.cpp:157] Top shape: 16 196 32 32 (3211264)
I0905 01:04:50.370167 73306 net.cpp:157] Top shape: 16 196 32 32 (3211264)
I0905 01:04:50.370173 73306 net.cpp:165] Memory required for data: 2125987904
I0905 01:04:50.370179 73306 layer_factory.hpp:77] Creating layer BatchNorm17
I0905 01:04:50.370190 73306 net.cpp:100] Creating Layer BatchNorm17
I0905 01:04:50.370198 73306 net.cpp:434] BatchNorm17 <- Concat15_Concat15_0_split_0
I0905 01:04:50.370211 73306 net.cpp:408] BatchNorm17 -> BatchNorm17
I0905 01:04:50.370390 73306 net.cpp:150] Setting up BatchNorm17
I0905 01:04:50.370404 73306 net.cpp:157] Top shape: 16 196 32 32 (3211264)
I0905 01:04:50.370411 73306 net.cpp:165] Memory required for data: 2138832960
I0905 01:04:50.370422 73306 layer_factory.hpp:77] Creating layer Scale17
I0905 01:04:50.370434 73306 net.cpp:100] Creating Layer Scale17
I0905 01:04:50.370441 73306 net.cpp:434] Scale17 <- BatchNorm17
I0905 01:04:50.370450 73306 net.cpp:395] Scale17 -> BatchNorm17 (in-place)
I0905 01:04:50.370539 73306 net.cpp:150] Setting up Scale17
I0905 01:04:50.370553 73306 net.cpp:157] Top shape: 16 196 32 32 (3211264)
I0905 01:04:50.370560 73306 net.cpp:165] Memory required for data: 2151678016
I0905 01:04:50.370569 73306 layer_factory.hpp:77] Creating layer ReLU17
I0905 01:04:50.370581 73306 net.cpp:100] Creating Layer ReLU17
I0905 01:04:50.370589 73306 net.cpp:434] ReLU17 <- BatchNorm17
I0905 01:04:50.370596 73306 net.cpp:395] ReLU17 -> BatchNorm17 (in-place)
I0905 01:04:50.370766 73306 net.cpp:150] Setting up ReLU17
I0905 01:04:50.370782 73306 net.cpp:157] Top shape: 16 196 32 32 (3211264)
I0905 01:04:50.370790 73306 net.cpp:165] Memory required for data: 2164523072
I0905 01:04:50.370797 73306 layer_factory.hpp:77] Creating layer Convolution18
I0905 01:04:50.370815 73306 net.cpp:100] Creating Layer Convolution18
I0905 01:04:50.370822 73306 net.cpp:434] Convolution18 <- BatchNorm17
I0905 01:04:50.370846 73306 net.cpp:408] Convolution18 -> Convolution18
I0905 01:04:50.372351 73306 net.cpp:150] Setting up Convolution18
I0905 01:04:50.372371 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.372380 73306 net.cpp:165] Memory required for data: 2165309504
I0905 01:04:50.372390 73306 layer_factory.hpp:77] Creating layer Dropout18
I0905 01:04:50.372400 73306 net.cpp:100] Creating Layer Dropout18
I0905 01:04:50.372408 73306 net.cpp:434] Dropout18 <- Convolution18
I0905 01:04:50.372416 73306 net.cpp:408] Dropout18 -> Dropout18
I0905 01:04:50.372463 73306 net.cpp:150] Setting up Dropout18
I0905 01:04:50.372474 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.372481 73306 net.cpp:165] Memory required for data: 2166095936
I0905 01:04:50.372488 73306 layer_factory.hpp:77] Creating layer Concat16
I0905 01:04:50.372501 73306 net.cpp:100] Creating Layer Concat16
I0905 01:04:50.372509 73306 net.cpp:434] Concat16 <- Concat15_Concat15_0_split_1
I0905 01:04:50.372517 73306 net.cpp:434] Concat16 <- Dropout18
I0905 01:04:50.372525 73306 net.cpp:408] Concat16 -> Concat16
I0905 01:04:50.372555 73306 net.cpp:150] Setting up Concat16
I0905 01:04:50.372565 73306 net.cpp:157] Top shape: 16 208 32 32 (3407872)
I0905 01:04:50.372571 73306 net.cpp:165] Memory required for data: 2179727424
I0905 01:04:50.372580 73306 layer_factory.hpp:77] Creating layer Concat16_Concat16_0_split
I0905 01:04:50.372588 73306 net.cpp:100] Creating Layer Concat16_Concat16_0_split
I0905 01:04:50.372596 73306 net.cpp:434] Concat16_Concat16_0_split <- Concat16
I0905 01:04:50.372606 73306 net.cpp:408] Concat16_Concat16_0_split -> Concat16_Concat16_0_split_0
I0905 01:04:50.372619 73306 net.cpp:408] Concat16_Concat16_0_split -> Concat16_Concat16_0_split_1
I0905 01:04:50.372655 73306 net.cpp:150] Setting up Concat16_Concat16_0_split
I0905 01:04:50.372665 73306 net.cpp:157] Top shape: 16 208 32 32 (3407872)
I0905 01:04:50.372673 73306 net.cpp:157] Top shape: 16 208 32 32 (3407872)
I0905 01:04:50.372679 73306 net.cpp:165] Memory required for data: 2206990400
I0905 01:04:50.372686 73306 layer_factory.hpp:77] Creating layer BatchNorm18
I0905 01:04:50.372696 73306 net.cpp:100] Creating Layer BatchNorm18
I0905 01:04:50.372704 73306 net.cpp:434] BatchNorm18 <- Concat16_Concat16_0_split_0
I0905 01:04:50.372716 73306 net.cpp:408] BatchNorm18 -> BatchNorm18
I0905 01:04:50.372897 73306 net.cpp:150] Setting up BatchNorm18
I0905 01:04:50.372910 73306 net.cpp:157] Top shape: 16 208 32 32 (3407872)
I0905 01:04:50.372918 73306 net.cpp:165] Memory required for data: 2220621888
I0905 01:04:50.372928 73306 layer_factory.hpp:77] Creating layer Scale18
I0905 01:04:50.372939 73306 net.cpp:100] Creating Layer Scale18
I0905 01:04:50.372946 73306 net.cpp:434] Scale18 <- BatchNorm18
I0905 01:04:50.372958 73306 net.cpp:395] Scale18 -> BatchNorm18 (in-place)
I0905 01:04:50.373044 73306 net.cpp:150] Setting up Scale18
I0905 01:04:50.373056 73306 net.cpp:157] Top shape: 16 208 32 32 (3407872)
I0905 01:04:50.373065 73306 net.cpp:165] Memory required for data: 2234253376
I0905 01:04:50.373073 73306 layer_factory.hpp:77] Creating layer ReLU18
I0905 01:04:50.373085 73306 net.cpp:100] Creating Layer ReLU18
I0905 01:04:50.373092 73306 net.cpp:434] ReLU18 <- BatchNorm18
I0905 01:04:50.373101 73306 net.cpp:395] ReLU18 -> BatchNorm18 (in-place)
I0905 01:04:50.373400 73306 net.cpp:150] Setting up ReLU18
I0905 01:04:50.373419 73306 net.cpp:157] Top shape: 16 208 32 32 (3407872)
I0905 01:04:50.373426 73306 net.cpp:165] Memory required for data: 2247884864
I0905 01:04:50.373435 73306 layer_factory.hpp:77] Creating layer Convolution19
I0905 01:04:50.373450 73306 net.cpp:100] Creating Layer Convolution19
I0905 01:04:50.373457 73306 net.cpp:434] Convolution19 <- BatchNorm18
I0905 01:04:50.373467 73306 net.cpp:408] Convolution19 -> Convolution19
I0905 01:04:50.374889 73306 net.cpp:150] Setting up Convolution19
I0905 01:04:50.374909 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.374917 73306 net.cpp:165] Memory required for data: 2248671296
I0905 01:04:50.374940 73306 layer_factory.hpp:77] Creating layer Dropout19
I0905 01:04:50.374956 73306 net.cpp:100] Creating Layer Dropout19
I0905 01:04:50.374965 73306 net.cpp:434] Dropout19 <- Convolution19
I0905 01:04:50.374974 73306 net.cpp:408] Dropout19 -> Dropout19
I0905 01:04:50.375020 73306 net.cpp:150] Setting up Dropout19
I0905 01:04:50.375031 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.375039 73306 net.cpp:165] Memory required for data: 2249457728
I0905 01:04:50.375046 73306 layer_factory.hpp:77] Creating layer Concat17
I0905 01:04:50.375058 73306 net.cpp:100] Creating Layer Concat17
I0905 01:04:50.375067 73306 net.cpp:434] Concat17 <- Concat16_Concat16_0_split_1
I0905 01:04:50.375075 73306 net.cpp:434] Concat17 <- Dropout19
I0905 01:04:50.375083 73306 net.cpp:408] Concat17 -> Concat17
I0905 01:04:50.375111 73306 net.cpp:150] Setting up Concat17
I0905 01:04:50.375121 73306 net.cpp:157] Top shape: 16 220 32 32 (3604480)
I0905 01:04:50.375130 73306 net.cpp:165] Memory required for data: 2263875648
I0905 01:04:50.375136 73306 layer_factory.hpp:77] Creating layer Concat17_Concat17_0_split
I0905 01:04:50.375156 73306 net.cpp:100] Creating Layer Concat17_Concat17_0_split
I0905 01:04:50.375165 73306 net.cpp:434] Concat17_Concat17_0_split <- Concat17
I0905 01:04:50.375180 73306 net.cpp:408] Concat17_Concat17_0_split -> Concat17_Concat17_0_split_0
I0905 01:04:50.375190 73306 net.cpp:408] Concat17_Concat17_0_split -> Concat17_Concat17_0_split_1
I0905 01:04:50.375229 73306 net.cpp:150] Setting up Concat17_Concat17_0_split
I0905 01:04:50.375241 73306 net.cpp:157] Top shape: 16 220 32 32 (3604480)
I0905 01:04:50.375247 73306 net.cpp:157] Top shape: 16 220 32 32 (3604480)
I0905 01:04:50.375262 73306 net.cpp:165] Memory required for data: 2292711488
I0905 01:04:50.375269 73306 layer_factory.hpp:77] Creating layer BatchNorm19
I0905 01:04:50.375285 73306 net.cpp:100] Creating Layer BatchNorm19
I0905 01:04:50.375293 73306 net.cpp:434] BatchNorm19 <- Concat17_Concat17_0_split_0
I0905 01:04:50.375303 73306 net.cpp:408] BatchNorm19 -> BatchNorm19
I0905 01:04:50.375493 73306 net.cpp:150] Setting up BatchNorm19
I0905 01:04:50.375506 73306 net.cpp:157] Top shape: 16 220 32 32 (3604480)
I0905 01:04:50.375514 73306 net.cpp:165] Memory required for data: 2307129408
I0905 01:04:50.375524 73306 layer_factory.hpp:77] Creating layer Scale19
I0905 01:04:50.375535 73306 net.cpp:100] Creating Layer Scale19
I0905 01:04:50.375541 73306 net.cpp:434] Scale19 <- BatchNorm19
I0905 01:04:50.375550 73306 net.cpp:395] Scale19 -> BatchNorm19 (in-place)
I0905 01:04:50.375644 73306 net.cpp:150] Setting up Scale19
I0905 01:04:50.375658 73306 net.cpp:157] Top shape: 16 220 32 32 (3604480)
I0905 01:04:50.375669 73306 net.cpp:165] Memory required for data: 2321547328
I0905 01:04:50.375676 73306 layer_factory.hpp:77] Creating layer ReLU19
I0905 01:04:50.375689 73306 net.cpp:100] Creating Layer ReLU19
I0905 01:04:50.375695 73306 net.cpp:434] ReLU19 <- BatchNorm19
I0905 01:04:50.375707 73306 net.cpp:395] ReLU19 -> BatchNorm19 (in-place)
I0905 01:04:50.375998 73306 net.cpp:150] Setting up ReLU19
I0905 01:04:50.376016 73306 net.cpp:157] Top shape: 16 220 32 32 (3604480)
I0905 01:04:50.376024 73306 net.cpp:165] Memory required for data: 2335965248
I0905 01:04:50.376031 73306 layer_factory.hpp:77] Creating layer Convolution20
I0905 01:04:50.376047 73306 net.cpp:100] Creating Layer Convolution20
I0905 01:04:50.376055 73306 net.cpp:434] Convolution20 <- BatchNorm19
I0905 01:04:50.376066 73306 net.cpp:408] Convolution20 -> Convolution20
I0905 01:04:50.377650 73306 net.cpp:150] Setting up Convolution20
I0905 01:04:50.377672 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.377681 73306 net.cpp:165] Memory required for data: 2336751680
I0905 01:04:50.377692 73306 layer_factory.hpp:77] Creating layer Dropout20
I0905 01:04:50.377702 73306 net.cpp:100] Creating Layer Dropout20
I0905 01:04:50.377710 73306 net.cpp:434] Dropout20 <- Convolution20
I0905 01:04:50.377718 73306 net.cpp:408] Dropout20 -> Dropout20
I0905 01:04:50.377774 73306 net.cpp:150] Setting up Dropout20
I0905 01:04:50.377795 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.377804 73306 net.cpp:165] Memory required for data: 2337538112
I0905 01:04:50.377810 73306 layer_factory.hpp:77] Creating layer Concat18
I0905 01:04:50.377823 73306 net.cpp:100] Creating Layer Concat18
I0905 01:04:50.377830 73306 net.cpp:434] Concat18 <- Concat17_Concat17_0_split_1
I0905 01:04:50.377840 73306 net.cpp:434] Concat18 <- Dropout20
I0905 01:04:50.377849 73306 net.cpp:408] Concat18 -> Concat18
I0905 01:04:50.377877 73306 net.cpp:150] Setting up Concat18
I0905 01:04:50.377897 73306 net.cpp:157] Top shape: 16 232 32 32 (3801088)
I0905 01:04:50.377902 73306 net.cpp:165] Memory required for data: 2352742464
I0905 01:04:50.377909 73306 layer_factory.hpp:77] Creating layer Concat18_Concat18_0_split
I0905 01:04:50.377918 73306 net.cpp:100] Creating Layer Concat18_Concat18_0_split
I0905 01:04:50.377924 73306 net.cpp:434] Concat18_Concat18_0_split <- Concat18
I0905 01:04:50.377943 73306 net.cpp:408] Concat18_Concat18_0_split -> Concat18_Concat18_0_split_0
I0905 01:04:50.377953 73306 net.cpp:408] Concat18_Concat18_0_split -> Concat18_Concat18_0_split_1
I0905 01:04:50.377991 73306 net.cpp:150] Setting up Concat18_Concat18_0_split
I0905 01:04:50.378000 73306 net.cpp:157] Top shape: 16 232 32 32 (3801088)
I0905 01:04:50.378008 73306 net.cpp:157] Top shape: 16 232 32 32 (3801088)
I0905 01:04:50.378015 73306 net.cpp:165] Memory required for data: 2383151168
I0905 01:04:50.378024 73306 layer_factory.hpp:77] Creating layer BatchNorm20
I0905 01:04:50.378036 73306 net.cpp:100] Creating Layer BatchNorm20
I0905 01:04:50.378046 73306 net.cpp:434] BatchNorm20 <- Concat18_Concat18_0_split_0
I0905 01:04:50.378058 73306 net.cpp:408] BatchNorm20 -> BatchNorm20
I0905 01:04:50.378242 73306 net.cpp:150] Setting up BatchNorm20
I0905 01:04:50.378257 73306 net.cpp:157] Top shape: 16 232 32 32 (3801088)
I0905 01:04:50.378267 73306 net.cpp:165] Memory required for data: 2398355520
I0905 01:04:50.378278 73306 layer_factory.hpp:77] Creating layer Scale20
I0905 01:04:50.378288 73306 net.cpp:100] Creating Layer Scale20
I0905 01:04:50.378296 73306 net.cpp:434] Scale20 <- BatchNorm20
I0905 01:04:50.378304 73306 net.cpp:395] Scale20 -> BatchNorm20 (in-place)
I0905 01:04:50.378391 73306 net.cpp:150] Setting up Scale20
I0905 01:04:50.378403 73306 net.cpp:157] Top shape: 16 232 32 32 (3801088)
I0905 01:04:50.378412 73306 net.cpp:165] Memory required for data: 2413559872
I0905 01:04:50.378419 73306 layer_factory.hpp:77] Creating layer ReLU20
I0905 01:04:50.378429 73306 net.cpp:100] Creating Layer ReLU20
I0905 01:04:50.378437 73306 net.cpp:434] ReLU20 <- BatchNorm20
I0905 01:04:50.378445 73306 net.cpp:395] ReLU20 -> BatchNorm20 (in-place)
I0905 01:04:50.378618 73306 net.cpp:150] Setting up ReLU20
I0905 01:04:50.378649 73306 net.cpp:157] Top shape: 16 232 32 32 (3801088)
I0905 01:04:50.378659 73306 net.cpp:165] Memory required for data: 2428764224
I0905 01:04:50.378672 73306 layer_factory.hpp:77] Creating layer Convolution21
I0905 01:04:50.378686 73306 net.cpp:100] Creating Layer Convolution21
I0905 01:04:50.378695 73306 net.cpp:434] Convolution21 <- BatchNorm20
I0905 01:04:50.378705 73306 net.cpp:408] Convolution21 -> Convolution21
I0905 01:04:50.380422 73306 net.cpp:150] Setting up Convolution21
I0905 01:04:50.380441 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.380452 73306 net.cpp:165] Memory required for data: 2429550656
I0905 01:04:50.380462 73306 layer_factory.hpp:77] Creating layer Dropout21
I0905 01:04:50.380473 73306 net.cpp:100] Creating Layer Dropout21
I0905 01:04:50.380481 73306 net.cpp:434] Dropout21 <- Convolution21
I0905 01:04:50.380489 73306 net.cpp:408] Dropout21 -> Dropout21
I0905 01:04:50.380535 73306 net.cpp:150] Setting up Dropout21
I0905 01:04:50.380548 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.380554 73306 net.cpp:165] Memory required for data: 2430337088
I0905 01:04:50.380561 73306 layer_factory.hpp:77] Creating layer Concat19
I0905 01:04:50.380573 73306 net.cpp:100] Creating Layer Concat19
I0905 01:04:50.380591 73306 net.cpp:434] Concat19 <- Concat18_Concat18_0_split_1
I0905 01:04:50.380600 73306 net.cpp:434] Concat19 <- Dropout21
I0905 01:04:50.380611 73306 net.cpp:408] Concat19 -> Concat19
I0905 01:04:50.380640 73306 net.cpp:150] Setting up Concat19
I0905 01:04:50.380653 73306 net.cpp:157] Top shape: 16 244 32 32 (3997696)
I0905 01:04:50.380661 73306 net.cpp:165] Memory required for data: 2446327872
I0905 01:04:50.380666 73306 layer_factory.hpp:77] Creating layer Concat19_Concat19_0_split
I0905 01:04:50.380676 73306 net.cpp:100] Creating Layer Concat19_Concat19_0_split
I0905 01:04:50.380687 73306 net.cpp:434] Concat19_Concat19_0_split <- Concat19
I0905 01:04:50.380697 73306 net.cpp:408] Concat19_Concat19_0_split -> Concat19_Concat19_0_split_0
I0905 01:04:50.380707 73306 net.cpp:408] Concat19_Concat19_0_split -> Concat19_Concat19_0_split_1
I0905 01:04:50.380744 73306 net.cpp:150] Setting up Concat19_Concat19_0_split
I0905 01:04:50.380756 73306 net.cpp:157] Top shape: 16 244 32 32 (3997696)
I0905 01:04:50.380764 73306 net.cpp:157] Top shape: 16 244 32 32 (3997696)
I0905 01:04:50.380769 73306 net.cpp:165] Memory required for data: 2478309440
I0905 01:04:50.380776 73306 layer_factory.hpp:77] Creating layer BatchNorm21
I0905 01:04:50.380787 73306 net.cpp:100] Creating Layer BatchNorm21
I0905 01:04:50.380795 73306 net.cpp:434] BatchNorm21 <- Concat19_Concat19_0_split_0
I0905 01:04:50.380805 73306 net.cpp:408] BatchNorm21 -> BatchNorm21
I0905 01:04:50.380988 73306 net.cpp:150] Setting up BatchNorm21
I0905 01:04:50.381002 73306 net.cpp:157] Top shape: 16 244 32 32 (3997696)
I0905 01:04:50.381012 73306 net.cpp:165] Memory required for data: 2494300224
I0905 01:04:50.381022 73306 layer_factory.hpp:77] Creating layer Scale21
I0905 01:04:50.381034 73306 net.cpp:100] Creating Layer Scale21
I0905 01:04:50.381042 73306 net.cpp:434] Scale21 <- BatchNorm21
I0905 01:04:50.381050 73306 net.cpp:395] Scale21 -> BatchNorm21 (in-place)
I0905 01:04:50.381136 73306 net.cpp:150] Setting up Scale21
I0905 01:04:50.381158 73306 net.cpp:157] Top shape: 16 244 32 32 (3997696)
I0905 01:04:50.381168 73306 net.cpp:165] Memory required for data: 2510291008
I0905 01:04:50.381178 73306 layer_factory.hpp:77] Creating layer ReLU21
I0905 01:04:50.381187 73306 net.cpp:100] Creating Layer ReLU21
I0905 01:04:50.381194 73306 net.cpp:434] ReLU21 <- BatchNorm21
I0905 01:04:50.381204 73306 net.cpp:395] ReLU21 -> BatchNorm21 (in-place)
I0905 01:04:50.381506 73306 net.cpp:150] Setting up ReLU21
I0905 01:04:50.381526 73306 net.cpp:157] Top shape: 16 244 32 32 (3997696)
I0905 01:04:50.381534 73306 net.cpp:165] Memory required for data: 2526281792
I0905 01:04:50.381542 73306 layer_factory.hpp:77] Creating layer Convolution22
I0905 01:04:50.381556 73306 net.cpp:100] Creating Layer Convolution22
I0905 01:04:50.381564 73306 net.cpp:434] Convolution22 <- BatchNorm21
I0905 01:04:50.381577 73306 net.cpp:408] Convolution22 -> Convolution22
I0905 01:04:50.383121 73306 net.cpp:150] Setting up Convolution22
I0905 01:04:50.383141 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.383148 73306 net.cpp:165] Memory required for data: 2527068224
I0905 01:04:50.383158 73306 layer_factory.hpp:77] Creating layer Dropout22
I0905 01:04:50.383169 73306 net.cpp:100] Creating Layer Dropout22
I0905 01:04:50.383179 73306 net.cpp:434] Dropout22 <- Convolution22
I0905 01:04:50.383188 73306 net.cpp:408] Dropout22 -> Dropout22
I0905 01:04:50.383235 73306 net.cpp:150] Setting up Dropout22
I0905 01:04:50.383255 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.383262 73306 net.cpp:165] Memory required for data: 2527854656
I0905 01:04:50.383270 73306 layer_factory.hpp:77] Creating layer Concat20
I0905 01:04:50.383285 73306 net.cpp:100] Creating Layer Concat20
I0905 01:04:50.383291 73306 net.cpp:434] Concat20 <- Concat19_Concat19_0_split_1
I0905 01:04:50.383308 73306 net.cpp:434] Concat20 <- Dropout22
I0905 01:04:50.383317 73306 net.cpp:408] Concat20 -> Concat20
I0905 01:04:50.383345 73306 net.cpp:150] Setting up Concat20
I0905 01:04:50.383366 73306 net.cpp:157] Top shape: 16 256 32 32 (4194304)
I0905 01:04:50.383373 73306 net.cpp:165] Memory required for data: 2544631872
I0905 01:04:50.383379 73306 layer_factory.hpp:77] Creating layer Concat20_Concat20_0_split
I0905 01:04:50.383391 73306 net.cpp:100] Creating Layer Concat20_Concat20_0_split
I0905 01:04:50.383400 73306 net.cpp:434] Concat20_Concat20_0_split <- Concat20
I0905 01:04:50.383409 73306 net.cpp:408] Concat20_Concat20_0_split -> Concat20_Concat20_0_split_0
I0905 01:04:50.383430 73306 net.cpp:408] Concat20_Concat20_0_split -> Concat20_Concat20_0_split_1
I0905 01:04:50.383471 73306 net.cpp:150] Setting up Concat20_Concat20_0_split
I0905 01:04:50.383482 73306 net.cpp:157] Top shape: 16 256 32 32 (4194304)
I0905 01:04:50.383491 73306 net.cpp:157] Top shape: 16 256 32 32 (4194304)
I0905 01:04:50.383496 73306 net.cpp:165] Memory required for data: 2578186304
I0905 01:04:50.383504 73306 layer_factory.hpp:77] Creating layer BatchNorm22
I0905 01:04:50.383513 73306 net.cpp:100] Creating Layer BatchNorm22
I0905 01:04:50.383520 73306 net.cpp:434] BatchNorm22 <- Concat20_Concat20_0_split_0
I0905 01:04:50.383532 73306 net.cpp:408] BatchNorm22 -> BatchNorm22
I0905 01:04:50.383702 73306 net.cpp:150] Setting up BatchNorm22
I0905 01:04:50.383715 73306 net.cpp:157] Top shape: 16 256 32 32 (4194304)
I0905 01:04:50.383725 73306 net.cpp:165] Memory required for data: 2594963520
I0905 01:04:50.383749 73306 layer_factory.hpp:77] Creating layer Scale22
I0905 01:04:50.383764 73306 net.cpp:100] Creating Layer Scale22
I0905 01:04:50.383771 73306 net.cpp:434] Scale22 <- BatchNorm22
I0905 01:04:50.383780 73306 net.cpp:395] Scale22 -> BatchNorm22 (in-place)
I0905 01:04:50.383865 73306 net.cpp:150] Setting up Scale22
I0905 01:04:50.383878 73306 net.cpp:157] Top shape: 16 256 32 32 (4194304)
I0905 01:04:50.383885 73306 net.cpp:165] Memory required for data: 2611740736
I0905 01:04:50.383893 73306 layer_factory.hpp:77] Creating layer ReLU22
I0905 01:04:50.383904 73306 net.cpp:100] Creating Layer ReLU22
I0905 01:04:50.383911 73306 net.cpp:434] ReLU22 <- BatchNorm22
I0905 01:04:50.383921 73306 net.cpp:395] ReLU22 -> BatchNorm22 (in-place)
I0905 01:04:50.384366 73306 net.cpp:150] Setting up ReLU22
I0905 01:04:50.384384 73306 net.cpp:157] Top shape: 16 256 32 32 (4194304)
I0905 01:04:50.384392 73306 net.cpp:165] Memory required for data: 2628517952
I0905 01:04:50.384400 73306 layer_factory.hpp:77] Creating layer Convolution23
I0905 01:04:50.384418 73306 net.cpp:100] Creating Layer Convolution23
I0905 01:04:50.384425 73306 net.cpp:434] Convolution23 <- BatchNorm22
I0905 01:04:50.384438 73306 net.cpp:408] Convolution23 -> Convolution23
I0905 01:04:50.386183 73306 net.cpp:150] Setting up Convolution23
I0905 01:04:50.386203 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.386210 73306 net.cpp:165] Memory required for data: 2629304384
I0905 01:04:50.386220 73306 layer_factory.hpp:77] Creating layer Dropout23
I0905 01:04:50.386232 73306 net.cpp:100] Creating Layer Dropout23
I0905 01:04:50.386240 73306 net.cpp:434] Dropout23 <- Convolution23
I0905 01:04:50.386251 73306 net.cpp:408] Dropout23 -> Dropout23
I0905 01:04:50.386298 73306 net.cpp:150] Setting up Dropout23
I0905 01:04:50.386309 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.386317 73306 net.cpp:165] Memory required for data: 2630090816
I0905 01:04:50.386324 73306 layer_factory.hpp:77] Creating layer Concat21
I0905 01:04:50.386335 73306 net.cpp:100] Creating Layer Concat21
I0905 01:04:50.386343 73306 net.cpp:434] Concat21 <- Concat20_Concat20_0_split_1
I0905 01:04:50.386350 73306 net.cpp:434] Concat21 <- Dropout23
I0905 01:04:50.386359 73306 net.cpp:408] Concat21 -> Concat21
I0905 01:04:50.386387 73306 net.cpp:150] Setting up Concat21
I0905 01:04:50.386397 73306 net.cpp:157] Top shape: 16 268 32 32 (4390912)
I0905 01:04:50.386404 73306 net.cpp:165] Memory required for data: 2647654464
I0905 01:04:50.386410 73306 layer_factory.hpp:77] Creating layer Concat21_Concat21_0_split
I0905 01:04:50.386420 73306 net.cpp:100] Creating Layer Concat21_Concat21_0_split
I0905 01:04:50.386440 73306 net.cpp:434] Concat21_Concat21_0_split <- Concat21
I0905 01:04:50.386451 73306 net.cpp:408] Concat21_Concat21_0_split -> Concat21_Concat21_0_split_0
I0905 01:04:50.386461 73306 net.cpp:408] Concat21_Concat21_0_split -> Concat21_Concat21_0_split_1
I0905 01:04:50.386500 73306 net.cpp:150] Setting up Concat21_Concat21_0_split
I0905 01:04:50.386513 73306 net.cpp:157] Top shape: 16 268 32 32 (4390912)
I0905 01:04:50.386521 73306 net.cpp:157] Top shape: 16 268 32 32 (4390912)
I0905 01:04:50.386528 73306 net.cpp:165] Memory required for data: 2682781760
I0905 01:04:50.386533 73306 layer_factory.hpp:77] Creating layer BatchNorm23
I0905 01:04:50.386544 73306 net.cpp:100] Creating Layer BatchNorm23
I0905 01:04:50.386551 73306 net.cpp:434] BatchNorm23 <- Concat21_Concat21_0_split_0
I0905 01:04:50.386561 73306 net.cpp:408] BatchNorm23 -> BatchNorm23
I0905 01:04:50.386754 73306 net.cpp:150] Setting up BatchNorm23
I0905 01:04:50.386767 73306 net.cpp:157] Top shape: 16 268 32 32 (4390912)
I0905 01:04:50.386776 73306 net.cpp:165] Memory required for data: 2700345408
I0905 01:04:50.386787 73306 layer_factory.hpp:77] Creating layer Scale23
I0905 01:04:50.386801 73306 net.cpp:100] Creating Layer Scale23
I0905 01:04:50.386808 73306 net.cpp:434] Scale23 <- BatchNorm23
I0905 01:04:50.386817 73306 net.cpp:395] Scale23 -> BatchNorm23 (in-place)
I0905 01:04:50.386905 73306 net.cpp:150] Setting up Scale23
I0905 01:04:50.386919 73306 net.cpp:157] Top shape: 16 268 32 32 (4390912)
I0905 01:04:50.386925 73306 net.cpp:165] Memory required for data: 2717909056
I0905 01:04:50.386934 73306 layer_factory.hpp:77] Creating layer ReLU23
I0905 01:04:50.386947 73306 net.cpp:100] Creating Layer ReLU23
I0905 01:04:50.386955 73306 net.cpp:434] ReLU23 <- BatchNorm23
I0905 01:04:50.386962 73306 net.cpp:395] ReLU23 -> BatchNorm23 (in-place)
I0905 01:04:50.387126 73306 net.cpp:150] Setting up ReLU23
I0905 01:04:50.387142 73306 net.cpp:157] Top shape: 16 268 32 32 (4390912)
I0905 01:04:50.387150 73306 net.cpp:165] Memory required for data: 2735472704
I0905 01:04:50.387156 73306 layer_factory.hpp:77] Creating layer Convolution24
I0905 01:04:50.387171 73306 net.cpp:100] Creating Layer Convolution24
I0905 01:04:50.387178 73306 net.cpp:434] Convolution24 <- BatchNorm23
I0905 01:04:50.387190 73306 net.cpp:408] Convolution24 -> Convolution24
I0905 01:04:50.388957 73306 net.cpp:150] Setting up Convolution24
I0905 01:04:50.388978 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.388985 73306 net.cpp:165] Memory required for data: 2736259136
I0905 01:04:50.388995 73306 layer_factory.hpp:77] Creating layer Dropout24
I0905 01:04:50.389009 73306 net.cpp:100] Creating Layer Dropout24
I0905 01:04:50.389017 73306 net.cpp:434] Dropout24 <- Convolution24
I0905 01:04:50.389026 73306 net.cpp:408] Dropout24 -> Dropout24
I0905 01:04:50.389078 73306 net.cpp:150] Setting up Dropout24
I0905 01:04:50.389089 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.389096 73306 net.cpp:165] Memory required for data: 2737045568
I0905 01:04:50.389104 73306 layer_factory.hpp:77] Creating layer Concat22
I0905 01:04:50.389114 73306 net.cpp:100] Creating Layer Concat22
I0905 01:04:50.389122 73306 net.cpp:434] Concat22 <- Concat21_Concat21_0_split_1
I0905 01:04:50.389129 73306 net.cpp:434] Concat22 <- Dropout24
I0905 01:04:50.389140 73306 net.cpp:408] Concat22 -> Concat22
I0905 01:04:50.389168 73306 net.cpp:150] Setting up Concat22
I0905 01:04:50.389178 73306 net.cpp:157] Top shape: 16 280 32 32 (4587520)
I0905 01:04:50.389184 73306 net.cpp:165] Memory required for data: 2755395648
I0905 01:04:50.389191 73306 layer_factory.hpp:77] Creating layer Concat22_Concat22_0_split
I0905 01:04:50.389201 73306 net.cpp:100] Creating Layer Concat22_Concat22_0_split
I0905 01:04:50.389209 73306 net.cpp:434] Concat22_Concat22_0_split <- Concat22
I0905 01:04:50.389217 73306 net.cpp:408] Concat22_Concat22_0_split -> Concat22_Concat22_0_split_0
I0905 01:04:50.389228 73306 net.cpp:408] Concat22_Concat22_0_split -> Concat22_Concat22_0_split_1
I0905 01:04:50.389267 73306 net.cpp:150] Setting up Concat22_Concat22_0_split
I0905 01:04:50.389289 73306 net.cpp:157] Top shape: 16 280 32 32 (4587520)
I0905 01:04:50.389297 73306 net.cpp:157] Top shape: 16 280 32 32 (4587520)
I0905 01:04:50.389303 73306 net.cpp:165] Memory required for data: 2792095808
I0905 01:04:50.389310 73306 layer_factory.hpp:77] Creating layer BatchNorm24
I0905 01:04:50.389322 73306 net.cpp:100] Creating Layer BatchNorm24
I0905 01:04:50.389330 73306 net.cpp:434] BatchNorm24 <- Concat22_Concat22_0_split_0
I0905 01:04:50.389339 73306 net.cpp:408] BatchNorm24 -> BatchNorm24
I0905 01:04:50.389529 73306 net.cpp:150] Setting up BatchNorm24
I0905 01:04:50.389544 73306 net.cpp:157] Top shape: 16 280 32 32 (4587520)
I0905 01:04:50.389552 73306 net.cpp:165] Memory required for data: 2810445888
I0905 01:04:50.389564 73306 layer_factory.hpp:77] Creating layer Scale24
I0905 01:04:50.389575 73306 net.cpp:100] Creating Layer Scale24
I0905 01:04:50.389582 73306 net.cpp:434] Scale24 <- BatchNorm24
I0905 01:04:50.389590 73306 net.cpp:395] Scale24 -> BatchNorm24 (in-place)
I0905 01:04:50.389683 73306 net.cpp:150] Setting up Scale24
I0905 01:04:50.389695 73306 net.cpp:157] Top shape: 16 280 32 32 (4587520)
I0905 01:04:50.389704 73306 net.cpp:165] Memory required for data: 2828795968
I0905 01:04:50.389714 73306 layer_factory.hpp:77] Creating layer ReLU24
I0905 01:04:50.389724 73306 net.cpp:100] Creating Layer ReLU24
I0905 01:04:50.389731 73306 net.cpp:434] ReLU24 <- BatchNorm24
I0905 01:04:50.389750 73306 net.cpp:395] ReLU24 -> BatchNorm24 (in-place)
I0905 01:04:50.390058 73306 net.cpp:150] Setting up ReLU24
I0905 01:04:50.390075 73306 net.cpp:157] Top shape: 16 280 32 32 (4587520)
I0905 01:04:50.390084 73306 net.cpp:165] Memory required for data: 2847146048
I0905 01:04:50.390090 73306 layer_factory.hpp:77] Creating layer Convolution25
I0905 01:04:50.390105 73306 net.cpp:100] Creating Layer Convolution25
I0905 01:04:50.390113 73306 net.cpp:434] Convolution25 <- BatchNorm24
I0905 01:04:50.390125 73306 net.cpp:408] Convolution25 -> Convolution25
I0905 01:04:50.392345 73306 net.cpp:150] Setting up Convolution25
I0905 01:04:50.392366 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.392375 73306 net.cpp:165] Memory required for data: 2847932480
I0905 01:04:50.392385 73306 layer_factory.hpp:77] Creating layer Dropout25
I0905 01:04:50.392398 73306 net.cpp:100] Creating Layer Dropout25
I0905 01:04:50.392406 73306 net.cpp:434] Dropout25 <- Convolution25
I0905 01:04:50.392416 73306 net.cpp:408] Dropout25 -> Dropout25
I0905 01:04:50.392473 73306 net.cpp:150] Setting up Dropout25
I0905 01:04:50.392484 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.392495 73306 net.cpp:165] Memory required for data: 2848718912
I0905 01:04:50.392501 73306 layer_factory.hpp:77] Creating layer Concat23
I0905 01:04:50.392511 73306 net.cpp:100] Creating Layer Concat23
I0905 01:04:50.392519 73306 net.cpp:434] Concat23 <- Concat22_Concat22_0_split_1
I0905 01:04:50.392527 73306 net.cpp:434] Concat23 <- Dropout25
I0905 01:04:50.392539 73306 net.cpp:408] Concat23 -> Concat23
I0905 01:04:50.392565 73306 net.cpp:150] Setting up Concat23
I0905 01:04:50.392575 73306 net.cpp:157] Top shape: 16 292 32 32 (4784128)
I0905 01:04:50.392582 73306 net.cpp:165] Memory required for data: 2867855424
I0905 01:04:50.392590 73306 layer_factory.hpp:77] Creating layer Concat23_Concat23_0_split
I0905 01:04:50.392601 73306 net.cpp:100] Creating Layer Concat23_Concat23_0_split
I0905 01:04:50.392608 73306 net.cpp:434] Concat23_Concat23_0_split <- Concat23
I0905 01:04:50.392617 73306 net.cpp:408] Concat23_Concat23_0_split -> Concat23_Concat23_0_split_0
I0905 01:04:50.392628 73306 net.cpp:408] Concat23_Concat23_0_split -> Concat23_Concat23_0_split_1
I0905 01:04:50.392665 73306 net.cpp:150] Setting up Concat23_Concat23_0_split
I0905 01:04:50.392676 73306 net.cpp:157] Top shape: 16 292 32 32 (4784128)
I0905 01:04:50.392683 73306 net.cpp:157] Top shape: 16 292 32 32 (4784128)
I0905 01:04:50.392690 73306 net.cpp:165] Memory required for data: 2906128448
I0905 01:04:50.392709 73306 layer_factory.hpp:77] Creating layer BatchNorm25
I0905 01:04:50.392720 73306 net.cpp:100] Creating Layer BatchNorm25
I0905 01:04:50.392729 73306 net.cpp:434] BatchNorm25 <- Concat23_Concat23_0_split_0
I0905 01:04:50.392736 73306 net.cpp:408] BatchNorm25 -> BatchNorm25
I0905 01:04:50.392930 73306 net.cpp:150] Setting up BatchNorm25
I0905 01:04:50.392945 73306 net.cpp:157] Top shape: 16 292 32 32 (4784128)
I0905 01:04:50.392952 73306 net.cpp:165] Memory required for data: 2925264960
I0905 01:04:50.392962 73306 layer_factory.hpp:77] Creating layer Scale25
I0905 01:04:50.392976 73306 net.cpp:100] Creating Layer Scale25
I0905 01:04:50.392982 73306 net.cpp:434] Scale25 <- BatchNorm25
I0905 01:04:50.392992 73306 net.cpp:395] Scale25 -> BatchNorm25 (in-place)
I0905 01:04:50.393085 73306 net.cpp:150] Setting up Scale25
I0905 01:04:50.393098 73306 net.cpp:157] Top shape: 16 292 32 32 (4784128)
I0905 01:04:50.393108 73306 net.cpp:165] Memory required for data: 2944401472
I0905 01:04:50.393116 73306 layer_factory.hpp:77] Creating layer ReLU25
I0905 01:04:50.393127 73306 net.cpp:100] Creating Layer ReLU25
I0905 01:04:50.393134 73306 net.cpp:434] ReLU25 <- BatchNorm25
I0905 01:04:50.393144 73306 net.cpp:395] ReLU25 -> BatchNorm25 (in-place)
I0905 01:04:50.393435 73306 net.cpp:150] Setting up ReLU25
I0905 01:04:50.393452 73306 net.cpp:157] Top shape: 16 292 32 32 (4784128)
I0905 01:04:50.393460 73306 net.cpp:165] Memory required for data: 2963537984
I0905 01:04:50.393470 73306 layer_factory.hpp:77] Creating layer Convolution26
I0905 01:04:50.393486 73306 net.cpp:100] Creating Layer Convolution26
I0905 01:04:50.393493 73306 net.cpp:434] Convolution26 <- BatchNorm25
I0905 01:04:50.393503 73306 net.cpp:408] Convolution26 -> Convolution26
I0905 01:04:50.395357 73306 net.cpp:150] Setting up Convolution26
I0905 01:04:50.395376 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.395385 73306 net.cpp:165] Memory required for data: 2964324416
I0905 01:04:50.395395 73306 layer_factory.hpp:77] Creating layer Dropout26
I0905 01:04:50.395407 73306 net.cpp:100] Creating Layer Dropout26
I0905 01:04:50.395416 73306 net.cpp:434] Dropout26 <- Convolution26
I0905 01:04:50.395424 73306 net.cpp:408] Dropout26 -> Dropout26
I0905 01:04:50.395472 73306 net.cpp:150] Setting up Dropout26
I0905 01:04:50.395483 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.395489 73306 net.cpp:165] Memory required for data: 2965110848
I0905 01:04:50.395498 73306 layer_factory.hpp:77] Creating layer Concat24
I0905 01:04:50.395508 73306 net.cpp:100] Creating Layer Concat24
I0905 01:04:50.395515 73306 net.cpp:434] Concat24 <- Concat23_Concat23_0_split_1
I0905 01:04:50.395522 73306 net.cpp:434] Concat24 <- Dropout26
I0905 01:04:50.395535 73306 net.cpp:408] Concat24 -> Concat24
I0905 01:04:50.395566 73306 net.cpp:150] Setting up Concat24
I0905 01:04:50.395576 73306 net.cpp:157] Top shape: 16 304 32 32 (4980736)
I0905 01:04:50.395583 73306 net.cpp:165] Memory required for data: 2985033792
I0905 01:04:50.395591 73306 layer_factory.hpp:77] Creating layer BatchNorm26
I0905 01:04:50.395604 73306 net.cpp:100] Creating Layer BatchNorm26
I0905 01:04:50.395612 73306 net.cpp:434] BatchNorm26 <- Concat24
I0905 01:04:50.395620 73306 net.cpp:408] BatchNorm26 -> BatchNorm26
I0905 01:04:50.395815 73306 net.cpp:150] Setting up BatchNorm26
I0905 01:04:50.395828 73306 net.cpp:157] Top shape: 16 304 32 32 (4980736)
I0905 01:04:50.395838 73306 net.cpp:165] Memory required for data: 3004956736
I0905 01:04:50.395849 73306 layer_factory.hpp:77] Creating layer Scale26
I0905 01:04:50.395862 73306 net.cpp:100] Creating Layer Scale26
I0905 01:04:50.395869 73306 net.cpp:434] Scale26 <- BatchNorm26
I0905 01:04:50.395877 73306 net.cpp:395] Scale26 -> BatchNorm26 (in-place)
I0905 01:04:50.395970 73306 net.cpp:150] Setting up Scale26
I0905 01:04:50.395983 73306 net.cpp:157] Top shape: 16 304 32 32 (4980736)
I0905 01:04:50.395990 73306 net.cpp:165] Memory required for data: 3024879680
I0905 01:04:50.395998 73306 layer_factory.hpp:77] Creating layer ReLU26
I0905 01:04:50.396019 73306 net.cpp:100] Creating Layer ReLU26
I0905 01:04:50.396028 73306 net.cpp:434] ReLU26 <- BatchNorm26
I0905 01:04:50.396037 73306 net.cpp:395] ReLU26 -> BatchNorm26 (in-place)
I0905 01:04:50.396214 73306 net.cpp:150] Setting up ReLU26
I0905 01:04:50.396229 73306 net.cpp:157] Top shape: 16 304 32 32 (4980736)
I0905 01:04:50.396236 73306 net.cpp:165] Memory required for data: 3044802624
I0905 01:04:50.396244 73306 layer_factory.hpp:77] Creating layer Convolution27
I0905 01:04:50.396260 73306 net.cpp:100] Creating Layer Convolution27
I0905 01:04:50.396267 73306 net.cpp:434] Convolution27 <- BatchNorm26
I0905 01:04:50.396278 73306 net.cpp:408] Convolution27 -> Convolution27
I0905 01:04:50.400319 73306 net.cpp:150] Setting up Convolution27
I0905 01:04:50.400339 73306 net.cpp:157] Top shape: 16 304 32 32 (4980736)
I0905 01:04:50.400353 73306 net.cpp:165] Memory required for data: 3064725568
I0905 01:04:50.400364 73306 layer_factory.hpp:77] Creating layer Dropout27
I0905 01:04:50.400374 73306 net.cpp:100] Creating Layer Dropout27
I0905 01:04:50.400382 73306 net.cpp:434] Dropout27 <- Convolution27
I0905 01:04:50.400391 73306 net.cpp:408] Dropout27 -> Dropout27
I0905 01:04:50.400439 73306 net.cpp:150] Setting up Dropout27
I0905 01:04:50.400451 73306 net.cpp:157] Top shape: 16 304 32 32 (4980736)
I0905 01:04:50.400459 73306 net.cpp:165] Memory required for data: 3084648512
I0905 01:04:50.400466 73306 layer_factory.hpp:77] Creating layer Pooling2
I0905 01:04:50.400476 73306 net.cpp:100] Creating Layer Pooling2
I0905 01:04:50.400485 73306 net.cpp:434] Pooling2 <- Dropout27
I0905 01:04:50.400496 73306 net.cpp:408] Pooling2 -> Pooling2
I0905 01:04:50.400807 73306 net.cpp:150] Setting up Pooling2
I0905 01:04:50.400826 73306 net.cpp:157] Top shape: 16 304 16 16 (1245184)
I0905 01:04:50.400835 73306 net.cpp:165] Memory required for data: 3089629248
I0905 01:04:50.400842 73306 layer_factory.hpp:77] Creating layer Pooling2_Pooling2_0_split
I0905 01:04:50.400853 73306 net.cpp:100] Creating Layer Pooling2_Pooling2_0_split
I0905 01:04:50.400861 73306 net.cpp:434] Pooling2_Pooling2_0_split <- Pooling2
I0905 01:04:50.400869 73306 net.cpp:408] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_0
I0905 01:04:50.400879 73306 net.cpp:408] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_1
I0905 01:04:50.400928 73306 net.cpp:150] Setting up Pooling2_Pooling2_0_split
I0905 01:04:50.400938 73306 net.cpp:157] Top shape: 16 304 16 16 (1245184)
I0905 01:04:50.400946 73306 net.cpp:157] Top shape: 16 304 16 16 (1245184)
I0905 01:04:50.400957 73306 net.cpp:165] Memory required for data: 3099590720
I0905 01:04:50.400964 73306 layer_factory.hpp:77] Creating layer BatchNorm27
I0905 01:04:50.400985 73306 net.cpp:100] Creating Layer BatchNorm27
I0905 01:04:50.400991 73306 net.cpp:434] BatchNorm27 <- Pooling2_Pooling2_0_split_0
I0905 01:04:50.401000 73306 net.cpp:408] BatchNorm27 -> BatchNorm27
I0905 01:04:50.401198 73306 net.cpp:150] Setting up BatchNorm27
I0905 01:04:50.401211 73306 net.cpp:157] Top shape: 16 304 16 16 (1245184)
I0905 01:04:50.401221 73306 net.cpp:165] Memory required for data: 3104571456
I0905 01:04:50.401240 73306 layer_factory.hpp:77] Creating layer Scale27
I0905 01:04:50.401257 73306 net.cpp:100] Creating Layer Scale27
I0905 01:04:50.401265 73306 net.cpp:434] Scale27 <- BatchNorm27
I0905 01:04:50.401273 73306 net.cpp:395] Scale27 -> BatchNorm27 (in-place)
I0905 01:04:50.401371 73306 net.cpp:150] Setting up Scale27
I0905 01:04:50.401383 73306 net.cpp:157] Top shape: 16 304 16 16 (1245184)
I0905 01:04:50.401393 73306 net.cpp:165] Memory required for data: 3109552192
I0905 01:04:50.401402 73306 layer_factory.hpp:77] Creating layer ReLU27
I0905 01:04:50.401412 73306 net.cpp:100] Creating Layer ReLU27
I0905 01:04:50.401419 73306 net.cpp:434] ReLU27 <- BatchNorm27
I0905 01:04:50.401428 73306 net.cpp:395] ReLU27 -> BatchNorm27 (in-place)
I0905 01:04:50.401590 73306 net.cpp:150] Setting up ReLU27
I0905 01:04:50.401613 73306 net.cpp:157] Top shape: 16 304 16 16 (1245184)
I0905 01:04:50.401620 73306 net.cpp:165] Memory required for data: 3114532928
I0905 01:04:50.401640 73306 layer_factory.hpp:77] Creating layer Convolution28
I0905 01:04:50.401660 73306 net.cpp:100] Creating Layer Convolution28
I0905 01:04:50.401669 73306 net.cpp:434] Convolution28 <- BatchNorm27
I0905 01:04:50.401680 73306 net.cpp:408] Convolution28 -> Convolution28
I0905 01:04:50.403578 73306 net.cpp:150] Setting up Convolution28
I0905 01:04:50.403599 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.403606 73306 net.cpp:165] Memory required for data: 3114729536
I0905 01:04:50.403617 73306 layer_factory.hpp:77] Creating layer Dropout28
I0905 01:04:50.403630 73306 net.cpp:100] Creating Layer Dropout28
I0905 01:04:50.403637 73306 net.cpp:434] Dropout28 <- Convolution28
I0905 01:04:50.403646 73306 net.cpp:408] Dropout28 -> Dropout28
I0905 01:04:50.403688 73306 net.cpp:150] Setting up Dropout28
I0905 01:04:50.403700 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.403707 73306 net.cpp:165] Memory required for data: 3114926144
I0905 01:04:50.403713 73306 layer_factory.hpp:77] Creating layer Concat25
I0905 01:04:50.403724 73306 net.cpp:100] Creating Layer Concat25
I0905 01:04:50.403735 73306 net.cpp:434] Concat25 <- Pooling2_Pooling2_0_split_1
I0905 01:04:50.403743 73306 net.cpp:434] Concat25 <- Dropout28
I0905 01:04:50.403753 73306 net.cpp:408] Concat25 -> Concat25
I0905 01:04:50.403780 73306 net.cpp:150] Setting up Concat25
I0905 01:04:50.403790 73306 net.cpp:157] Top shape: 16 316 16 16 (1294336)
I0905 01:04:50.403796 73306 net.cpp:165] Memory required for data: 3120103488
I0905 01:04:50.403812 73306 layer_factory.hpp:77] Creating layer Concat25_Concat25_0_split
I0905 01:04:50.403828 73306 net.cpp:100] Creating Layer Concat25_Concat25_0_split
I0905 01:04:50.403836 73306 net.cpp:434] Concat25_Concat25_0_split <- Concat25
I0905 01:04:50.403844 73306 net.cpp:408] Concat25_Concat25_0_split -> Concat25_Concat25_0_split_0
I0905 01:04:50.403853 73306 net.cpp:408] Concat25_Concat25_0_split -> Concat25_Concat25_0_split_1
I0905 01:04:50.403897 73306 net.cpp:150] Setting up Concat25_Concat25_0_split
I0905 01:04:50.403908 73306 net.cpp:157] Top shape: 16 316 16 16 (1294336)
I0905 01:04:50.403923 73306 net.cpp:157] Top shape: 16 316 16 16 (1294336)
I0905 01:04:50.403930 73306 net.cpp:165] Memory required for data: 3130458176
I0905 01:04:50.403944 73306 layer_factory.hpp:77] Creating layer BatchNorm28
I0905 01:04:50.403956 73306 net.cpp:100] Creating Layer BatchNorm28
I0905 01:04:50.403964 73306 net.cpp:434] BatchNorm28 <- Concat25_Concat25_0_split_0
I0905 01:04:50.403973 73306 net.cpp:408] BatchNorm28 -> BatchNorm28
I0905 01:04:50.404165 73306 net.cpp:150] Setting up BatchNorm28
I0905 01:04:50.404177 73306 net.cpp:157] Top shape: 16 316 16 16 (1294336)
I0905 01:04:50.404183 73306 net.cpp:165] Memory required for data: 3135635520
I0905 01:04:50.404197 73306 layer_factory.hpp:77] Creating layer Scale28
I0905 01:04:50.404209 73306 net.cpp:100] Creating Layer Scale28
I0905 01:04:50.404217 73306 net.cpp:434] Scale28 <- BatchNorm28
I0905 01:04:50.404227 73306 net.cpp:395] Scale28 -> BatchNorm28 (in-place)
I0905 01:04:50.404323 73306 net.cpp:150] Setting up Scale28
I0905 01:04:50.404335 73306 net.cpp:157] Top shape: 16 316 16 16 (1294336)
I0905 01:04:50.404342 73306 net.cpp:165] Memory required for data: 3140812864
I0905 01:04:50.404350 73306 layer_factory.hpp:77] Creating layer ReLU28
I0905 01:04:50.404361 73306 net.cpp:100] Creating Layer ReLU28
I0905 01:04:50.404367 73306 net.cpp:434] ReLU28 <- BatchNorm28
I0905 01:04:50.404376 73306 net.cpp:395] ReLU28 -> BatchNorm28 (in-place)
I0905 01:04:50.404680 73306 net.cpp:150] Setting up ReLU28
I0905 01:04:50.404700 73306 net.cpp:157] Top shape: 16 316 16 16 (1294336)
I0905 01:04:50.404708 73306 net.cpp:165] Memory required for data: 3145990208
I0905 01:04:50.404716 73306 layer_factory.hpp:77] Creating layer Convolution29
I0905 01:04:50.404733 73306 net.cpp:100] Creating Layer Convolution29
I0905 01:04:50.404742 73306 net.cpp:434] Convolution29 <- BatchNorm28
I0905 01:04:50.404752 73306 net.cpp:408] Convolution29 -> Convolution29
I0905 01:04:50.406744 73306 net.cpp:150] Setting up Convolution29
I0905 01:04:50.406764 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.406771 73306 net.cpp:165] Memory required for data: 3146186816
I0905 01:04:50.406782 73306 layer_factory.hpp:77] Creating layer Dropout29
I0905 01:04:50.406795 73306 net.cpp:100] Creating Layer Dropout29
I0905 01:04:50.406803 73306 net.cpp:434] Dropout29 <- Convolution29
I0905 01:04:50.406812 73306 net.cpp:408] Dropout29 -> Dropout29
I0905 01:04:50.406857 73306 net.cpp:150] Setting up Dropout29
I0905 01:04:50.406867 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.406874 73306 net.cpp:165] Memory required for data: 3146383424
I0905 01:04:50.406882 73306 layer_factory.hpp:77] Creating layer Concat26
I0905 01:04:50.406890 73306 net.cpp:100] Creating Layer Concat26
I0905 01:04:50.406898 73306 net.cpp:434] Concat26 <- Concat25_Concat25_0_split_1
I0905 01:04:50.406905 73306 net.cpp:434] Concat26 <- Dropout29
I0905 01:04:50.406913 73306 net.cpp:408] Concat26 -> Concat26
I0905 01:04:50.406942 73306 net.cpp:150] Setting up Concat26
I0905 01:04:50.406954 73306 net.cpp:157] Top shape: 16 328 16 16 (1343488)
I0905 01:04:50.406960 73306 net.cpp:165] Memory required for data: 3151757376
I0905 01:04:50.406966 73306 layer_factory.hpp:77] Creating layer Concat26_Concat26_0_split
I0905 01:04:50.406977 73306 net.cpp:100] Creating Layer Concat26_Concat26_0_split
I0905 01:04:50.406985 73306 net.cpp:434] Concat26_Concat26_0_split <- Concat26
I0905 01:04:50.406992 73306 net.cpp:408] Concat26_Concat26_0_split -> Concat26_Concat26_0_split_0
I0905 01:04:50.407001 73306 net.cpp:408] Concat26_Concat26_0_split -> Concat26_Concat26_0_split_1
I0905 01:04:50.407047 73306 net.cpp:150] Setting up Concat26_Concat26_0_split
I0905 01:04:50.407058 73306 net.cpp:157] Top shape: 16 328 16 16 (1343488)
I0905 01:04:50.407064 73306 net.cpp:157] Top shape: 16 328 16 16 (1343488)
I0905 01:04:50.407071 73306 net.cpp:165] Memory required for data: 3162505280
I0905 01:04:50.407078 73306 layer_factory.hpp:77] Creating layer BatchNorm29
I0905 01:04:50.407088 73306 net.cpp:100] Creating Layer BatchNorm29
I0905 01:04:50.407094 73306 net.cpp:434] BatchNorm29 <- Concat26_Concat26_0_split_0
I0905 01:04:50.407105 73306 net.cpp:408] BatchNorm29 -> BatchNorm29
I0905 01:04:50.407313 73306 net.cpp:150] Setting up BatchNorm29
I0905 01:04:50.407333 73306 net.cpp:157] Top shape: 16 328 16 16 (1343488)
I0905 01:04:50.407341 73306 net.cpp:165] Memory required for data: 3167879232
I0905 01:04:50.407352 73306 layer_factory.hpp:77] Creating layer Scale29
I0905 01:04:50.407362 73306 net.cpp:100] Creating Layer Scale29
I0905 01:04:50.407369 73306 net.cpp:434] Scale29 <- BatchNorm29
I0905 01:04:50.407377 73306 net.cpp:395] Scale29 -> BatchNorm29 (in-place)
I0905 01:04:50.407471 73306 net.cpp:150] Setting up Scale29
I0905 01:04:50.407485 73306 net.cpp:157] Top shape: 16 328 16 16 (1343488)
I0905 01:04:50.407495 73306 net.cpp:165] Memory required for data: 3173253184
I0905 01:04:50.407502 73306 layer_factory.hpp:77] Creating layer ReLU29
I0905 01:04:50.407517 73306 net.cpp:100] Creating Layer ReLU29
I0905 01:04:50.407524 73306 net.cpp:434] ReLU29 <- BatchNorm29
I0905 01:04:50.407542 73306 net.cpp:395] ReLU29 -> BatchNorm29 (in-place)
I0905 01:04:50.407847 73306 net.cpp:150] Setting up ReLU29
I0905 01:04:50.407866 73306 net.cpp:157] Top shape: 16 328 16 16 (1343488)
I0905 01:04:50.407876 73306 net.cpp:165] Memory required for data: 3178627136
I0905 01:04:50.407884 73306 layer_factory.hpp:77] Creating layer Convolution30
I0905 01:04:50.407897 73306 net.cpp:100] Creating Layer Convolution30
I0905 01:04:50.407905 73306 net.cpp:434] Convolution30 <- BatchNorm29
I0905 01:04:50.407917 73306 net.cpp:408] Convolution30 -> Convolution30
I0905 01:04:50.412047 73306 net.cpp:150] Setting up Convolution30
I0905 01:04:50.412070 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.412078 73306 net.cpp:165] Memory required for data: 3178823744
I0905 01:04:50.412089 73306 layer_factory.hpp:77] Creating layer Dropout30
I0905 01:04:50.412116 73306 net.cpp:100] Creating Layer Dropout30
I0905 01:04:50.412124 73306 net.cpp:434] Dropout30 <- Convolution30
I0905 01:04:50.412139 73306 net.cpp:408] Dropout30 -> Dropout30
I0905 01:04:50.412185 73306 net.cpp:150] Setting up Dropout30
I0905 01:04:50.412197 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.412206 73306 net.cpp:165] Memory required for data: 3179020352
I0905 01:04:50.412214 73306 layer_factory.hpp:77] Creating layer Concat27
I0905 01:04:50.412225 73306 net.cpp:100] Creating Layer Concat27
I0905 01:04:50.412233 73306 net.cpp:434] Concat27 <- Concat26_Concat26_0_split_1
I0905 01:04:50.412243 73306 net.cpp:434] Concat27 <- Dropout30
I0905 01:04:50.412253 73306 net.cpp:408] Concat27 -> Concat27
I0905 01:04:50.412281 73306 net.cpp:150] Setting up Concat27
I0905 01:04:50.412294 73306 net.cpp:157] Top shape: 16 340 16 16 (1392640)
I0905 01:04:50.412302 73306 net.cpp:165] Memory required for data: 3184590912
I0905 01:04:50.412309 73306 layer_factory.hpp:77] Creating layer Concat27_Concat27_0_split
I0905 01:04:50.412318 73306 net.cpp:100] Creating Layer Concat27_Concat27_0_split
I0905 01:04:50.412325 73306 net.cpp:434] Concat27_Concat27_0_split <- Concat27
I0905 01:04:50.412333 73306 net.cpp:408] Concat27_Concat27_0_split -> Concat27_Concat27_0_split_0
I0905 01:04:50.412343 73306 net.cpp:408] Concat27_Concat27_0_split -> Concat27_Concat27_0_split_1
I0905 01:04:50.412384 73306 net.cpp:150] Setting up Concat27_Concat27_0_split
I0905 01:04:50.412395 73306 net.cpp:157] Top shape: 16 340 16 16 (1392640)
I0905 01:04:50.412402 73306 net.cpp:157] Top shape: 16 340 16 16 (1392640)
I0905 01:04:50.412408 73306 net.cpp:165] Memory required for data: 3195732032
I0905 01:04:50.412415 73306 layer_factory.hpp:77] Creating layer BatchNorm30
I0905 01:04:50.412425 73306 net.cpp:100] Creating Layer BatchNorm30
I0905 01:04:50.412432 73306 net.cpp:434] BatchNorm30 <- Concat27_Concat27_0_split_0
I0905 01:04:50.412441 73306 net.cpp:408] BatchNorm30 -> BatchNorm30
I0905 01:04:50.412643 73306 net.cpp:150] Setting up BatchNorm30
I0905 01:04:50.412658 73306 net.cpp:157] Top shape: 16 340 16 16 (1392640)
I0905 01:04:50.412667 73306 net.cpp:165] Memory required for data: 3201302592
I0905 01:04:50.412678 73306 layer_factory.hpp:77] Creating layer Scale30
I0905 01:04:50.412689 73306 net.cpp:100] Creating Layer Scale30
I0905 01:04:50.412696 73306 net.cpp:434] Scale30 <- BatchNorm30
I0905 01:04:50.412705 73306 net.cpp:395] Scale30 -> BatchNorm30 (in-place)
I0905 01:04:50.412806 73306 net.cpp:150] Setting up Scale30
I0905 01:04:50.412819 73306 net.cpp:157] Top shape: 16 340 16 16 (1392640)
I0905 01:04:50.412827 73306 net.cpp:165] Memory required for data: 3206873152
I0905 01:04:50.412835 73306 layer_factory.hpp:77] Creating layer ReLU30
I0905 01:04:50.412848 73306 net.cpp:100] Creating Layer ReLU30
I0905 01:04:50.412855 73306 net.cpp:434] ReLU30 <- BatchNorm30
I0905 01:04:50.412863 73306 net.cpp:395] ReLU30 -> BatchNorm30 (in-place)
I0905 01:04:50.413030 73306 net.cpp:150] Setting up ReLU30
I0905 01:04:50.413051 73306 net.cpp:157] Top shape: 16 340 16 16 (1392640)
I0905 01:04:50.413058 73306 net.cpp:165] Memory required for data: 3212443712
I0905 01:04:50.413065 73306 layer_factory.hpp:77] Creating layer Convolution31
I0905 01:04:50.413080 73306 net.cpp:100] Creating Layer Convolution31
I0905 01:04:50.413089 73306 net.cpp:434] Convolution31 <- BatchNorm30
I0905 01:04:50.413097 73306 net.cpp:408] Convolution31 -> Convolution31
I0905 01:04:50.419018 73306 net.cpp:150] Setting up Convolution31
I0905 01:04:50.419040 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.419052 73306 net.cpp:165] Memory required for data: 3212640320
I0905 01:04:50.419062 73306 layer_factory.hpp:77] Creating layer Dropout31
I0905 01:04:50.419073 73306 net.cpp:100] Creating Layer Dropout31
I0905 01:04:50.419081 73306 net.cpp:434] Dropout31 <- Convolution31
I0905 01:04:50.419090 73306 net.cpp:408] Dropout31 -> Dropout31
I0905 01:04:50.419136 73306 net.cpp:150] Setting up Dropout31
I0905 01:04:50.419147 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.419169 73306 net.cpp:165] Memory required for data: 3212836928
I0905 01:04:50.419176 73306 layer_factory.hpp:77] Creating layer Concat28
I0905 01:04:50.419189 73306 net.cpp:100] Creating Layer Concat28
I0905 01:04:50.419198 73306 net.cpp:434] Concat28 <- Concat27_Concat27_0_split_1
I0905 01:04:50.419205 73306 net.cpp:434] Concat28 <- Dropout31
I0905 01:04:50.419215 73306 net.cpp:408] Concat28 -> Concat28
I0905 01:04:50.419247 73306 net.cpp:150] Setting up Concat28
I0905 01:04:50.419260 73306 net.cpp:157] Top shape: 16 352 16 16 (1441792)
I0905 01:04:50.419266 73306 net.cpp:165] Memory required for data: 3218604096
I0905 01:04:50.419273 73306 layer_factory.hpp:77] Creating layer Concat28_Concat28_0_split
I0905 01:04:50.419282 73306 net.cpp:100] Creating Layer Concat28_Concat28_0_split
I0905 01:04:50.419298 73306 net.cpp:434] Concat28_Concat28_0_split <- Concat28
I0905 01:04:50.419306 73306 net.cpp:408] Concat28_Concat28_0_split -> Concat28_Concat28_0_split_0
I0905 01:04:50.419315 73306 net.cpp:408] Concat28_Concat28_0_split -> Concat28_Concat28_0_split_1
I0905 01:04:50.419355 73306 net.cpp:150] Setting up Concat28_Concat28_0_split
I0905 01:04:50.419365 73306 net.cpp:157] Top shape: 16 352 16 16 (1441792)
I0905 01:04:50.419373 73306 net.cpp:157] Top shape: 16 352 16 16 (1441792)
I0905 01:04:50.419383 73306 net.cpp:165] Memory required for data: 3230138432
I0905 01:04:50.419389 73306 layer_factory.hpp:77] Creating layer BatchNorm31
I0905 01:04:50.419400 73306 net.cpp:100] Creating Layer BatchNorm31
I0905 01:04:50.419407 73306 net.cpp:434] BatchNorm31 <- Concat28_Concat28_0_split_0
I0905 01:04:50.419416 73306 net.cpp:408] BatchNorm31 -> BatchNorm31
I0905 01:04:50.419617 73306 net.cpp:150] Setting up BatchNorm31
I0905 01:04:50.419632 73306 net.cpp:157] Top shape: 16 352 16 16 (1441792)
I0905 01:04:50.419643 73306 net.cpp:165] Memory required for data: 3235905600
I0905 01:04:50.419653 73306 layer_factory.hpp:77] Creating layer Scale31
I0905 01:04:50.419663 73306 net.cpp:100] Creating Layer Scale31
I0905 01:04:50.419673 73306 net.cpp:434] Scale31 <- BatchNorm31
I0905 01:04:50.419682 73306 net.cpp:395] Scale31 -> BatchNorm31 (in-place)
I0905 01:04:50.419783 73306 net.cpp:150] Setting up Scale31
I0905 01:04:50.419797 73306 net.cpp:157] Top shape: 16 352 16 16 (1441792)
I0905 01:04:50.419807 73306 net.cpp:165] Memory required for data: 3241672768
I0905 01:04:50.419816 73306 layer_factory.hpp:77] Creating layer ReLU31
I0905 01:04:50.419831 73306 net.cpp:100] Creating Layer ReLU31
I0905 01:04:50.419837 73306 net.cpp:434] ReLU31 <- BatchNorm31
I0905 01:04:50.419845 73306 net.cpp:395] ReLU31 -> BatchNorm31 (in-place)
I0905 01:04:50.421831 73306 net.cpp:150] Setting up ReLU31
I0905 01:04:50.421851 73306 net.cpp:157] Top shape: 16 352 16 16 (1441792)
I0905 01:04:50.421862 73306 net.cpp:165] Memory required for data: 3247439936
I0905 01:04:50.421869 73306 layer_factory.hpp:77] Creating layer Convolution32
I0905 01:04:50.421885 73306 net.cpp:100] Creating Layer Convolution32
I0905 01:04:50.421893 73306 net.cpp:434] Convolution32 <- BatchNorm31
I0905 01:04:50.421903 73306 net.cpp:408] Convolution32 -> Convolution32
I0905 01:04:50.423784 73306 net.cpp:150] Setting up Convolution32
I0905 01:04:50.423806 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.423816 73306 net.cpp:165] Memory required for data: 3247636544
I0905 01:04:50.423826 73306 layer_factory.hpp:77] Creating layer Dropout32
I0905 01:04:50.423840 73306 net.cpp:100] Creating Layer Dropout32
I0905 01:04:50.423847 73306 net.cpp:434] Dropout32 <- Convolution32
I0905 01:04:50.423856 73306 net.cpp:408] Dropout32 -> Dropout32
I0905 01:04:50.423907 73306 net.cpp:150] Setting up Dropout32
I0905 01:04:50.423918 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.423925 73306 net.cpp:165] Memory required for data: 3247833152
I0905 01:04:50.423933 73306 layer_factory.hpp:77] Creating layer Concat29
I0905 01:04:50.423941 73306 net.cpp:100] Creating Layer Concat29
I0905 01:04:50.423949 73306 net.cpp:434] Concat29 <- Concat28_Concat28_0_split_1
I0905 01:04:50.423969 73306 net.cpp:434] Concat29 <- Dropout32
I0905 01:04:50.423980 73306 net.cpp:408] Concat29 -> Concat29
I0905 01:04:50.424013 73306 net.cpp:150] Setting up Concat29
I0905 01:04:50.424029 73306 net.cpp:157] Top shape: 16 364 16 16 (1490944)
I0905 01:04:50.424036 73306 net.cpp:165] Memory required for data: 3253796928
I0905 01:04:50.424042 73306 layer_factory.hpp:77] Creating layer Concat29_Concat29_0_split
I0905 01:04:50.424052 73306 net.cpp:100] Creating Layer Concat29_Concat29_0_split
I0905 01:04:50.424060 73306 net.cpp:434] Concat29_Concat29_0_split <- Concat29
I0905 01:04:50.424068 73306 net.cpp:408] Concat29_Concat29_0_split -> Concat29_Concat29_0_split_0
I0905 01:04:50.424078 73306 net.cpp:408] Concat29_Concat29_0_split -> Concat29_Concat29_0_split_1
I0905 01:04:50.424120 73306 net.cpp:150] Setting up Concat29_Concat29_0_split
I0905 01:04:50.424130 73306 net.cpp:157] Top shape: 16 364 16 16 (1490944)
I0905 01:04:50.424139 73306 net.cpp:157] Top shape: 16 364 16 16 (1490944)
I0905 01:04:50.424145 73306 net.cpp:165] Memory required for data: 3265724480
I0905 01:04:50.424152 73306 layer_factory.hpp:77] Creating layer BatchNorm32
I0905 01:04:50.424175 73306 net.cpp:100] Creating Layer BatchNorm32
I0905 01:04:50.424181 73306 net.cpp:434] BatchNorm32 <- Concat29_Concat29_0_split_0
I0905 01:04:50.424190 73306 net.cpp:408] BatchNorm32 -> BatchNorm32
I0905 01:04:50.424394 73306 net.cpp:150] Setting up BatchNorm32
I0905 01:04:50.424408 73306 net.cpp:157] Top shape: 16 364 16 16 (1490944)
I0905 01:04:50.424417 73306 net.cpp:165] Memory required for data: 3271688256
I0905 01:04:50.424427 73306 layer_factory.hpp:77] Creating layer Scale32
I0905 01:04:50.424438 73306 net.cpp:100] Creating Layer Scale32
I0905 01:04:50.424445 73306 net.cpp:434] Scale32 <- BatchNorm32
I0905 01:04:50.424456 73306 net.cpp:395] Scale32 -> BatchNorm32 (in-place)
I0905 01:04:50.424556 73306 net.cpp:150] Setting up Scale32
I0905 01:04:50.424568 73306 net.cpp:157] Top shape: 16 364 16 16 (1490944)
I0905 01:04:50.424577 73306 net.cpp:165] Memory required for data: 3277652032
I0905 01:04:50.424585 73306 layer_factory.hpp:77] Creating layer ReLU32
I0905 01:04:50.424597 73306 net.cpp:100] Creating Layer ReLU32
I0905 01:04:50.424604 73306 net.cpp:434] ReLU32 <- BatchNorm32
I0905 01:04:50.424612 73306 net.cpp:395] ReLU32 -> BatchNorm32 (in-place)
I0905 01:04:50.424948 73306 net.cpp:150] Setting up ReLU32
I0905 01:04:50.424967 73306 net.cpp:157] Top shape: 16 364 16 16 (1490944)
I0905 01:04:50.424978 73306 net.cpp:165] Memory required for data: 3283615808
I0905 01:04:50.424984 73306 layer_factory.hpp:77] Creating layer Convolution33
I0905 01:04:50.425003 73306 net.cpp:100] Creating Layer Convolution33
I0905 01:04:50.425011 73306 net.cpp:434] Convolution33 <- BatchNorm32
I0905 01:04:50.425021 73306 net.cpp:408] Convolution33 -> Convolution33
I0905 01:04:50.427633 73306 net.cpp:150] Setting up Convolution33
I0905 01:04:50.427654 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.427665 73306 net.cpp:165] Memory required for data: 3283812416
I0905 01:04:50.427676 73306 layer_factory.hpp:77] Creating layer Dropout33
I0905 01:04:50.427691 73306 net.cpp:100] Creating Layer Dropout33
I0905 01:04:50.427700 73306 net.cpp:434] Dropout33 <- Convolution33
I0905 01:04:50.427708 73306 net.cpp:408] Dropout33 -> Dropout33
I0905 01:04:50.427752 73306 net.cpp:150] Setting up Dropout33
I0905 01:04:50.427767 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.427774 73306 net.cpp:165] Memory required for data: 3284009024
I0905 01:04:50.427781 73306 layer_factory.hpp:77] Creating layer Concat30
I0905 01:04:50.427790 73306 net.cpp:100] Creating Layer Concat30
I0905 01:04:50.427798 73306 net.cpp:434] Concat30 <- Concat29_Concat29_0_split_1
I0905 01:04:50.427806 73306 net.cpp:434] Concat30 <- Dropout33
I0905 01:04:50.427817 73306 net.cpp:408] Concat30 -> Concat30
I0905 01:04:50.427845 73306 net.cpp:150] Setting up Concat30
I0905 01:04:50.427855 73306 net.cpp:157] Top shape: 16 376 16 16 (1540096)
I0905 01:04:50.427863 73306 net.cpp:165] Memory required for data: 3290169408
I0905 01:04:50.427881 73306 layer_factory.hpp:77] Creating layer Concat30_Concat30_0_split
I0905 01:04:50.427892 73306 net.cpp:100] Creating Layer Concat30_Concat30_0_split
I0905 01:04:50.427899 73306 net.cpp:434] Concat30_Concat30_0_split <- Concat30
I0905 01:04:50.427908 73306 net.cpp:408] Concat30_Concat30_0_split -> Concat30_Concat30_0_split_0
I0905 01:04:50.427917 73306 net.cpp:408] Concat30_Concat30_0_split -> Concat30_Concat30_0_split_1
I0905 01:04:50.427973 73306 net.cpp:150] Setting up Concat30_Concat30_0_split
I0905 01:04:50.427984 73306 net.cpp:157] Top shape: 16 376 16 16 (1540096)
I0905 01:04:50.427991 73306 net.cpp:157] Top shape: 16 376 16 16 (1540096)
I0905 01:04:50.427999 73306 net.cpp:165] Memory required for data: 3302490176
I0905 01:04:50.428004 73306 layer_factory.hpp:77] Creating layer BatchNorm33
I0905 01:04:50.428016 73306 net.cpp:100] Creating Layer BatchNorm33
I0905 01:04:50.428025 73306 net.cpp:434] BatchNorm33 <- Concat30_Concat30_0_split_0
I0905 01:04:50.428032 73306 net.cpp:408] BatchNorm33 -> BatchNorm33
I0905 01:04:50.428243 73306 net.cpp:150] Setting up BatchNorm33
I0905 01:04:50.428258 73306 net.cpp:157] Top shape: 16 376 16 16 (1540096)
I0905 01:04:50.428268 73306 net.cpp:165] Memory required for data: 3308650560
I0905 01:04:50.428279 73306 layer_factory.hpp:77] Creating layer Scale33
I0905 01:04:50.428292 73306 net.cpp:100] Creating Layer Scale33
I0905 01:04:50.428300 73306 net.cpp:434] Scale33 <- BatchNorm33
I0905 01:04:50.428308 73306 net.cpp:395] Scale33 -> BatchNorm33 (in-place)
I0905 01:04:50.428409 73306 net.cpp:150] Setting up Scale33
I0905 01:04:50.428422 73306 net.cpp:157] Top shape: 16 376 16 16 (1540096)
I0905 01:04:50.428429 73306 net.cpp:165] Memory required for data: 3314810944
I0905 01:04:50.428437 73306 layer_factory.hpp:77] Creating layer ReLU33
I0905 01:04:50.428452 73306 net.cpp:100] Creating Layer ReLU33
I0905 01:04:50.428457 73306 net.cpp:434] ReLU33 <- BatchNorm33
I0905 01:04:50.428467 73306 net.cpp:395] ReLU33 -> BatchNorm33 (in-place)
I0905 01:04:50.428635 73306 net.cpp:150] Setting up ReLU33
I0905 01:04:50.428658 73306 net.cpp:157] Top shape: 16 376 16 16 (1540096)
I0905 01:04:50.428665 73306 net.cpp:165] Memory required for data: 3320971328
I0905 01:04:50.428673 73306 layer_factory.hpp:77] Creating layer Convolution34
I0905 01:04:50.428686 73306 net.cpp:100] Creating Layer Convolution34
I0905 01:04:50.428694 73306 net.cpp:434] Convolution34 <- BatchNorm33
I0905 01:04:50.428705 73306 net.cpp:408] Convolution34 -> Convolution34
I0905 01:04:50.430790 73306 net.cpp:150] Setting up Convolution34
I0905 01:04:50.430810 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.430821 73306 net.cpp:165] Memory required for data: 3321167936
I0905 01:04:50.430831 73306 layer_factory.hpp:77] Creating layer Dropout34
I0905 01:04:50.430848 73306 net.cpp:100] Creating Layer Dropout34
I0905 01:04:50.430856 73306 net.cpp:434] Dropout34 <- Convolution34
I0905 01:04:50.430866 73306 net.cpp:408] Dropout34 -> Dropout34
I0905 01:04:50.430912 73306 net.cpp:150] Setting up Dropout34
I0905 01:04:50.430923 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.430929 73306 net.cpp:165] Memory required for data: 3321364544
I0905 01:04:50.430937 73306 layer_factory.hpp:77] Creating layer Concat31
I0905 01:04:50.430948 73306 net.cpp:100] Creating Layer Concat31
I0905 01:04:50.430956 73306 net.cpp:434] Concat31 <- Concat30_Concat30_0_split_1
I0905 01:04:50.430964 73306 net.cpp:434] Concat31 <- Dropout34
I0905 01:04:50.430975 73306 net.cpp:408] Concat31 -> Concat31
I0905 01:04:50.431004 73306 net.cpp:150] Setting up Concat31
I0905 01:04:50.431015 73306 net.cpp:157] Top shape: 16 388 16 16 (1589248)
I0905 01:04:50.431021 73306 net.cpp:165] Memory required for data: 3327721536
I0905 01:04:50.431027 73306 layer_factory.hpp:77] Creating layer Concat31_Concat31_0_split
I0905 01:04:50.431038 73306 net.cpp:100] Creating Layer Concat31_Concat31_0_split
I0905 01:04:50.431046 73306 net.cpp:434] Concat31_Concat31_0_split <- Concat31
I0905 01:04:50.431066 73306 net.cpp:408] Concat31_Concat31_0_split -> Concat31_Concat31_0_split_0
I0905 01:04:50.431077 73306 net.cpp:408] Concat31_Concat31_0_split -> Concat31_Concat31_0_split_1
I0905 01:04:50.431121 73306 net.cpp:150] Setting up Concat31_Concat31_0_split
I0905 01:04:50.431138 73306 net.cpp:157] Top shape: 16 388 16 16 (1589248)
I0905 01:04:50.431146 73306 net.cpp:157] Top shape: 16 388 16 16 (1589248)
I0905 01:04:50.431152 73306 net.cpp:165] Memory required for data: 3340435520
I0905 01:04:50.431159 73306 layer_factory.hpp:77] Creating layer BatchNorm34
I0905 01:04:50.431170 73306 net.cpp:100] Creating Layer BatchNorm34
I0905 01:04:50.431177 73306 net.cpp:434] BatchNorm34 <- Concat31_Concat31_0_split_0
I0905 01:04:50.431187 73306 net.cpp:408] BatchNorm34 -> BatchNorm34
I0905 01:04:50.431390 73306 net.cpp:150] Setting up BatchNorm34
I0905 01:04:50.431402 73306 net.cpp:157] Top shape: 16 388 16 16 (1589248)
I0905 01:04:50.431411 73306 net.cpp:165] Memory required for data: 3346792512
I0905 01:04:50.431421 73306 layer_factory.hpp:77] Creating layer Scale34
I0905 01:04:50.431439 73306 net.cpp:100] Creating Layer Scale34
I0905 01:04:50.431447 73306 net.cpp:434] Scale34 <- BatchNorm34
I0905 01:04:50.431455 73306 net.cpp:395] Scale34 -> BatchNorm34 (in-place)
I0905 01:04:50.431560 73306 net.cpp:150] Setting up Scale34
I0905 01:04:50.431574 73306 net.cpp:157] Top shape: 16 388 16 16 (1589248)
I0905 01:04:50.431584 73306 net.cpp:165] Memory required for data: 3353149504
I0905 01:04:50.431592 73306 layer_factory.hpp:77] Creating layer ReLU34
I0905 01:04:50.431603 73306 net.cpp:100] Creating Layer ReLU34
I0905 01:04:50.431610 73306 net.cpp:434] ReLU34 <- BatchNorm34
I0905 01:04:50.431625 73306 net.cpp:395] ReLU34 -> BatchNorm34 (in-place)
I0905 01:04:50.431927 73306 net.cpp:150] Setting up ReLU34
I0905 01:04:50.431946 73306 net.cpp:157] Top shape: 16 388 16 16 (1589248)
I0905 01:04:50.431957 73306 net.cpp:165] Memory required for data: 3359506496
I0905 01:04:50.431963 73306 layer_factory.hpp:77] Creating layer Convolution35
I0905 01:04:50.431980 73306 net.cpp:100] Creating Layer Convolution35
I0905 01:04:50.431989 73306 net.cpp:434] Convolution35 <- BatchNorm34
I0905 01:04:50.431999 73306 net.cpp:408] Convolution35 -> Convolution35
I0905 01:04:50.434134 73306 net.cpp:150] Setting up Convolution35
I0905 01:04:50.434154 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.434164 73306 net.cpp:165] Memory required for data: 3359703104
I0905 01:04:50.434173 73306 layer_factory.hpp:77] Creating layer Dropout35
I0905 01:04:50.434185 73306 net.cpp:100] Creating Layer Dropout35
I0905 01:04:50.434191 73306 net.cpp:434] Dropout35 <- Convolution35
I0905 01:04:50.434201 73306 net.cpp:408] Dropout35 -> Dropout35
I0905 01:04:50.434249 73306 net.cpp:150] Setting up Dropout35
I0905 01:04:50.434260 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.434267 73306 net.cpp:165] Memory required for data: 3359899712
I0905 01:04:50.434274 73306 layer_factory.hpp:77] Creating layer Concat32
I0905 01:04:50.434284 73306 net.cpp:100] Creating Layer Concat32
I0905 01:04:50.434293 73306 net.cpp:434] Concat32 <- Concat31_Concat31_0_split_1
I0905 01:04:50.434300 73306 net.cpp:434] Concat32 <- Dropout35
I0905 01:04:50.434309 73306 net.cpp:408] Concat32 -> Concat32
I0905 01:04:50.434340 73306 net.cpp:150] Setting up Concat32
I0905 01:04:50.434350 73306 net.cpp:157] Top shape: 16 400 16 16 (1638400)
I0905 01:04:50.434358 73306 net.cpp:165] Memory required for data: 3366453312
I0905 01:04:50.434365 73306 layer_factory.hpp:77] Creating layer Concat32_Concat32_0_split
I0905 01:04:50.434381 73306 net.cpp:100] Creating Layer Concat32_Concat32_0_split
I0905 01:04:50.434388 73306 net.cpp:434] Concat32_Concat32_0_split <- Concat32
I0905 01:04:50.434396 73306 net.cpp:408] Concat32_Concat32_0_split -> Concat32_Concat32_0_split_0
I0905 01:04:50.434406 73306 net.cpp:408] Concat32_Concat32_0_split -> Concat32_Concat32_0_split_1
I0905 01:04:50.434448 73306 net.cpp:150] Setting up Concat32_Concat32_0_split
I0905 01:04:50.434458 73306 net.cpp:157] Top shape: 16 400 16 16 (1638400)
I0905 01:04:50.434476 73306 net.cpp:157] Top shape: 16 400 16 16 (1638400)
I0905 01:04:50.434484 73306 net.cpp:165] Memory required for data: 3379560512
I0905 01:04:50.434491 73306 layer_factory.hpp:77] Creating layer BatchNorm35
I0905 01:04:50.434501 73306 net.cpp:100] Creating Layer BatchNorm35
I0905 01:04:50.434509 73306 net.cpp:434] BatchNorm35 <- Concat32_Concat32_0_split_0
I0905 01:04:50.434519 73306 net.cpp:408] BatchNorm35 -> BatchNorm35
I0905 01:04:50.434768 73306 net.cpp:150] Setting up BatchNorm35
I0905 01:04:50.434782 73306 net.cpp:157] Top shape: 16 400 16 16 (1638400)
I0905 01:04:50.434803 73306 net.cpp:165] Memory required for data: 3386114112
I0905 01:04:50.434814 73306 layer_factory.hpp:77] Creating layer Scale35
I0905 01:04:50.434824 73306 net.cpp:100] Creating Layer Scale35
I0905 01:04:50.434833 73306 net.cpp:434] Scale35 <- BatchNorm35
I0905 01:04:50.434840 73306 net.cpp:395] Scale35 -> BatchNorm35 (in-place)
I0905 01:04:50.434938 73306 net.cpp:150] Setting up Scale35
I0905 01:04:50.434952 73306 net.cpp:157] Top shape: 16 400 16 16 (1638400)
I0905 01:04:50.434962 73306 net.cpp:165] Memory required for data: 3392667712
I0905 01:04:50.434970 73306 layer_factory.hpp:77] Creating layer ReLU35
I0905 01:04:50.434983 73306 net.cpp:100] Creating Layer ReLU35
I0905 01:04:50.434995 73306 net.cpp:434] ReLU35 <- BatchNorm35
I0905 01:04:50.435005 73306 net.cpp:395] ReLU35 -> BatchNorm35 (in-place)
I0905 01:04:50.435310 73306 net.cpp:150] Setting up ReLU35
I0905 01:04:50.435328 73306 net.cpp:157] Top shape: 16 400 16 16 (1638400)
I0905 01:04:50.435338 73306 net.cpp:165] Memory required for data: 3399221312
I0905 01:04:50.435345 73306 layer_factory.hpp:77] Creating layer Convolution36
I0905 01:04:50.435359 73306 net.cpp:100] Creating Layer Convolution36
I0905 01:04:50.435367 73306 net.cpp:434] Convolution36 <- BatchNorm35
I0905 01:04:50.435379 73306 net.cpp:408] Convolution36 -> Convolution36
I0905 01:04:50.437546 73306 net.cpp:150] Setting up Convolution36
I0905 01:04:50.437568 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.437579 73306 net.cpp:165] Memory required for data: 3399417920
I0905 01:04:50.437589 73306 layer_factory.hpp:77] Creating layer Dropout36
I0905 01:04:50.437603 73306 net.cpp:100] Creating Layer Dropout36
I0905 01:04:50.437611 73306 net.cpp:434] Dropout36 <- Convolution36
I0905 01:04:50.437620 73306 net.cpp:408] Dropout36 -> Dropout36
I0905 01:04:50.437674 73306 net.cpp:150] Setting up Dropout36
I0905 01:04:50.437686 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.437693 73306 net.cpp:165] Memory required for data: 3399614528
I0905 01:04:50.437700 73306 layer_factory.hpp:77] Creating layer Concat33
I0905 01:04:50.437711 73306 net.cpp:100] Creating Layer Concat33
I0905 01:04:50.437719 73306 net.cpp:434] Concat33 <- Concat32_Concat32_0_split_1
I0905 01:04:50.437727 73306 net.cpp:434] Concat33 <- Dropout36
I0905 01:04:50.437738 73306 net.cpp:408] Concat33 -> Concat33
I0905 01:04:50.437767 73306 net.cpp:150] Setting up Concat33
I0905 01:04:50.437782 73306 net.cpp:157] Top shape: 16 412 16 16 (1687552)
I0905 01:04:50.437788 73306 net.cpp:165] Memory required for data: 3406364736
I0905 01:04:50.437798 73306 layer_factory.hpp:77] Creating layer Concat33_Concat33_0_split
I0905 01:04:50.437808 73306 net.cpp:100] Creating Layer Concat33_Concat33_0_split
I0905 01:04:50.437814 73306 net.cpp:434] Concat33_Concat33_0_split <- Concat33
I0905 01:04:50.437822 73306 net.cpp:408] Concat33_Concat33_0_split -> Concat33_Concat33_0_split_0
I0905 01:04:50.437831 73306 net.cpp:408] Concat33_Concat33_0_split -> Concat33_Concat33_0_split_1
I0905 01:04:50.437875 73306 net.cpp:150] Setting up Concat33_Concat33_0_split
I0905 01:04:50.437885 73306 net.cpp:157] Top shape: 16 412 16 16 (1687552)
I0905 01:04:50.437891 73306 net.cpp:157] Top shape: 16 412 16 16 (1687552)
I0905 01:04:50.437897 73306 net.cpp:165] Memory required for data: 3419865152
I0905 01:04:50.437904 73306 layer_factory.hpp:77] Creating layer BatchNorm36
I0905 01:04:50.437916 73306 net.cpp:100] Creating Layer BatchNorm36
I0905 01:04:50.437935 73306 net.cpp:434] BatchNorm36 <- Concat33_Concat33_0_split_0
I0905 01:04:50.437944 73306 net.cpp:408] BatchNorm36 -> BatchNorm36
I0905 01:04:50.438169 73306 net.cpp:150] Setting up BatchNorm36
I0905 01:04:50.438182 73306 net.cpp:157] Top shape: 16 412 16 16 (1687552)
I0905 01:04:50.438189 73306 net.cpp:165] Memory required for data: 3426615360
I0905 01:04:50.438201 73306 layer_factory.hpp:77] Creating layer Scale36
I0905 01:04:50.438212 73306 net.cpp:100] Creating Layer Scale36
I0905 01:04:50.438220 73306 net.cpp:434] Scale36 <- BatchNorm36
I0905 01:04:50.438227 73306 net.cpp:395] Scale36 -> BatchNorm36 (in-place)
I0905 01:04:50.438328 73306 net.cpp:150] Setting up Scale36
I0905 01:04:50.438341 73306 net.cpp:157] Top shape: 16 412 16 16 (1687552)
I0905 01:04:50.438349 73306 net.cpp:165] Memory required for data: 3433365568
I0905 01:04:50.438357 73306 layer_factory.hpp:77] Creating layer ReLU36
I0905 01:04:50.438369 73306 net.cpp:100] Creating Layer ReLU36
I0905 01:04:50.438375 73306 net.cpp:434] ReLU36 <- BatchNorm36
I0905 01:04:50.438383 73306 net.cpp:395] ReLU36 -> BatchNorm36 (in-place)
I0905 01:04:50.438551 73306 net.cpp:150] Setting up ReLU36
I0905 01:04:50.438566 73306 net.cpp:157] Top shape: 16 412 16 16 (1687552)
I0905 01:04:50.438576 73306 net.cpp:165] Memory required for data: 3440115776
I0905 01:04:50.438585 73306 layer_factory.hpp:77] Creating layer Convolution37
I0905 01:04:50.438598 73306 net.cpp:100] Creating Layer Convolution37
I0905 01:04:50.438607 73306 net.cpp:434] Convolution37 <- BatchNorm36
I0905 01:04:50.438618 73306 net.cpp:408] Convolution37 -> Convolution37
I0905 01:04:50.440953 73306 net.cpp:150] Setting up Convolution37
I0905 01:04:50.440973 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.440986 73306 net.cpp:165] Memory required for data: 3440312384
I0905 01:04:50.440996 73306 layer_factory.hpp:77] Creating layer Dropout37
I0905 01:04:50.441009 73306 net.cpp:100] Creating Layer Dropout37
I0905 01:04:50.441017 73306 net.cpp:434] Dropout37 <- Convolution37
I0905 01:04:50.441026 73306 net.cpp:408] Dropout37 -> Dropout37
I0905 01:04:50.441072 73306 net.cpp:150] Setting up Dropout37
I0905 01:04:50.441084 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.441092 73306 net.cpp:165] Memory required for data: 3440508992
I0905 01:04:50.441098 73306 layer_factory.hpp:77] Creating layer Concat34
I0905 01:04:50.441107 73306 net.cpp:100] Creating Layer Concat34
I0905 01:04:50.441118 73306 net.cpp:434] Concat34 <- Concat33_Concat33_0_split_1
I0905 01:04:50.441125 73306 net.cpp:434] Concat34 <- Dropout37
I0905 01:04:50.441136 73306 net.cpp:408] Concat34 -> Concat34
I0905 01:04:50.441166 73306 net.cpp:150] Setting up Concat34
I0905 01:04:50.441182 73306 net.cpp:157] Top shape: 16 424 16 16 (1736704)
I0905 01:04:50.441190 73306 net.cpp:165] Memory required for data: 3447455808
I0905 01:04:50.441197 73306 layer_factory.hpp:77] Creating layer Concat34_Concat34_0_split
I0905 01:04:50.441211 73306 net.cpp:100] Creating Layer Concat34_Concat34_0_split
I0905 01:04:50.441218 73306 net.cpp:434] Concat34_Concat34_0_split <- Concat34
I0905 01:04:50.441227 73306 net.cpp:408] Concat34_Concat34_0_split -> Concat34_Concat34_0_split_0
I0905 01:04:50.441236 73306 net.cpp:408] Concat34_Concat34_0_split -> Concat34_Concat34_0_split_1
I0905 01:04:50.441280 73306 net.cpp:150] Setting up Concat34_Concat34_0_split
I0905 01:04:50.441292 73306 net.cpp:157] Top shape: 16 424 16 16 (1736704)
I0905 01:04:50.441298 73306 net.cpp:157] Top shape: 16 424 16 16 (1736704)
I0905 01:04:50.441305 73306 net.cpp:165] Memory required for data: 3461349440
I0905 01:04:50.441311 73306 layer_factory.hpp:77] Creating layer BatchNorm37
I0905 01:04:50.441323 73306 net.cpp:100] Creating Layer BatchNorm37
I0905 01:04:50.441329 73306 net.cpp:434] BatchNorm37 <- Concat34_Concat34_0_split_0
I0905 01:04:50.441337 73306 net.cpp:408] BatchNorm37 -> BatchNorm37
I0905 01:04:50.441545 73306 net.cpp:150] Setting up BatchNorm37
I0905 01:04:50.441560 73306 net.cpp:157] Top shape: 16 424 16 16 (1736704)
I0905 01:04:50.441578 73306 net.cpp:165] Memory required for data: 3468296256
I0905 01:04:50.441591 73306 layer_factory.hpp:77] Creating layer Scale37
I0905 01:04:50.441602 73306 net.cpp:100] Creating Layer Scale37
I0905 01:04:50.441612 73306 net.cpp:434] Scale37 <- BatchNorm37
I0905 01:04:50.441619 73306 net.cpp:395] Scale37 -> BatchNorm37 (in-place)
I0905 01:04:50.441722 73306 net.cpp:150] Setting up Scale37
I0905 01:04:50.441736 73306 net.cpp:157] Top shape: 16 424 16 16 (1736704)
I0905 01:04:50.441745 73306 net.cpp:165] Memory required for data: 3475243072
I0905 01:04:50.441761 73306 layer_factory.hpp:77] Creating layer ReLU37
I0905 01:04:50.441798 73306 net.cpp:100] Creating Layer ReLU37
I0905 01:04:50.441807 73306 net.cpp:434] ReLU37 <- BatchNorm37
I0905 01:04:50.441815 73306 net.cpp:395] ReLU37 -> BatchNorm37 (in-place)
I0905 01:04:50.442131 73306 net.cpp:150] Setting up ReLU37
I0905 01:04:50.442149 73306 net.cpp:157] Top shape: 16 424 16 16 (1736704)
I0905 01:04:50.442160 73306 net.cpp:165] Memory required for data: 3482189888
I0905 01:04:50.442168 73306 layer_factory.hpp:77] Creating layer Convolution38
I0905 01:04:50.442185 73306 net.cpp:100] Creating Layer Convolution38
I0905 01:04:50.442193 73306 net.cpp:434] Convolution38 <- BatchNorm37
I0905 01:04:50.442203 73306 net.cpp:408] Convolution38 -> Convolution38
I0905 01:04:50.445497 73306 net.cpp:150] Setting up Convolution38
I0905 01:04:50.445534 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.445544 73306 net.cpp:165] Memory required for data: 3482386496
I0905 01:04:50.445564 73306 layer_factory.hpp:77] Creating layer Dropout38
I0905 01:04:50.445588 73306 net.cpp:100] Creating Layer Dropout38
I0905 01:04:50.445610 73306 net.cpp:434] Dropout38 <- Convolution38
I0905 01:04:50.445624 73306 net.cpp:408] Dropout38 -> Dropout38
I0905 01:04:50.445674 73306 net.cpp:150] Setting up Dropout38
I0905 01:04:50.445688 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.445698 73306 net.cpp:165] Memory required for data: 3482583104
I0905 01:04:50.445704 73306 layer_factory.hpp:77] Creating layer Concat35
I0905 01:04:50.445719 73306 net.cpp:100] Creating Layer Concat35
I0905 01:04:50.445731 73306 net.cpp:434] Concat35 <- Concat34_Concat34_0_split_1
I0905 01:04:50.445739 73306 net.cpp:434] Concat35 <- Dropout38
I0905 01:04:50.445747 73306 net.cpp:408] Concat35 -> Concat35
I0905 01:04:50.445780 73306 net.cpp:150] Setting up Concat35
I0905 01:04:50.445791 73306 net.cpp:157] Top shape: 16 436 16 16 (1785856)
I0905 01:04:50.445798 73306 net.cpp:165] Memory required for data: 3489726528
I0905 01:04:50.445804 73306 layer_factory.hpp:77] Creating layer Concat35_Concat35_0_split
I0905 01:04:50.445816 73306 net.cpp:100] Creating Layer Concat35_Concat35_0_split
I0905 01:04:50.445822 73306 net.cpp:434] Concat35_Concat35_0_split <- Concat35
I0905 01:04:50.445832 73306 net.cpp:408] Concat35_Concat35_0_split -> Concat35_Concat35_0_split_0
I0905 01:04:50.445843 73306 net.cpp:408] Concat35_Concat35_0_split -> Concat35_Concat35_0_split_1
I0905 01:04:50.445885 73306 net.cpp:150] Setting up Concat35_Concat35_0_split
I0905 01:04:50.445896 73306 net.cpp:157] Top shape: 16 436 16 16 (1785856)
I0905 01:04:50.445912 73306 net.cpp:157] Top shape: 16 436 16 16 (1785856)
I0905 01:04:50.445919 73306 net.cpp:165] Memory required for data: 3504013376
I0905 01:04:50.445925 73306 layer_factory.hpp:77] Creating layer BatchNorm38
I0905 01:04:50.445935 73306 net.cpp:100] Creating Layer BatchNorm38
I0905 01:04:50.445945 73306 net.cpp:434] BatchNorm38 <- Concat35_Concat35_0_split_0
I0905 01:04:50.445958 73306 net.cpp:408] BatchNorm38 -> BatchNorm38
I0905 01:04:50.446246 73306 net.cpp:150] Setting up BatchNorm38
I0905 01:04:50.446261 73306 net.cpp:157] Top shape: 16 436 16 16 (1785856)
I0905 01:04:50.446272 73306 net.cpp:165] Memory required for data: 3511156800
I0905 01:04:50.446283 73306 layer_factory.hpp:77] Creating layer Scale38
I0905 01:04:50.446295 73306 net.cpp:100] Creating Layer Scale38
I0905 01:04:50.446302 73306 net.cpp:434] Scale38 <- BatchNorm38
I0905 01:04:50.446326 73306 net.cpp:395] Scale38 -> BatchNorm38 (in-place)
I0905 01:04:50.446452 73306 net.cpp:150] Setting up Scale38
I0905 01:04:50.446466 73306 net.cpp:157] Top shape: 16 436 16 16 (1785856)
I0905 01:04:50.446473 73306 net.cpp:165] Memory required for data: 3518300224
I0905 01:04:50.446482 73306 layer_factory.hpp:77] Creating layer ReLU38
I0905 01:04:50.446492 73306 net.cpp:100] Creating Layer ReLU38
I0905 01:04:50.446499 73306 net.cpp:434] ReLU38 <- BatchNorm38
I0905 01:04:50.446509 73306 net.cpp:395] ReLU38 -> BatchNorm38 (in-place)
I0905 01:04:50.446823 73306 net.cpp:150] Setting up ReLU38
I0905 01:04:50.446842 73306 net.cpp:157] Top shape: 16 436 16 16 (1785856)
I0905 01:04:50.446853 73306 net.cpp:165] Memory required for data: 3525443648
I0905 01:04:50.446861 73306 layer_factory.hpp:77] Creating layer Convolution39
I0905 01:04:50.446878 73306 net.cpp:100] Creating Layer Convolution39
I0905 01:04:50.446887 73306 net.cpp:434] Convolution39 <- BatchNorm38
I0905 01:04:50.446899 73306 net.cpp:408] Convolution39 -> Convolution39
I0905 01:04:50.449802 73306 net.cpp:150] Setting up Convolution39
I0905 01:04:50.449826 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.449833 73306 net.cpp:165] Memory required for data: 3525640256
I0905 01:04:50.449844 73306 layer_factory.hpp:77] Creating layer Dropout39
I0905 01:04:50.449854 73306 net.cpp:100] Creating Layer Dropout39
I0905 01:04:50.449862 73306 net.cpp:434] Dropout39 <- Convolution39
I0905 01:04:50.449872 73306 net.cpp:408] Dropout39 -> Dropout39
I0905 01:04:50.449918 73306 net.cpp:150] Setting up Dropout39
I0905 01:04:50.449930 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.449939 73306 net.cpp:165] Memory required for data: 3525836864
I0905 01:04:50.449946 73306 layer_factory.hpp:77] Creating layer Concat36
I0905 01:04:50.449956 73306 net.cpp:100] Creating Layer Concat36
I0905 01:04:50.449965 73306 net.cpp:434] Concat36 <- Concat35_Concat35_0_split_1
I0905 01:04:50.449973 73306 net.cpp:434] Concat36 <- Dropout39
I0905 01:04:50.449985 73306 net.cpp:408] Concat36 -> Concat36
I0905 01:04:50.450016 73306 net.cpp:150] Setting up Concat36
I0905 01:04:50.450029 73306 net.cpp:157] Top shape: 16 448 16 16 (1835008)
I0905 01:04:50.450037 73306 net.cpp:165] Memory required for data: 3533176896
I0905 01:04:50.450042 73306 layer_factory.hpp:77] Creating layer BatchNorm39
I0905 01:04:50.450052 73306 net.cpp:100] Creating Layer BatchNorm39
I0905 01:04:50.450058 73306 net.cpp:434] BatchNorm39 <- Concat36
I0905 01:04:50.450073 73306 net.cpp:408] BatchNorm39 -> BatchNorm39
I0905 01:04:50.450307 73306 net.cpp:150] Setting up BatchNorm39
I0905 01:04:50.450322 73306 net.cpp:157] Top shape: 16 448 16 16 (1835008)
I0905 01:04:50.450332 73306 net.cpp:165] Memory required for data: 3540516928
I0905 01:04:50.450343 73306 layer_factory.hpp:77] Creating layer Scale39
I0905 01:04:50.450356 73306 net.cpp:100] Creating Layer Scale39
I0905 01:04:50.450364 73306 net.cpp:434] Scale39 <- BatchNorm39
I0905 01:04:50.450372 73306 net.cpp:395] Scale39 -> BatchNorm39 (in-place)
I0905 01:04:50.450475 73306 net.cpp:150] Setting up Scale39
I0905 01:04:50.450489 73306 net.cpp:157] Top shape: 16 448 16 16 (1835008)
I0905 01:04:50.450500 73306 net.cpp:165] Memory required for data: 3547856960
I0905 01:04:50.450507 73306 layer_factory.hpp:77] Creating layer ReLU39
I0905 01:04:50.450528 73306 net.cpp:100] Creating Layer ReLU39
I0905 01:04:50.450536 73306 net.cpp:434] ReLU39 <- BatchNorm39
I0905 01:04:50.450544 73306 net.cpp:395] ReLU39 -> BatchNorm39 (in-place)
I0905 01:04:50.450729 73306 net.cpp:150] Setting up ReLU39
I0905 01:04:50.450753 73306 net.cpp:157] Top shape: 16 448 16 16 (1835008)
I0905 01:04:50.450762 73306 net.cpp:165] Memory required for data: 3555196992
I0905 01:04:50.450769 73306 layer_factory.hpp:77] Creating layer Pooling3
I0905 01:04:50.450781 73306 net.cpp:100] Creating Layer Pooling3
I0905 01:04:50.450789 73306 net.cpp:434] Pooling3 <- BatchNorm39
I0905 01:04:50.450800 73306 net.cpp:408] Pooling3 -> Pooling3
I0905 01:04:50.451114 73306 net.cpp:150] Setting up Pooling3
I0905 01:04:50.451146 73306 net.cpp:157] Top shape: 16 448 1 1 (7168)
I0905 01:04:50.451154 73306 net.cpp:165] Memory required for data: 3555225664
I0905 01:04:50.451161 73306 layer_factory.hpp:77] Creating layer InnerProduct1
I0905 01:04:50.451179 73306 net.cpp:100] Creating Layer InnerProduct1
I0905 01:04:50.451185 73306 net.cpp:434] InnerProduct1 <- Pooling3
I0905 01:04:50.451195 73306 net.cpp:408] InnerProduct1 -> InnerProduct1
I0905 01:04:50.451421 73306 net.cpp:150] Setting up InnerProduct1
I0905 01:04:50.451437 73306 net.cpp:157] Top shape: 16 2 (32)
I0905 01:04:50.451448 73306 net.cpp:165] Memory required for data: 3555225792
I0905 01:04:50.451457 73306 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0905 01:04:50.451473 73306 net.cpp:100] Creating Layer SoftmaxWithLoss1
I0905 01:04:50.451483 73306 net.cpp:434] SoftmaxWithLoss1 <- InnerProduct1
I0905 01:04:50.451490 73306 net.cpp:434] SoftmaxWithLoss1 <- Data2
I0905 01:04:50.451501 73306 net.cpp:408] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0905 01:04:50.451520 73306 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0905 01:04:50.451808 73306 net.cpp:150] Setting up SoftmaxWithLoss1
I0905 01:04:50.451824 73306 net.cpp:157] Top shape: (1)
I0905 01:04:50.451835 73306 net.cpp:160]     with loss weight 1
I0905 01:04:50.451871 73306 net.cpp:165] Memory required for data: 3555225796
I0905 01:04:50.451882 73306 net.cpp:226] SoftmaxWithLoss1 needs backward computation.
I0905 01:04:50.451891 73306 net.cpp:226] InnerProduct1 needs backward computation.
I0905 01:04:50.451899 73306 net.cpp:226] Pooling3 needs backward computation.
I0905 01:04:50.451905 73306 net.cpp:226] ReLU39 needs backward computation.
I0905 01:04:50.451915 73306 net.cpp:226] Scale39 needs backward computation.
I0905 01:04:50.451920 73306 net.cpp:226] BatchNorm39 needs backward computation.
I0905 01:04:50.451926 73306 net.cpp:226] Concat36 needs backward computation.
I0905 01:04:50.451933 73306 net.cpp:226] Dropout39 needs backward computation.
I0905 01:04:50.451941 73306 net.cpp:226] Convolution39 needs backward computation.
I0905 01:04:50.451946 73306 net.cpp:226] ReLU38 needs backward computation.
I0905 01:04:50.451952 73306 net.cpp:226] Scale38 needs backward computation.
I0905 01:04:50.451958 73306 net.cpp:226] BatchNorm38 needs backward computation.
I0905 01:04:50.451969 73306 net.cpp:226] Concat35_Concat35_0_split needs backward computation.
I0905 01:04:50.451977 73306 net.cpp:226] Concat35 needs backward computation.
I0905 01:04:50.451988 73306 net.cpp:226] Dropout38 needs backward computation.
I0905 01:04:50.451995 73306 net.cpp:226] Convolution38 needs backward computation.
I0905 01:04:50.452003 73306 net.cpp:226] ReLU37 needs backward computation.
I0905 01:04:50.452009 73306 net.cpp:226] Scale37 needs backward computation.
I0905 01:04:50.452016 73306 net.cpp:226] BatchNorm37 needs backward computation.
I0905 01:04:50.452023 73306 net.cpp:226] Concat34_Concat34_0_split needs backward computation.
I0905 01:04:50.452029 73306 net.cpp:226] Concat34 needs backward computation.
I0905 01:04:50.452036 73306 net.cpp:226] Dropout37 needs backward computation.
I0905 01:04:50.452044 73306 net.cpp:226] Convolution37 needs backward computation.
I0905 01:04:50.452050 73306 net.cpp:226] ReLU36 needs backward computation.
I0905 01:04:50.452056 73306 net.cpp:226] Scale36 needs backward computation.
I0905 01:04:50.452064 73306 net.cpp:226] BatchNorm36 needs backward computation.
I0905 01:04:50.452069 73306 net.cpp:226] Concat33_Concat33_0_split needs backward computation.
I0905 01:04:50.452076 73306 net.cpp:226] Concat33 needs backward computation.
I0905 01:04:50.452083 73306 net.cpp:226] Dropout36 needs backward computation.
I0905 01:04:50.452090 73306 net.cpp:226] Convolution36 needs backward computation.
I0905 01:04:50.452100 73306 net.cpp:226] ReLU35 needs backward computation.
I0905 01:04:50.452106 73306 net.cpp:226] Scale35 needs backward computation.
I0905 01:04:50.452112 73306 net.cpp:226] BatchNorm35 needs backward computation.
I0905 01:04:50.452119 73306 net.cpp:226] Concat32_Concat32_0_split needs backward computation.
I0905 01:04:50.452137 73306 net.cpp:226] Concat32 needs backward computation.
I0905 01:04:50.452144 73306 net.cpp:226] Dropout35 needs backward computation.
I0905 01:04:50.452157 73306 net.cpp:226] Convolution35 needs backward computation.
I0905 01:04:50.452163 73306 net.cpp:226] ReLU34 needs backward computation.
I0905 01:04:50.452169 73306 net.cpp:226] Scale34 needs backward computation.
I0905 01:04:50.452175 73306 net.cpp:226] BatchNorm34 needs backward computation.
I0905 01:04:50.452183 73306 net.cpp:226] Concat31_Concat31_0_split needs backward computation.
I0905 01:04:50.452190 73306 net.cpp:226] Concat31 needs backward computation.
I0905 01:04:50.452199 73306 net.cpp:226] Dropout34 needs backward computation.
I0905 01:04:50.452206 73306 net.cpp:226] Convolution34 needs backward computation.
I0905 01:04:50.452214 73306 net.cpp:226] ReLU33 needs backward computation.
I0905 01:04:50.452219 73306 net.cpp:226] Scale33 needs backward computation.
I0905 01:04:50.452234 73306 net.cpp:226] BatchNorm33 needs backward computation.
I0905 01:04:50.452241 73306 net.cpp:226] Concat30_Concat30_0_split needs backward computation.
I0905 01:04:50.452249 73306 net.cpp:226] Concat30 needs backward computation.
I0905 01:04:50.452255 73306 net.cpp:226] Dropout33 needs backward computation.
I0905 01:04:50.452261 73306 net.cpp:226] Convolution33 needs backward computation.
I0905 01:04:50.452270 73306 net.cpp:226] ReLU32 needs backward computation.
I0905 01:04:50.452275 73306 net.cpp:226] Scale32 needs backward computation.
I0905 01:04:50.452281 73306 net.cpp:226] BatchNorm32 needs backward computation.
I0905 01:04:50.452289 73306 net.cpp:226] Concat29_Concat29_0_split needs backward computation.
I0905 01:04:50.452296 73306 net.cpp:226] Concat29 needs backward computation.
I0905 01:04:50.452303 73306 net.cpp:226] Dropout32 needs backward computation.
I0905 01:04:50.452311 73306 net.cpp:226] Convolution32 needs backward computation.
I0905 01:04:50.452319 73306 net.cpp:226] ReLU31 needs backward computation.
I0905 01:04:50.452327 73306 net.cpp:226] Scale31 needs backward computation.
I0905 01:04:50.452334 73306 net.cpp:226] BatchNorm31 needs backward computation.
I0905 01:04:50.452342 73306 net.cpp:226] Concat28_Concat28_0_split needs backward computation.
I0905 01:04:50.452349 73306 net.cpp:226] Concat28 needs backward computation.
I0905 01:04:50.452356 73306 net.cpp:226] Dropout31 needs backward computation.
I0905 01:04:50.452364 73306 net.cpp:226] Convolution31 needs backward computation.
I0905 01:04:50.452373 73306 net.cpp:226] ReLU30 needs backward computation.
I0905 01:04:50.452379 73306 net.cpp:226] Scale30 needs backward computation.
I0905 01:04:50.452386 73306 net.cpp:226] BatchNorm30 needs backward computation.
I0905 01:04:50.452394 73306 net.cpp:226] Concat27_Concat27_0_split needs backward computation.
I0905 01:04:50.452400 73306 net.cpp:226] Concat27 needs backward computation.
I0905 01:04:50.452407 73306 net.cpp:226] Dropout30 needs backward computation.
I0905 01:04:50.452415 73306 net.cpp:226] Convolution30 needs backward computation.
I0905 01:04:50.452425 73306 net.cpp:226] ReLU29 needs backward computation.
I0905 01:04:50.452431 73306 net.cpp:226] Scale29 needs backward computation.
I0905 01:04:50.452438 73306 net.cpp:226] BatchNorm29 needs backward computation.
I0905 01:04:50.452446 73306 net.cpp:226] Concat26_Concat26_0_split needs backward computation.
I0905 01:04:50.452452 73306 net.cpp:226] Concat26 needs backward computation.
I0905 01:04:50.452461 73306 net.cpp:226] Dropout29 needs backward computation.
I0905 01:04:50.452476 73306 net.cpp:226] Convolution29 needs backward computation.
I0905 01:04:50.452484 73306 net.cpp:226] ReLU28 needs backward computation.
I0905 01:04:50.452492 73306 net.cpp:226] Scale28 needs backward computation.
I0905 01:04:50.452500 73306 net.cpp:226] BatchNorm28 needs backward computation.
I0905 01:04:50.452507 73306 net.cpp:226] Concat25_Concat25_0_split needs backward computation.
I0905 01:04:50.452517 73306 net.cpp:226] Concat25 needs backward computation.
I0905 01:04:50.452533 73306 net.cpp:226] Dropout28 needs backward computation.
I0905 01:04:50.452543 73306 net.cpp:226] Convolution28 needs backward computation.
I0905 01:04:50.452550 73306 net.cpp:226] ReLU27 needs backward computation.
I0905 01:04:50.452558 73306 net.cpp:226] Scale27 needs backward computation.
I0905 01:04:50.452565 73306 net.cpp:226] BatchNorm27 needs backward computation.
I0905 01:04:50.452572 73306 net.cpp:226] Pooling2_Pooling2_0_split needs backward computation.
I0905 01:04:50.452579 73306 net.cpp:226] Pooling2 needs backward computation.
I0905 01:04:50.452586 73306 net.cpp:226] Dropout27 needs backward computation.
I0905 01:04:50.452594 73306 net.cpp:226] Convolution27 needs backward computation.
I0905 01:04:50.452601 73306 net.cpp:226] ReLU26 needs backward computation.
I0905 01:04:50.452608 73306 net.cpp:226] Scale26 needs backward computation.
I0905 01:04:50.452616 73306 net.cpp:226] BatchNorm26 needs backward computation.
I0905 01:04:50.452623 73306 net.cpp:226] Concat24 needs backward computation.
I0905 01:04:50.452630 73306 net.cpp:226] Dropout26 needs backward computation.
I0905 01:04:50.452638 73306 net.cpp:226] Convolution26 needs backward computation.
I0905 01:04:50.452646 73306 net.cpp:226] ReLU25 needs backward computation.
I0905 01:04:50.452652 73306 net.cpp:226] Scale25 needs backward computation.
I0905 01:04:50.452661 73306 net.cpp:226] BatchNorm25 needs backward computation.
I0905 01:04:50.452667 73306 net.cpp:226] Concat23_Concat23_0_split needs backward computation.
I0905 01:04:50.452675 73306 net.cpp:226] Concat23 needs backward computation.
I0905 01:04:50.452683 73306 net.cpp:226] Dropout25 needs backward computation.
I0905 01:04:50.452692 73306 net.cpp:226] Convolution25 needs backward computation.
I0905 01:04:50.452699 73306 net.cpp:226] ReLU24 needs backward computation.
I0905 01:04:50.452707 73306 net.cpp:226] Scale24 needs backward computation.
I0905 01:04:50.452713 73306 net.cpp:226] BatchNorm24 needs backward computation.
I0905 01:04:50.452721 73306 net.cpp:226] Concat22_Concat22_0_split needs backward computation.
I0905 01:04:50.452728 73306 net.cpp:226] Concat22 needs backward computation.
I0905 01:04:50.452736 73306 net.cpp:226] Dropout24 needs backward computation.
I0905 01:04:50.452744 73306 net.cpp:226] Convolution24 needs backward computation.
I0905 01:04:50.452751 73306 net.cpp:226] ReLU23 needs backward computation.
I0905 01:04:50.452759 73306 net.cpp:226] Scale23 needs backward computation.
I0905 01:04:50.452766 73306 net.cpp:226] BatchNorm23 needs backward computation.
I0905 01:04:50.452775 73306 net.cpp:226] Concat21_Concat21_0_split needs backward computation.
I0905 01:04:50.452782 73306 net.cpp:226] Concat21 needs backward computation.
I0905 01:04:50.452790 73306 net.cpp:226] Dropout23 needs backward computation.
I0905 01:04:50.452798 73306 net.cpp:226] Convolution23 needs backward computation.
I0905 01:04:50.452805 73306 net.cpp:226] ReLU22 needs backward computation.
I0905 01:04:50.452812 73306 net.cpp:226] Scale22 needs backward computation.
I0905 01:04:50.452818 73306 net.cpp:226] BatchNorm22 needs backward computation.
I0905 01:04:50.452826 73306 net.cpp:226] Concat20_Concat20_0_split needs backward computation.
I0905 01:04:50.452832 73306 net.cpp:226] Concat20 needs backward computation.
I0905 01:04:50.452841 73306 net.cpp:226] Dropout22 needs backward computation.
I0905 01:04:50.452849 73306 net.cpp:226] Convolution22 needs backward computation.
I0905 01:04:50.452858 73306 net.cpp:226] ReLU21 needs backward computation.
I0905 01:04:50.452864 73306 net.cpp:226] Scale21 needs backward computation.
I0905 01:04:50.452873 73306 net.cpp:226] BatchNorm21 needs backward computation.
I0905 01:04:50.452882 73306 net.cpp:226] Concat19_Concat19_0_split needs backward computation.
I0905 01:04:50.452888 73306 net.cpp:226] Concat19 needs backward computation.
I0905 01:04:50.452896 73306 net.cpp:226] Dropout21 needs backward computation.
I0905 01:04:50.452905 73306 net.cpp:226] Convolution21 needs backward computation.
I0905 01:04:50.452914 73306 net.cpp:226] ReLU20 needs backward computation.
I0905 01:04:50.452927 73306 net.cpp:226] Scale20 needs backward computation.
I0905 01:04:50.452936 73306 net.cpp:226] BatchNorm20 needs backward computation.
I0905 01:04:50.452944 73306 net.cpp:226] Concat18_Concat18_0_split needs backward computation.
I0905 01:04:50.452952 73306 net.cpp:226] Concat18 needs backward computation.
I0905 01:04:50.452960 73306 net.cpp:226] Dropout20 needs backward computation.
I0905 01:04:50.452968 73306 net.cpp:226] Convolution20 needs backward computation.
I0905 01:04:50.452976 73306 net.cpp:226] ReLU19 needs backward computation.
I0905 01:04:50.452986 73306 net.cpp:226] Scale19 needs backward computation.
I0905 01:04:50.452991 73306 net.cpp:226] BatchNorm19 needs backward computation.
I0905 01:04:50.453001 73306 net.cpp:226] Concat17_Concat17_0_split needs backward computation.
I0905 01:04:50.453009 73306 net.cpp:226] Concat17 needs backward computation.
I0905 01:04:50.453018 73306 net.cpp:226] Dropout19 needs backward computation.
I0905 01:04:50.453027 73306 net.cpp:226] Convolution19 needs backward computation.
I0905 01:04:50.453033 73306 net.cpp:226] ReLU18 needs backward computation.
I0905 01:04:50.453042 73306 net.cpp:226] Scale18 needs backward computation.
I0905 01:04:50.453050 73306 net.cpp:226] BatchNorm18 needs backward computation.
I0905 01:04:50.453058 73306 net.cpp:226] Concat16_Concat16_0_split needs backward computation.
I0905 01:04:50.453066 73306 net.cpp:226] Concat16 needs backward computation.
I0905 01:04:50.453074 73306 net.cpp:226] Dropout18 needs backward computation.
I0905 01:04:50.453083 73306 net.cpp:226] Convolution18 needs backward computation.
I0905 01:04:50.453090 73306 net.cpp:226] ReLU17 needs backward computation.
I0905 01:04:50.453097 73306 net.cpp:226] Scale17 needs backward computation.
I0905 01:04:50.453106 73306 net.cpp:226] BatchNorm17 needs backward computation.
I0905 01:04:50.453114 73306 net.cpp:226] Concat15_Concat15_0_split needs backward computation.
I0905 01:04:50.453122 73306 net.cpp:226] Concat15 needs backward computation.
I0905 01:04:50.453140 73306 net.cpp:226] Dropout17 needs backward computation.
I0905 01:04:50.453146 73306 net.cpp:226] Convolution17 needs backward computation.
I0905 01:04:50.453155 73306 net.cpp:226] ReLU16 needs backward computation.
I0905 01:04:50.453162 73306 net.cpp:226] Scale16 needs backward computation.
I0905 01:04:50.453171 73306 net.cpp:226] BatchNorm16 needs backward computation.
I0905 01:04:50.453178 73306 net.cpp:226] Concat14_Concat14_0_split needs backward computation.
I0905 01:04:50.453187 73306 net.cpp:226] Concat14 needs backward computation.
I0905 01:04:50.453196 73306 net.cpp:226] Dropout16 needs backward computation.
I0905 01:04:50.453205 73306 net.cpp:226] Convolution16 needs backward computation.
I0905 01:04:50.453212 73306 net.cpp:226] ReLU15 needs backward computation.
I0905 01:04:50.453222 73306 net.cpp:226] Scale15 needs backward computation.
I0905 01:04:50.453228 73306 net.cpp:226] BatchNorm15 needs backward computation.
I0905 01:04:50.453236 73306 net.cpp:226] Concat13_Concat13_0_split needs backward computation.
I0905 01:04:50.453243 73306 net.cpp:226] Concat13 needs backward computation.
I0905 01:04:50.453250 73306 net.cpp:226] Dropout15 needs backward computation.
I0905 01:04:50.453256 73306 net.cpp:226] Convolution15 needs backward computation.
I0905 01:04:50.453261 73306 net.cpp:226] ReLU14 needs backward computation.
I0905 01:04:50.453266 73306 net.cpp:226] Scale14 needs backward computation.
I0905 01:04:50.453271 73306 net.cpp:226] BatchNorm14 needs backward computation.
I0905 01:04:50.453284 73306 net.cpp:226] Pooling1_Pooling1_0_split needs backward computation.
I0905 01:04:50.453291 73306 net.cpp:226] Pooling1 needs backward computation.
I0905 01:04:50.453299 73306 net.cpp:226] Dropout14 needs backward computation.
I0905 01:04:50.453307 73306 net.cpp:226] Convolution14 needs backward computation.
I0905 01:04:50.453321 73306 net.cpp:226] ReLU13 needs backward computation.
I0905 01:04:50.453330 73306 net.cpp:226] Scale13 needs backward computation.
I0905 01:04:50.453343 73306 net.cpp:226] BatchNorm13 needs backward computation.
I0905 01:04:50.453351 73306 net.cpp:226] Concat12 needs backward computation.
I0905 01:04:50.453359 73306 net.cpp:226] Dropout13 needs backward computation.
I0905 01:04:50.453368 73306 net.cpp:226] Convolution13 needs backward computation.
I0905 01:04:50.453375 73306 net.cpp:226] ReLU12 needs backward computation.
I0905 01:04:50.453383 73306 net.cpp:226] Scale12 needs backward computation.
I0905 01:04:50.453392 73306 net.cpp:226] BatchNorm12 needs backward computation.
I0905 01:04:50.453399 73306 net.cpp:226] Concat11_Concat11_0_split needs backward computation.
I0905 01:04:50.453408 73306 net.cpp:226] Concat11 needs backward computation.
I0905 01:04:50.453415 73306 net.cpp:226] Dropout12 needs backward computation.
I0905 01:04:50.453424 73306 net.cpp:226] Convolution12 needs backward computation.
I0905 01:04:50.453433 73306 net.cpp:226] ReLU11 needs backward computation.
I0905 01:04:50.453438 73306 net.cpp:226] Scale11 needs backward computation.
I0905 01:04:50.453449 73306 net.cpp:226] BatchNorm11 needs backward computation.
I0905 01:04:50.453455 73306 net.cpp:226] Concat10_Concat10_0_split needs backward computation.
I0905 01:04:50.453464 73306 net.cpp:226] Concat10 needs backward computation.
I0905 01:04:50.453471 73306 net.cpp:226] Dropout11 needs backward computation.
I0905 01:04:50.453480 73306 net.cpp:226] Convolution11 needs backward computation.
I0905 01:04:50.453488 73306 net.cpp:226] ReLU10 needs backward computation.
I0905 01:04:50.453495 73306 net.cpp:226] Scale10 needs backward computation.
I0905 01:04:50.453502 73306 net.cpp:226] BatchNorm10 needs backward computation.
I0905 01:04:50.453510 73306 net.cpp:226] Concat9_Concat9_0_split needs backward computation.
I0905 01:04:50.453519 73306 net.cpp:226] Concat9 needs backward computation.
I0905 01:04:50.453527 73306 net.cpp:226] Dropout10 needs backward computation.
I0905 01:04:50.453536 73306 net.cpp:226] Convolution10 needs backward computation.
I0905 01:04:50.453544 73306 net.cpp:226] ReLU9 needs backward computation.
I0905 01:04:50.453552 73306 net.cpp:226] Scale9 needs backward computation.
I0905 01:04:50.453562 73306 net.cpp:226] BatchNorm9 needs backward computation.
I0905 01:04:50.453569 73306 net.cpp:226] Concat8_Concat8_0_split needs backward computation.
I0905 01:04:50.453577 73306 net.cpp:226] Concat8 needs backward computation.
I0905 01:04:50.453584 73306 net.cpp:226] Dropout9 needs backward computation.
I0905 01:04:50.453593 73306 net.cpp:226] Convolution9 needs backward computation.
I0905 01:04:50.453600 73306 net.cpp:226] ReLU8 needs backward computation.
I0905 01:04:50.453608 73306 net.cpp:226] Scale8 needs backward computation.
I0905 01:04:50.453616 73306 net.cpp:226] BatchNorm8 needs backward computation.
I0905 01:04:50.453624 73306 net.cpp:226] Concat7_Concat7_0_split needs backward computation.
I0905 01:04:50.453632 73306 net.cpp:226] Concat7 needs backward computation.
I0905 01:04:50.453640 73306 net.cpp:226] Dropout8 needs backward computation.
I0905 01:04:50.453649 73306 net.cpp:226] Convolution8 needs backward computation.
I0905 01:04:50.453656 73306 net.cpp:226] ReLU7 needs backward computation.
I0905 01:04:50.453665 73306 net.cpp:226] Scale7 needs backward computation.
I0905 01:04:50.453672 73306 net.cpp:226] BatchNorm7 needs backward computation.
I0905 01:04:50.453680 73306 net.cpp:226] Concat6_Concat6_0_split needs backward computation.
I0905 01:04:50.453688 73306 net.cpp:226] Concat6 needs backward computation.
I0905 01:04:50.453696 73306 net.cpp:226] Dropout7 needs backward computation.
I0905 01:04:50.453706 73306 net.cpp:226] Convolution7 needs backward computation.
I0905 01:04:50.453713 73306 net.cpp:226] ReLU6 needs backward computation.
I0905 01:04:50.453722 73306 net.cpp:226] Scale6 needs backward computation.
I0905 01:04:50.453728 73306 net.cpp:226] BatchNorm6 needs backward computation.
I0905 01:04:50.453735 73306 net.cpp:226] Concat5_Concat5_0_split needs backward computation.
I0905 01:04:50.453743 73306 net.cpp:226] Concat5 needs backward computation.
I0905 01:04:50.453758 73306 net.cpp:226] Dropout6 needs backward computation.
I0905 01:04:50.453766 73306 net.cpp:226] Convolution6 needs backward computation.
I0905 01:04:50.453774 73306 net.cpp:226] ReLU5 needs backward computation.
I0905 01:04:50.453783 73306 net.cpp:226] Scale5 needs backward computation.
I0905 01:04:50.453789 73306 net.cpp:226] BatchNorm5 needs backward computation.
I0905 01:04:50.453796 73306 net.cpp:226] Concat4_Concat4_0_split needs backward computation.
I0905 01:04:50.453804 73306 net.cpp:226] Concat4 needs backward computation.
I0905 01:04:50.453812 73306 net.cpp:226] Dropout5 needs backward computation.
I0905 01:04:50.453819 73306 net.cpp:226] Convolution5 needs backward computation.
I0905 01:04:50.453827 73306 net.cpp:226] ReLU4 needs backward computation.
I0905 01:04:50.453835 73306 net.cpp:226] Scale4 needs backward computation.
I0905 01:04:50.453841 73306 net.cpp:226] BatchNorm4 needs backward computation.
I0905 01:04:50.453850 73306 net.cpp:226] Concat3_Concat3_0_split needs backward computation.
I0905 01:04:50.453856 73306 net.cpp:226] Concat3 needs backward computation.
I0905 01:04:50.453863 73306 net.cpp:226] Dropout4 needs backward computation.
I0905 01:04:50.453871 73306 net.cpp:226] Convolution4 needs backward computation.
I0905 01:04:50.453877 73306 net.cpp:226] ReLU3 needs backward computation.
I0905 01:04:50.453886 73306 net.cpp:226] Scale3 needs backward computation.
I0905 01:04:50.453893 73306 net.cpp:226] BatchNorm3 needs backward computation.
I0905 01:04:50.453902 73306 net.cpp:226] Concat2_Concat2_0_split needs backward computation.
I0905 01:04:50.453908 73306 net.cpp:226] Concat2 needs backward computation.
I0905 01:04:50.453917 73306 net.cpp:226] Dropout3 needs backward computation.
I0905 01:04:50.453924 73306 net.cpp:226] Convolution3 needs backward computation.
I0905 01:04:50.453933 73306 net.cpp:226] ReLU2 needs backward computation.
I0905 01:04:50.453939 73306 net.cpp:226] Scale2 needs backward computation.
I0905 01:04:50.453948 73306 net.cpp:226] BatchNorm2 needs backward computation.
I0905 01:04:50.453955 73306 net.cpp:226] Concat1_Concat1_0_split needs backward computation.
I0905 01:04:50.453963 73306 net.cpp:226] Concat1 needs backward computation.
I0905 01:04:50.453970 73306 net.cpp:226] Dropout2 needs backward computation.
I0905 01:04:50.453979 73306 net.cpp:226] Convolution2 needs backward computation.
I0905 01:04:50.453987 73306 net.cpp:226] ReLU1 needs backward computation.
I0905 01:04:50.453994 73306 net.cpp:226] Scale1 needs backward computation.
I0905 01:04:50.454001 73306 net.cpp:226] BatchNorm1 needs backward computation.
I0905 01:04:50.454010 73306 net.cpp:226] Dropout1_Dropout1_0_split needs backward computation.
I0905 01:04:50.454017 73306 net.cpp:226] Dropout1 needs backward computation.
I0905 01:04:50.454025 73306 net.cpp:226] Convolution1 needs backward computation.
I0905 01:04:50.454030 73306 net.cpp:228] Data1 does not need backward computation.
I0905 01:04:50.454036 73306 net.cpp:270] This network produces output SoftmaxWithLoss1
I0905 01:04:50.454166 73306 net.cpp:283] Network initialization done.
I0905 01:04:50.457954 73306 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I0905 01:04:50.458180 73306 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer Data1
I0905 01:04:50.459053 73306 net.cpp:58] Initializing net from parameters: 
name: "DenseNN"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "ImageData"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 150
    mean_value: 150
    mean_value: 150
  }
  image_data_param {
    source: "../list_bias-1_te.lst"
    batch_size: 16
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout1"
  type: "Dropout"
  bottom: "Convolution1"
  top: "Dropout1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Dropout1"
  top: "BatchNorm1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "BatchNorm1"
  top: "Convolution2"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout2"
  type: "Dropout"
  bottom: "Convolution2"
  top: "Dropout2"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "Dropout1"
  bottom: "Dropout2"
  top: "Concat1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Concat1"
  top: "BatchNorm2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "BatchNorm2"
  top: "Convolution3"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout3"
  type: "Dropout"
  bottom: "Convolution3"
  top: "Dropout3"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat2"
  type: "Concat"
  bottom: "Concat1"
  bottom: "Dropout3"
  top: "Concat2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Concat2"
  top: "BatchNorm3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "BatchNorm3"
  top: "Convolution4"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout4"
  type: "Dropout"
  bottom: "Convolution4"
  top: "Dropout4"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat3"
  type: "Concat"
  bottom: "Concat2"
  bottom: "Dropout4"
  top: "Concat3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Concat3"
  top: "BatchNorm4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "BatchNorm4"
  top: "BatchNorm4"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "BatchNorm4"
  top: "BatchNorm4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "BatchNorm4"
  top: "Convolution5"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout5"
  type: "Dropout"
  bottom: "Convolution5"
  top: "Dropout5"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat4"
  type: "Concat"
  bottom: "Concat3"
  bottom: "Dropout5"
  top: "Concat4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Concat4"
  top: "BatchNorm5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "BatchNorm5"
  top: "BatchNorm5"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "BatchNorm5"
  top: "BatchNorm5"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "BatchNorm5"
  top: "Convolution6"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout6"
  type: "Dropout"
  bottom: "Convolution6"
  top: "Dropout6"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat5"
  type: "Concat"
  bottom: "Concat4"
  bottom: "Dropout6"
  top: "Concat5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Concat5"
  top: "BatchNorm6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "BatchNorm6"
  top: "BatchNorm6"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "BatchNorm6"
  top: "BatchNorm6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "BatchNorm6"
  top: "Convolution7"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout7"
  type: "Dropout"
  bottom: "Convolution7"
  top: "Dropout7"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat6"
  type: "Concat"
  bottom: "Concat5"
  bottom: "Dropout7"
  top: "Concat6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Concat6"
  top: "BatchNorm7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "BatchNorm7"
  top: "BatchNorm7"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "BatchNorm7"
  top: "BatchNorm7"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "BatchNorm7"
  top: "Convolution8"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout8"
  type: "Dropout"
  bottom: "Convolution8"
  top: "Dropout8"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat7"
  type: "Concat"
  bottom: "Concat6"
  bottom: "Dropout8"
  top: "Concat7"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Concat7"
  top: "BatchNorm8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "BatchNorm8"
  top: "BatchNorm8"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "BatchNorm8"
  top: "BatchNorm8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "BatchNorm8"
  top: "Convolution9"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout9"
  type: "Dropout"
  bottom: "Convolution9"
  top: "Dropout9"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat8"
  type: "Concat"
  bottom: "Concat7"
  bottom: "Dropout9"
  top: "Concat8"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Concat8"
  top: "BatchNorm9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "BatchNorm9"
  top: "BatchNorm9"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "BatchNorm9"
  top: "BatchNorm9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "BatchNorm9"
  top: "Convolution10"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout10"
  type: "Dropout"
  bottom: "Convolution10"
  top: "Dropout10"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat9"
  type: "Concat"
  bottom: "Concat8"
  bottom: "Dropout10"
  top: "Concat9"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Concat9"
  top: "BatchNorm10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "BatchNorm10"
  top: "BatchNorm10"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "BatchNorm10"
  top: "BatchNorm10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "BatchNorm10"
  top: "Convolution11"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout11"
  type: "Dropout"
  bottom: "Convolution11"
  top: "Dropout11"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat10"
  type: "Concat"
  bottom: "Concat9"
  bottom: "Dropout11"
  top: "Concat10"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Concat10"
  top: "BatchNorm11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "BatchNorm11"
  top: "BatchNorm11"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "BatchNorm11"
  top: "BatchNorm11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "BatchNorm11"
  top: "Convolution12"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout12"
  type: "Dropout"
  bottom: "Convolution12"
  top: "Dropout12"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat11"
  type: "Concat"
  bottom: "Concat10"
  bottom: "Dropout12"
  top: "Concat11"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Concat11"
  top: "BatchNorm12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "BatchNorm12"
  top: "BatchNorm12"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "BatchNorm12"
  top: "BatchNorm12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "BatchNorm12"
  top: "Convolution13"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout13"
  type: "Dropout"
  bottom: "Convolution13"
  top: "Dropout13"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat12"
  type: "Concat"
  bottom: "Concat11"
  bottom: "Dropout13"
  top: "Concat12"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Concat12"
  top: "BatchNorm13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "BatchNorm13"
  top: "BatchNorm13"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "BatchNorm13"
  top: "BatchNorm13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "BatchNorm13"
  top: "Convolution14"
  convolution_param {
    num_output: 160
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout14"
  type: "Dropout"
  bottom: "Convolution14"
  top: "Dropout14"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Dropout14"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Pooling1"
  top: "BatchNorm14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "BatchNorm14"
  top: "BatchNorm14"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "BatchNorm14"
  top: "BatchNorm14"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "BatchNorm14"
  top: "Convolution15"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout15"
  type: "Dropout"
  bottom: "Convolution15"
  top: "Dropout15"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat13"
  type: "Concat"
  bottom: "Pooling1"
  bottom: "Dropout15"
  top: "Concat13"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Concat13"
  top: "BatchNorm15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "BatchNorm15"
  top: "BatchNorm15"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "BatchNorm15"
  top: "BatchNorm15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "BatchNorm15"
  top: "Convolution16"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout16"
  type: "Dropout"
  bottom: "Convolution16"
  top: "Dropout16"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat14"
  type: "Concat"
  bottom: "Concat13"
  bottom: "Dropout16"
  top: "Concat14"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Concat14"
  top: "BatchNorm16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "BatchNorm16"
  top: "BatchNorm16"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "BatchNorm16"
  top: "BatchNorm16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "BatchNorm16"
  top: "Convolution17"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout17"
  type: "Dropout"
  bottom: "Convolution17"
  top: "Dropout17"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat15"
  type: "Concat"
  bottom: "Concat14"
  bottom: "Dropout17"
  top: "Concat15"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Concat15"
  top: "BatchNorm17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "BatchNorm17"
  top: "BatchNorm17"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "BatchNorm17"
  top: "BatchNorm17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "BatchNorm17"
  top: "Convolution18"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout18"
  type: "Dropout"
  bottom: "Convolution18"
  top: "Dropout18"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat16"
  type: "Concat"
  bottom: "Concat15"
  bottom: "Dropout18"
  top: "Concat16"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Concat16"
  top: "BatchNorm18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "BatchNorm18"
  top: "BatchNorm18"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "BatchNorm18"
  top: "BatchNorm18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "BatchNorm18"
  top: "Convolution19"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout19"
  type: "Dropout"
  bottom: "Convolution19"
  top: "Dropout19"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat17"
  type: "Concat"
  bottom: "Concat16"
  bottom: "Dropout19"
  top: "Concat17"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Concat17"
  top: "BatchNorm19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "BatchNorm19"
  top: "BatchNorm19"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "BatchNorm19"
  top: "BatchNorm19"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "BatchNorm19"
  top: "Convolution20"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout20"
  type: "Dropout"
  bottom: "Convolution20"
  top: "Dropout20"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat18"
  type: "Concat"
  bottom: "Concat17"
  bottom: "Dropout20"
  top: "Concat18"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Concat18"
  top: "BatchNorm20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "BatchNorm20"
  top: "BatchNorm20"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "BatchNorm20"
  top: "BatchNorm20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "BatchNorm20"
  top: "Convolution21"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout21"
  type: "Dropout"
  bottom: "Convolution21"
  top: "Dropout21"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat19"
  type: "Concat"
  bottom: "Concat18"
  bottom: "Dropout21"
  top: "Concat19"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Concat19"
  top: "BatchNorm21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "BatchNorm21"
  top: "BatchNorm21"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "BatchNorm21"
  top: "BatchNorm21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "BatchNorm21"
  top: "Convolution22"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout22"
  type: "Dropout"
  bottom: "Convolution22"
  top: "Dropout22"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat20"
  type: "Concat"
  bottom: "Concat19"
  bottom: "Dropout22"
  top: "Concat20"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Concat20"
  top: "BatchNorm22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "BatchNorm22"
  top: "BatchNorm22"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "BatchNorm22"
  top: "BatchNorm22"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "BatchNorm22"
  top: "Convolution23"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout23"
  type: "Dropout"
  bottom: "Convolution23"
  top: "Dropout23"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat21"
  type: "Concat"
  bottom: "Concat20"
  bottom: "Dropout23"
  top: "Concat21"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Concat21"
  top: "BatchNorm23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "BatchNorm23"
  top: "BatchNorm23"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "BatchNorm23"
  top: "BatchNorm23"
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "BatchNorm23"
  top: "Convolution24"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout24"
  type: "Dropout"
  bottom: "Convolution24"
  top: "Dropout24"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat22"
  type: "Concat"
  bottom: "Concat21"
  bottom: "Dropout24"
  top: "Concat22"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Concat22"
  top: "BatchNorm24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "BatchNorm24"
  top: "BatchNorm24"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "BatchNorm24"
  top: "BatchNorm24"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "BatchNorm24"
  top: "Convolution25"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout25"
  type: "Dropout"
  bottom: "Convolution25"
  top: "Dropout25"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat23"
  type: "Concat"
  bottom: "Concat22"
  bottom: "Dropout25"
  top: "Concat23"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Concat23"
  top: "BatchNorm25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "BatchNorm25"
  top: "BatchNorm25"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "BatchNorm25"
  top: "BatchNorm25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "BatchNorm25"
  top: "Convolution26"
  convolution_param {
    num_output: 12
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout26"
  type: "Dropout"
  bottom: "Convolution26"
  top: "Dropout26"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat24"
  type: "Concat"
  bottom: "Concat23"
  bottom: "Dropout26"
  top: "Concat24"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Concat24"
  top: "BatchNorm26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "BatchNorm26"
  top: "BatchNorm26"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "BatchNorm26"
  top: "BatchNorm26"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "BatchNorm26"
  top: "Convolution27"
  convolution_param {
    num_output: 304
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout27"
  type: "Dropout"
  bottom: "Convolution27"
  top: "Dropout27"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Dropout27"
  top: "Pooling2"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Pooling2"
  top: "BatchNorm27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "BatchNorm27"
  top: "BatchNorm27"
  scale_param {
    filler {
      value: 1
    }
    bias_term: false
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Batc
I0905 01:04:50.464725 73306 layer_factory.hpp:77] Creating layer Data1
I0905 01:04:50.464746 73306 net.cpp:100] Creating Layer Data1
I0905 01:04:50.464756 73306 net.cpp:408] Data1 -> Data1
I0905 01:04:50.464766 73306 net.cpp:408] Data1 -> Data2
I0905 01:04:50.464779 73306 image_data_layer.cpp:38] Opening file ../list_bias-1_te.lst
I0905 01:04:50.506569 73306 image_data_layer.cpp:53] Shuffling data
I0905 01:04:50.520064 73306 image_data_layer.cpp:58] A total of 117400 images.
I0905 01:04:50.520548 73306 image_data_layer.cpp:85] output data size: 16,3,64,64
I0905 01:04:50.523679 73306 net.cpp:150] Setting up Data1
I0905 01:04:50.523704 73306 net.cpp:157] Top shape: 16 3 64 64 (196608)
I0905 01:04:50.523716 73306 net.cpp:157] Top shape: 16 (16)
I0905 01:04:50.523725 73306 net.cpp:165] Memory required for data: 786496
I0905 01:04:50.523736 73306 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I0905 01:04:50.523756 73306 net.cpp:100] Creating Layer Data2_Data1_1_split
I0905 01:04:50.523766 73306 net.cpp:434] Data2_Data1_1_split <- Data2
I0905 01:04:50.523775 73306 net.cpp:408] Data2_Data1_1_split -> Data2_Data1_1_split_0
I0905 01:04:50.523797 73306 net.cpp:408] Data2_Data1_1_split -> Data2_Data1_1_split_1
I0905 01:04:50.524027 73306 net.cpp:150] Setting up Data2_Data1_1_split
I0905 01:04:50.524054 73306 net.cpp:157] Top shape: 16 (16)
I0905 01:04:50.524060 73306 net.cpp:157] Top shape: 16 (16)
I0905 01:04:50.524063 73306 net.cpp:165] Memory required for data: 786624
I0905 01:04:50.524070 73306 layer_factory.hpp:77] Creating layer Convolution1
I0905 01:04:50.524091 73306 net.cpp:100] Creating Layer Convolution1
I0905 01:04:50.524096 73306 net.cpp:434] Convolution1 <- Data1
I0905 01:04:50.524109 73306 net.cpp:408] Convolution1 -> Convolution1
I0905 01:04:50.525902 73306 net.cpp:150] Setting up Convolution1
I0905 01:04:50.525936 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.525946 73306 net.cpp:165] Memory required for data: 4980928
I0905 01:04:50.525964 73306 layer_factory.hpp:77] Creating layer Dropout1
I0905 01:04:50.525977 73306 net.cpp:100] Creating Layer Dropout1
I0905 01:04:50.525986 73306 net.cpp:434] Dropout1 <- Convolution1
I0905 01:04:50.526008 73306 net.cpp:408] Dropout1 -> Dropout1
I0905 01:04:50.526075 73306 net.cpp:150] Setting up Dropout1
I0905 01:04:50.526100 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.526120 73306 net.cpp:165] Memory required for data: 9175232
I0905 01:04:50.526139 73306 layer_factory.hpp:77] Creating layer Dropout1_Dropout1_0_split
I0905 01:04:50.526151 73306 net.cpp:100] Creating Layer Dropout1_Dropout1_0_split
I0905 01:04:50.526160 73306 net.cpp:434] Dropout1_Dropout1_0_split <- Dropout1
I0905 01:04:50.526171 73306 net.cpp:408] Dropout1_Dropout1_0_split -> Dropout1_Dropout1_0_split_0
I0905 01:04:50.526182 73306 net.cpp:408] Dropout1_Dropout1_0_split -> Dropout1_Dropout1_0_split_1
I0905 01:04:50.526247 73306 net.cpp:150] Setting up Dropout1_Dropout1_0_split
I0905 01:04:50.526259 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.526268 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.526276 73306 net.cpp:165] Memory required for data: 17563840
I0905 01:04:50.526285 73306 layer_factory.hpp:77] Creating layer BatchNorm1
I0905 01:04:50.526299 73306 net.cpp:100] Creating Layer BatchNorm1
I0905 01:04:50.526310 73306 net.cpp:434] BatchNorm1 <- Dropout1_Dropout1_0_split_0
I0905 01:04:50.526320 73306 net.cpp:408] BatchNorm1 -> BatchNorm1
I0905 01:04:50.526638 73306 net.cpp:150] Setting up BatchNorm1
I0905 01:04:50.526665 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.526674 73306 net.cpp:165] Memory required for data: 21758144
I0905 01:04:50.526711 73306 layer_factory.hpp:77] Creating layer Scale1
I0905 01:04:50.526770 73306 net.cpp:100] Creating Layer Scale1
I0905 01:04:50.526777 73306 net.cpp:434] Scale1 <- BatchNorm1
I0905 01:04:50.526793 73306 net.cpp:395] Scale1 -> BatchNorm1 (in-place)
I0905 01:04:50.526995 73306 net.cpp:150] Setting up Scale1
I0905 01:04:50.527004 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.527009 73306 net.cpp:165] Memory required for data: 25952448
I0905 01:04:50.527015 73306 layer_factory.hpp:77] Creating layer ReLU1
I0905 01:04:50.527024 73306 net.cpp:100] Creating Layer ReLU1
I0905 01:04:50.527027 73306 net.cpp:434] ReLU1 <- BatchNorm1
I0905 01:04:50.527032 73306 net.cpp:395] ReLU1 -> BatchNorm1 (in-place)
I0905 01:04:50.527425 73306 net.cpp:150] Setting up ReLU1
I0905 01:04:50.527436 73306 net.cpp:157] Top shape: 16 16 64 64 (1048576)
I0905 01:04:50.527441 73306 net.cpp:165] Memory required for data: 30146752
I0905 01:04:50.527446 73306 layer_factory.hpp:77] Creating layer Convolution2
I0905 01:04:50.527461 73306 net.cpp:100] Creating Layer Convolution2
I0905 01:04:50.527464 73306 net.cpp:434] Convolution2 <- BatchNorm1
I0905 01:04:50.527473 73306 net.cpp:408] Convolution2 -> Convolution2
I0905 01:04:50.528702 73306 net.cpp:150] Setting up Convolution2
I0905 01:04:50.528714 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.528718 73306 net.cpp:165] Memory required for data: 33292480
I0905 01:04:50.528726 73306 layer_factory.hpp:77] Creating layer Dropout2
I0905 01:04:50.528734 73306 net.cpp:100] Creating Layer Dropout2
I0905 01:04:50.528739 73306 net.cpp:434] Dropout2 <- Convolution2
I0905 01:04:50.528748 73306 net.cpp:408] Dropout2 -> Dropout2
I0905 01:04:50.528800 73306 net.cpp:150] Setting up Dropout2
I0905 01:04:50.528807 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.528811 73306 net.cpp:165] Memory required for data: 36438208
I0905 01:04:50.528815 73306 layer_factory.hpp:77] Creating layer Concat1
I0905 01:04:50.528825 73306 net.cpp:100] Creating Layer Concat1
I0905 01:04:50.528830 73306 net.cpp:434] Concat1 <- Dropout1_Dropout1_0_split_1
I0905 01:04:50.528836 73306 net.cpp:434] Concat1 <- Dropout2
I0905 01:04:50.528841 73306 net.cpp:408] Concat1 -> Concat1
I0905 01:04:50.528874 73306 net.cpp:150] Setting up Concat1
I0905 01:04:50.528880 73306 net.cpp:157] Top shape: 16 28 64 64 (1835008)
I0905 01:04:50.528884 73306 net.cpp:165] Memory required for data: 43778240
I0905 01:04:50.528888 73306 layer_factory.hpp:77] Creating layer Concat1_Concat1_0_split
I0905 01:04:50.528895 73306 net.cpp:100] Creating Layer Concat1_Concat1_0_split
I0905 01:04:50.528899 73306 net.cpp:434] Concat1_Concat1_0_split <- Concat1
I0905 01:04:50.528906 73306 net.cpp:408] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_0
I0905 01:04:50.528913 73306 net.cpp:408] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_1
I0905 01:04:50.528956 73306 net.cpp:150] Setting up Concat1_Concat1_0_split
I0905 01:04:50.528964 73306 net.cpp:157] Top shape: 16 28 64 64 (1835008)
I0905 01:04:50.528967 73306 net.cpp:157] Top shape: 16 28 64 64 (1835008)
I0905 01:04:50.528971 73306 net.cpp:165] Memory required for data: 58458304
I0905 01:04:50.528975 73306 layer_factory.hpp:77] Creating layer BatchNorm2
I0905 01:04:50.528985 73306 net.cpp:100] Creating Layer BatchNorm2
I0905 01:04:50.528990 73306 net.cpp:434] BatchNorm2 <- Concat1_Concat1_0_split_0
I0905 01:04:50.528995 73306 net.cpp:408] BatchNorm2 -> BatchNorm2
I0905 01:04:50.529399 73306 net.cpp:150] Setting up BatchNorm2
I0905 01:04:50.529420 73306 net.cpp:157] Top shape: 16 28 64 64 (1835008)
I0905 01:04:50.529429 73306 net.cpp:165] Memory required for data: 65798336
I0905 01:04:50.529445 73306 layer_factory.hpp:77] Creating layer Scale2
I0905 01:04:50.529459 73306 net.cpp:100] Creating Layer Scale2
I0905 01:04:50.529491 73306 net.cpp:434] Scale2 <- BatchNorm2
I0905 01:04:50.529501 73306 net.cpp:395] Scale2 -> BatchNorm2 (in-place)
I0905 01:04:50.529629 73306 net.cpp:150] Setting up Scale2
I0905 01:04:50.529644 73306 net.cpp:157] Top shape: 16 28 64 64 (1835008)
I0905 01:04:50.529651 73306 net.cpp:165] Memory required for data: 73138368
I0905 01:04:50.529660 73306 layer_factory.hpp:77] Creating layer ReLU2
I0905 01:04:50.529670 73306 net.cpp:100] Creating Layer ReLU2
I0905 01:04:50.529678 73306 net.cpp:434] ReLU2 <- BatchNorm2
I0905 01:04:50.529692 73306 net.cpp:395] ReLU2 -> BatchNorm2 (in-place)
I0905 01:04:50.530038 73306 net.cpp:150] Setting up ReLU2
I0905 01:04:50.530071 73306 net.cpp:157] Top shape: 16 28 64 64 (1835008)
I0905 01:04:50.530079 73306 net.cpp:165] Memory required for data: 80478400
I0905 01:04:50.530087 73306 layer_factory.hpp:77] Creating layer Convolution3
I0905 01:04:50.530103 73306 net.cpp:100] Creating Layer Convolution3
I0905 01:04:50.530112 73306 net.cpp:434] Convolution3 <- BatchNorm2
I0905 01:04:50.530124 73306 net.cpp:408] Convolution3 -> Convolution3
I0905 01:04:50.531429 73306 net.cpp:150] Setting up Convolution3
I0905 01:04:50.531462 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.531471 73306 net.cpp:165] Memory required for data: 83624128
I0905 01:04:50.531483 73306 layer_factory.hpp:77] Creating layer Dropout3
I0905 01:04:50.531499 73306 net.cpp:100] Creating Layer Dropout3
I0905 01:04:50.531507 73306 net.cpp:434] Dropout3 <- Convolution3
I0905 01:04:50.531518 73306 net.cpp:408] Dropout3 -> Dropout3
I0905 01:04:50.531577 73306 net.cpp:150] Setting up Dropout3
I0905 01:04:50.531604 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.531622 73306 net.cpp:165] Memory required for data: 86769856
I0905 01:04:50.531633 73306 layer_factory.hpp:77] Creating layer Concat2
I0905 01:04:50.531647 73306 net.cpp:100] Creating Layer Concat2
I0905 01:04:50.531656 73306 net.cpp:434] Concat2 <- Concat1_Concat1_0_split_1
I0905 01:04:50.531666 73306 net.cpp:434] Concat2 <- Dropout3
I0905 01:04:50.531674 73306 net.cpp:408] Concat2 -> Concat2
I0905 01:04:50.531710 73306 net.cpp:150] Setting up Concat2
I0905 01:04:50.531731 73306 net.cpp:157] Top shape: 16 40 64 64 (2621440)
I0905 01:04:50.531751 73306 net.cpp:165] Memory required for data: 97255616
I0905 01:04:50.531759 73306 layer_factory.hpp:77] Creating layer Concat2_Concat2_0_split
I0905 01:04:50.531780 73306 net.cpp:100] Creating Layer Concat2_Concat2_0_split
I0905 01:04:50.531788 73306 net.cpp:434] Concat2_Concat2_0_split <- Concat2
I0905 01:04:50.531803 73306 net.cpp:408] Concat2_Concat2_0_split -> Concat2_Concat2_0_split_0
I0905 01:04:50.531815 73306 net.cpp:408] Concat2_Concat2_0_split -> Concat2_Concat2_0_split_1
I0905 01:04:50.531862 73306 net.cpp:150] Setting up Concat2_Concat2_0_split
I0905 01:04:50.531884 73306 net.cpp:157] Top shape: 16 40 64 64 (2621440)
I0905 01:04:50.531893 73306 net.cpp:157] Top shape: 16 40 64 64 (2621440)
I0905 01:04:50.531901 73306 net.cpp:165] Memory required for data: 118227136
I0905 01:04:50.531908 73306 layer_factory.hpp:77] Creating layer BatchNorm3
I0905 01:04:50.531919 73306 net.cpp:100] Creating Layer BatchNorm3
I0905 01:04:50.531927 73306 net.cpp:434] BatchNorm3 <- Concat2_Concat2_0_split_0
I0905 01:04:50.531942 73306 net.cpp:408] BatchNorm3 -> BatchNorm3
I0905 01:04:50.532243 73306 net.cpp:150] Setting up BatchNorm3
I0905 01:04:50.532269 73306 net.cpp:157] Top shape: 16 40 64 64 (2621440)
I0905 01:04:50.532277 73306 net.cpp:165] Memory required for data: 128712896
I0905 01:04:50.532294 73306 layer_factory.hpp:77] Creating layer Scale3
I0905 01:04:50.532305 73306 net.cpp:100] Creating Layer Scale3
I0905 01:04:50.532315 73306 net.cpp:434] Scale3 <- BatchNorm3
I0905 01:04:50.532323 73306 net.cpp:395] Scale3 -> BatchNorm3 (in-place)
I0905 01:04:50.532452 73306 net.cpp:150] Setting up Scale3
I0905 01:04:50.532485 73306 net.cpp:157] Top shape: 16 40 64 64 (2621440)
I0905 01:04:50.532493 73306 net.cpp:165] Memory required for data: 139198656
I0905 01:04:50.532503 73306 layer_factory.hpp:77] Creating layer ReLU3
I0905 01:04:50.532527 73306 net.cpp:100] Creating Layer ReLU3
I0905 01:04:50.532536 73306 net.cpp:434] ReLU3 <- BatchNorm3
I0905 01:04:50.532547 73306 net.cpp:395] ReLU3 -> BatchNorm3 (in-place)
I0905 01:04:50.532788 73306 net.cpp:150] Setting up ReLU3
I0905 01:04:50.532816 73306 net.cpp:157] Top shape: 16 40 64 64 (2621440)
I0905 01:04:50.532824 73306 net.cpp:165] Memory required for data: 149684416
I0905 01:04:50.532833 73306 layer_factory.hpp:77] Creating layer Convolution4
I0905 01:04:50.532850 73306 net.cpp:100] Creating Layer Convolution4
I0905 01:04:50.532858 73306 net.cpp:434] Convolution4 <- BatchNorm3
I0905 01:04:50.532869 73306 net.cpp:408] Convolution4 -> Convolution4
I0905 01:04:50.534139 73306 net.cpp:150] Setting up Convolution4
I0905 01:04:50.534173 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.534181 73306 net.cpp:165] Memory required for data: 152830144
I0905 01:04:50.534193 73306 layer_factory.hpp:77] Creating layer Dropout4
I0905 01:04:50.534204 73306 net.cpp:100] Creating Layer Dropout4
I0905 01:04:50.534212 73306 net.cpp:434] Dropout4 <- Convolution4
I0905 01:04:50.534224 73306 net.cpp:408] Dropout4 -> Dropout4
I0905 01:04:50.534281 73306 net.cpp:150] Setting up Dropout4
I0905 01:04:50.534294 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.534301 73306 net.cpp:165] Memory required for data: 155975872
I0905 01:04:50.534309 73306 layer_factory.hpp:77] Creating layer Concat3
I0905 01:04:50.534323 73306 net.cpp:100] Creating Layer Concat3
I0905 01:04:50.534333 73306 net.cpp:434] Concat3 <- Concat2_Concat2_0_split_1
I0905 01:04:50.534343 73306 net.cpp:434] Concat3 <- Dropout4
I0905 01:04:50.534353 73306 net.cpp:408] Concat3 -> Concat3
I0905 01:04:50.534410 73306 net.cpp:150] Setting up Concat3
I0905 01:04:50.534422 73306 net.cpp:157] Top shape: 16 52 64 64 (3407872)
I0905 01:04:50.534441 73306 net.cpp:165] Memory required for data: 169607360
I0905 01:04:50.534449 73306 layer_factory.hpp:77] Creating layer Concat3_Concat3_0_split
I0905 01:04:50.534462 73306 net.cpp:100] Creating Layer Concat3_Concat3_0_split
I0905 01:04:50.534471 73306 net.cpp:434] Concat3_Concat3_0_split <- Concat3
I0905 01:04:50.534481 73306 net.cpp:408] Concat3_Concat3_0_split -> Concat3_Concat3_0_split_0
I0905 01:04:50.534492 73306 net.cpp:408] Concat3_Concat3_0_split -> Concat3_Concat3_0_split_1
I0905 01:04:50.534557 73306 net.cpp:150] Setting up Concat3_Concat3_0_split
I0905 01:04:50.534570 73306 net.cpp:157] Top shape: 16 52 64 64 (3407872)
I0905 01:04:50.534579 73306 net.cpp:157] Top shape: 16 52 64 64 (3407872)
I0905 01:04:50.534587 73306 net.cpp:165] Memory required for data: 196870336
I0905 01:04:50.534595 73306 layer_factory.hpp:77] Creating layer BatchNorm4
I0905 01:04:50.534607 73306 net.cpp:100] Creating Layer BatchNorm4
I0905 01:04:50.534615 73306 net.cpp:434] BatchNorm4 <- Concat3_Concat3_0_split_0
I0905 01:04:50.534644 73306 net.cpp:408] BatchNorm4 -> BatchNorm4
I0905 01:04:50.534941 73306 net.cpp:150] Setting up BatchNorm4
I0905 01:04:50.534956 73306 net.cpp:157] Top shape: 16 52 64 64 (3407872)
I0905 01:04:50.534976 73306 net.cpp:165] Memory required for data: 210501824
I0905 01:04:50.534988 73306 layer_factory.hpp:77] Creating layer Scale4
I0905 01:04:50.534998 73306 net.cpp:100] Creating Layer Scale4
I0905 01:04:50.535007 73306 net.cpp:434] Scale4 <- BatchNorm4
I0905 01:04:50.535018 73306 net.cpp:395] Scale4 -> BatchNorm4 (in-place)
I0905 01:04:50.535183 73306 net.cpp:150] Setting up Scale4
I0905 01:04:50.535197 73306 net.cpp:157] Top shape: 16 52 64 64 (3407872)
I0905 01:04:50.535217 73306 net.cpp:165] Memory required for data: 224133312
I0905 01:04:50.535226 73306 layer_factory.hpp:77] Creating layer ReLU4
I0905 01:04:50.535246 73306 net.cpp:100] Creating Layer ReLU4
I0905 01:04:50.535254 73306 net.cpp:434] ReLU4 <- BatchNorm4
I0905 01:04:50.535262 73306 net.cpp:395] ReLU4 -> BatchNorm4 (in-place)
I0905 01:04:50.535682 73306 net.cpp:150] Setting up ReLU4
I0905 01:04:50.535712 73306 net.cpp:157] Top shape: 16 52 64 64 (3407872)
I0905 01:04:50.535720 73306 net.cpp:165] Memory required for data: 237764800
I0905 01:04:50.535742 73306 layer_factory.hpp:77] Creating layer Convolution5
I0905 01:04:50.535758 73306 net.cpp:100] Creating Layer Convolution5
I0905 01:04:50.535768 73306 net.cpp:434] Convolution5 <- BatchNorm4
I0905 01:04:50.535779 73306 net.cpp:408] Convolution5 -> Convolution5
I0905 01:04:50.536980 73306 net.cpp:150] Setting up Convolution5
I0905 01:04:50.537015 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.537024 73306 net.cpp:165] Memory required for data: 240910528
I0905 01:04:50.537035 73306 layer_factory.hpp:77] Creating layer Dropout5
I0905 01:04:50.537046 73306 net.cpp:100] Creating Layer Dropout5
I0905 01:04:50.537055 73306 net.cpp:434] Dropout5 <- Convolution5
I0905 01:04:50.537065 73306 net.cpp:408] Dropout5 -> Dropout5
I0905 01:04:50.537133 73306 net.cpp:150] Setting up Dropout5
I0905 01:04:50.537147 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.537155 73306 net.cpp:165] Memory required for data: 244056256
I0905 01:04:50.537175 73306 layer_factory.hpp:77] Creating layer Concat4
I0905 01:04:50.537186 73306 net.cpp:100] Creating Layer Concat4
I0905 01:04:50.537195 73306 net.cpp:434] Concat4 <- Concat3_Concat3_0_split_1
I0905 01:04:50.537205 73306 net.cpp:434] Concat4 <- Dropout5
I0905 01:04:50.537228 73306 net.cpp:408] Concat4 -> Concat4
I0905 01:04:50.537276 73306 net.cpp:150] Setting up Concat4
I0905 01:04:50.537300 73306 net.cpp:157] Top shape: 16 64 64 64 (4194304)
I0905 01:04:50.537310 73306 net.cpp:165] Memory required for data: 260833472
I0905 01:04:50.537318 73306 layer_factory.hpp:77] Creating layer Concat4_Concat4_0_split
I0905 01:04:50.537343 73306 net.cpp:100] Creating Layer Concat4_Concat4_0_split
I0905 01:04:50.537351 73306 net.cpp:434] Concat4_Concat4_0_split <- Concat4
I0905 01:04:50.537364 73306 net.cpp:408] Concat4_Concat4_0_split -> Concat4_Concat4_0_split_0
I0905 01:04:50.537375 73306 net.cpp:408] Concat4_Concat4_0_split -> Concat4_Concat4_0_split_1
I0905 01:04:50.537457 73306 net.cpp:150] Setting up Concat4_Concat4_0_split
I0905 01:04:50.537480 73306 net.cpp:157] Top shape: 16 64 64 64 (4194304)
I0905 01:04:50.537500 73306 net.cpp:157] Top shape: 16 64 64 64 (4194304)
I0905 01:04:50.537521 73306 net.cpp:165] Memory required for data: 294387904
I0905 01:04:50.537529 73306 layer_factory.hpp:77] Creating layer BatchNorm5
I0905 01:04:50.537541 73306 net.cpp:100] Creating Layer BatchNorm5
I0905 01:04:50.537550 73306 net.cpp:434] BatchNorm5 <- Concat4_Concat4_0_split_0
I0905 01:04:50.537562 73306 net.cpp:408] BatchNorm5 -> BatchNorm5
I0905 01:04:50.537894 73306 net.cpp:150] Setting up BatchNorm5
I0905 01:04:50.537919 73306 net.cpp:157] Top shape: 16 64 64 64 (4194304)
I0905 01:04:50.537927 73306 net.cpp:165] Memory required for data: 311165120
I0905 01:04:50.537940 73306 layer_factory.hpp:77] Creating layer Scale5
I0905 01:04:50.537952 73306 net.cpp:100] Creating Layer Scale5
I0905 01:04:50.537961 73306 net.cpp:434] Scale5 <- BatchNorm5
I0905 01:04:50.537971 73306 net.cpp:395] Scale5 -> BatchNorm5 (in-place)
I0905 01:04:50.538120 73306 net.cpp:150] Setting up Scale5
I0905 01:04:50.538144 73306 net.cpp:157] Top shape: 16 64 64 64 (4194304)
I0905 01:04:50.538168 73306 net.cpp:165] Memory required for data: 327942336
I0905 01:04:50.538188 73306 layer_factory.hpp:77] Creating layer ReLU5
I0905 01:04:50.538198 73306 net.cpp:100] Creating Layer ReLU5
I0905 01:04:50.538206 73306 net.cpp:434] ReLU5 <- BatchNorm5
I0905 01:04:50.538216 73306 net.cpp:395] ReLU5 -> BatchNorm5 (in-place)
I0905 01:04:50.538597 73306 net.cpp:150] Setting up ReLU5
I0905 01:04:50.538635 73306 net.cpp:157] Top shape: 16 64 64 64 (4194304)
I0905 01:04:50.538656 73306 net.cpp:165] Memory required for data: 344719552
I0905 01:04:50.538664 73306 layer_factory.hpp:77] Creating layer Convolution6
I0905 01:04:50.538681 73306 net.cpp:100] Creating Layer Convolution6
I0905 01:04:50.538691 73306 net.cpp:434] Convolution6 <- BatchNorm5
I0905 01:04:50.538703 73306 net.cpp:408] Convolution6 -> Convolution6
I0905 01:04:50.540043 73306 net.cpp:150] Setting up Convolution6
I0905 01:04:50.540086 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.540096 73306 net.cpp:165] Memory required for data: 347865280
I0905 01:04:50.540107 73306 layer_factory.hpp:77] Creating layer Dropout6
I0905 01:04:50.540120 73306 net.cpp:100] Creating Layer Dropout6
I0905 01:04:50.540129 73306 net.cpp:434] Dropout6 <- Convolution6
I0905 01:04:50.540139 73306 net.cpp:408] Dropout6 -> Dropout6
I0905 01:04:50.540210 73306 net.cpp:150] Setting up Dropout6
I0905 01:04:50.540236 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.540256 73306 net.cpp:165] Memory required for data: 351011008
I0905 01:04:50.540282 73306 layer_factory.hpp:77] Creating layer Concat5
I0905 01:04:50.540302 73306 net.cpp:100] Creating Layer Concat5
I0905 01:04:50.540312 73306 net.cpp:434] Concat5 <- Concat4_Concat4_0_split_1
I0905 01:04:50.540320 73306 net.cpp:434] Concat5 <- Dropout6
I0905 01:04:50.540331 73306 net.cpp:408] Concat5 -> Concat5
I0905 01:04:50.540375 73306 net.cpp:150] Setting up Concat5
I0905 01:04:50.540390 73306 net.cpp:157] Top shape: 16 76 64 64 (4980736)
I0905 01:04:50.540410 73306 net.cpp:165] Memory required for data: 370933952
I0905 01:04:50.540416 73306 layer_factory.hpp:77] Creating layer Concat5_Concat5_0_split
I0905 01:04:50.540441 73306 net.cpp:100] Creating Layer Concat5_Concat5_0_split
I0905 01:04:50.540449 73306 net.cpp:434] Concat5_Concat5_0_split <- Concat5
I0905 01:04:50.540472 73306 net.cpp:408] Concat5_Concat5_0_split -> Concat5_Concat5_0_split_0
I0905 01:04:50.540483 73306 net.cpp:408] Concat5_Concat5_0_split -> Concat5_Concat5_0_split_1
I0905 01:04:50.540566 73306 net.cpp:150] Setting up Concat5_Concat5_0_split
I0905 01:04:50.540578 73306 net.cpp:157] Top shape: 16 76 64 64 (4980736)
I0905 01:04:50.540586 73306 net.cpp:157] Top shape: 16 76 64 64 (4980736)
I0905 01:04:50.540594 73306 net.cpp:165] Memory required for data: 410779840
I0905 01:04:50.540614 73306 layer_factory.hpp:77] Creating layer BatchNorm6
I0905 01:04:50.540627 73306 net.cpp:100] Creating Layer BatchNorm6
I0905 01:04:50.540635 73306 net.cpp:434] BatchNorm6 <- Concat5_Concat5_0_split_0
I0905 01:04:50.540645 73306 net.cpp:408] BatchNorm6 -> BatchNorm6
I0905 01:04:50.540962 73306 net.cpp:150] Setting up BatchNorm6
I0905 01:04:50.540977 73306 net.cpp:157] Top shape: 16 76 64 64 (4980736)
I0905 01:04:50.540984 73306 net.cpp:165] Memory required for data: 430702784
I0905 01:04:50.541000 73306 layer_factory.hpp:77] Creating layer Scale6
I0905 01:04:50.541028 73306 net.cpp:100] Creating Layer Scale6
I0905 01:04:50.541049 73306 net.cpp:434] Scale6 <- BatchNorm6
I0905 01:04:50.541057 73306 net.cpp:395] Scale6 -> BatchNorm6 (in-place)
I0905 01:04:50.541263 73306 net.cpp:150] Setting up Scale6
I0905 01:04:50.541276 73306 net.cpp:157] Top shape: 16 76 64 64 (4980736)
I0905 01:04:50.541296 73306 net.cpp:165] Memory required for data: 450625728
I0905 01:04:50.541306 73306 layer_factory.hpp:77] Creating layer ReLU6
I0905 01:04:50.541326 73306 net.cpp:100] Creating Layer ReLU6
I0905 01:04:50.541334 73306 net.cpp:434] ReLU6 <- BatchNorm6
I0905 01:04:50.541344 73306 net.cpp:395] ReLU6 -> BatchNorm6 (in-place)
I0905 01:04:50.541590 73306 net.cpp:150] Setting up ReLU6
I0905 01:04:50.541617 73306 net.cpp:157] Top shape: 16 76 64 64 (4980736)
I0905 01:04:50.541625 73306 net.cpp:165] Memory required for data: 470548672
I0905 01:04:50.541635 73306 layer_factory.hpp:77] Creating layer Convolution7
I0905 01:04:50.541651 73306 net.cpp:100] Creating Layer Convolution7
I0905 01:04:50.541658 73306 net.cpp:434] Convolution7 <- BatchNorm6
I0905 01:04:50.541669 73306 net.cpp:408] Convolution7 -> Convolution7
I0905 01:04:50.543118 73306 net.cpp:150] Setting up Convolution7
I0905 01:04:50.543149 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.543157 73306 net.cpp:165] Memory required for data: 473694400
I0905 01:04:50.543169 73306 layer_factory.hpp:77] Creating layer Dropout7
I0905 01:04:50.543179 73306 net.cpp:100] Creating Layer Dropout7
I0905 01:04:50.543189 73306 net.cpp:434] Dropout7 <- Convolution7
I0905 01:04:50.543213 73306 net.cpp:408] Dropout7 -> Dropout7
I0905 01:04:50.543306 73306 net.cpp:150] Setting up Dropout7
I0905 01:04:50.543321 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.543341 73306 net.cpp:165] Memory required for data: 476840128
I0905 01:04:50.543360 73306 layer_factory.hpp:77] Creating layer Concat6
I0905 01:04:50.543372 73306 net.cpp:100] Creating Layer Concat6
I0905 01:04:50.543380 73306 net.cpp:434] Concat6 <- Concat5_Concat5_0_split_1
I0905 01:04:50.543390 73306 net.cpp:434] Concat6 <- Dropout7
I0905 01:04:50.543398 73306 net.cpp:408] Concat6 -> Concat6
I0905 01:04:50.543445 73306 net.cpp:150] Setting up Concat6
I0905 01:04:50.543470 73306 net.cpp:157] Top shape: 16 88 64 64 (5767168)
I0905 01:04:50.543478 73306 net.cpp:165] Memory required for data: 499908800
I0905 01:04:50.543485 73306 layer_factory.hpp:77] Creating layer Concat6_Concat6_0_split
I0905 01:04:50.543496 73306 net.cpp:100] Creating Layer Concat6_Concat6_0_split
I0905 01:04:50.543515 73306 net.cpp:434] Concat6_Concat6_0_split <- Concat6
I0905 01:04:50.543527 73306 net.cpp:408] Concat6_Concat6_0_split -> Concat6_Concat6_0_split_0
I0905 01:04:50.543539 73306 net.cpp:408] Concat6_Concat6_0_split -> Concat6_Concat6_0_split_1
I0905 01:04:50.543622 73306 net.cpp:150] Setting up Concat6_Concat6_0_split
I0905 01:04:50.543645 73306 net.cpp:157] Top shape: 16 88 64 64 (5767168)
I0905 01:04:50.543665 73306 net.cpp:157] Top shape: 16 88 64 64 (5767168)
I0905 01:04:50.543673 73306 net.cpp:165] Memory required for data: 546046144
I0905 01:04:50.543691 73306 layer_factory.hpp:77] Creating layer BatchNorm7
I0905 01:04:50.543704 73306 net.cpp:100] Creating Layer BatchNorm7
I0905 01:04:50.543712 73306 net.cpp:434] BatchNorm7 <- Concat6_Concat6_0_split_0
I0905 01:04:50.543723 73306 net.cpp:408] BatchNorm7 -> BatchNorm7
I0905 01:04:50.544039 73306 net.cpp:150] Setting up BatchNorm7
I0905 01:04:50.544064 73306 net.cpp:157] Top shape: 16 88 64 64 (5767168)
I0905 01:04:50.544071 73306 net.cpp:165] Memory required for data: 569114816
I0905 01:04:50.544083 73306 layer_factory.hpp:77] Creating layer Scale7
I0905 01:04:50.544095 73306 net.cpp:100] Creating Layer Scale7
I0905 01:04:50.544101 73306 net.cpp:434] Scale7 <- BatchNorm7
I0905 01:04:50.544116 73306 net.cpp:395] Scale7 -> BatchNorm7 (in-place)
I0905 01:04:50.544282 73306 net.cpp:150] Setting up Scale7
I0905 01:04:50.544311 73306 net.cpp:157] Top shape: 16 88 64 64 (5767168)
I0905 01:04:50.544317 73306 net.cpp:165] Memory required for data: 592183488
I0905 01:04:50.544339 73306 layer_factory.hpp:77] Creating layer ReLU7
I0905 01:04:50.544348 73306 net.cpp:100] Creating Layer ReLU7
I0905 01:04:50.544356 73306 net.cpp:434] ReLU7 <- BatchNorm7
I0905 01:04:50.544364 73306 net.cpp:395] ReLU7 -> BatchNorm7 (in-place)
I0905 01:04:50.544744 73306 net.cpp:150] Setting up ReLU7
I0905 01:04:50.544772 73306 net.cpp:157] Top shape: 16 88 64 64 (5767168)
I0905 01:04:50.544781 73306 net.cpp:165] Memory required for data: 615252160
I0905 01:04:50.544790 73306 layer_factory.hpp:77] Creating layer Convolution8
I0905 01:04:50.544803 73306 net.cpp:100] Creating Layer Convolution8
I0905 01:04:50.544812 73306 net.cpp:434] Convolution8 <- BatchNorm7
I0905 01:04:50.544822 73306 net.cpp:408] Convolution8 -> Convolution8
I0905 01:04:50.546703 73306 net.cpp:150] Setting up Convolution8
I0905 01:04:50.546736 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.546756 73306 net.cpp:165] Memory required for data: 618397888
I0905 01:04:50.546767 73306 layer_factory.hpp:77] Creating layer Dropout8
I0905 01:04:50.546780 73306 net.cpp:100] Creating Layer Dropout8
I0905 01:04:50.546788 73306 net.cpp:434] Dropout8 <- Convolution8
I0905 01:04:50.546797 73306 net.cpp:408] Dropout8 -> Dropout8
I0905 01:04:50.546870 73306 net.cpp:150] Setting up Dropout8
I0905 01:04:50.546897 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.546916 73306 net.cpp:165] Memory required for data: 621543616
I0905 01:04:50.546926 73306 layer_factory.hpp:77] Creating layer Concat7
I0905 01:04:50.546936 73306 net.cpp:100] Creating Layer Concat7
I0905 01:04:50.546967 73306 net.cpp:434] Concat7 <- Concat6_Concat6_0_split_1
I0905 01:04:50.546977 73306 net.cpp:434] Concat7 <- Dropout8
I0905 01:04:50.546988 73306 net.cpp:408] Concat7 -> Concat7
I0905 01:04:50.547049 73306 net.cpp:150] Setting up Concat7
I0905 01:04:50.547061 73306 net.cpp:157] Top shape: 16 100 64 64 (6553600)
I0905 01:04:50.547070 73306 net.cpp:165] Memory required for data: 647758016
I0905 01:04:50.547077 73306 layer_factory.hpp:77] Creating layer Concat7_Concat7_0_split
I0905 01:04:50.547101 73306 net.cpp:100] Creating Layer Concat7_Concat7_0_split
I0905 01:04:50.547108 73306 net.cpp:434] Concat7_Concat7_0_split <- Concat7
I0905 01:04:50.547117 73306 net.cpp:408] Concat7_Concat7_0_split -> Concat7_Concat7_0_split_0
I0905 01:04:50.547128 73306 net.cpp:408] Concat7_Concat7_0_split -> Concat7_Concat7_0_split_1
I0905 01:04:50.547219 73306 net.cpp:150] Setting up Concat7_Concat7_0_split
I0905 01:04:50.547253 73306 net.cpp:157] Top shape: 16 100 64 64 (6553600)
I0905 01:04:50.547273 73306 net.cpp:157] Top shape: 16 100 64 64 (6553600)
I0905 01:04:50.547281 73306 net.cpp:165] Memory required for data: 700186816
I0905 01:04:50.547289 73306 layer_factory.hpp:77] Creating layer BatchNorm8
I0905 01:04:50.547313 73306 net.cpp:100] Creating Layer BatchNorm8
I0905 01:04:50.547322 73306 net.cpp:434] BatchNorm8 <- Concat7_Concat7_0_split_0
I0905 01:04:50.547330 73306 net.cpp:408] BatchNorm8 -> BatchNorm8
I0905 01:04:50.547641 73306 net.cpp:150] Setting up BatchNorm8
I0905 01:04:50.547664 73306 net.cpp:157] Top shape: 16 100 64 64 (6553600)
I0905 01:04:50.547672 73306 net.cpp:165] Memory required for data: 726401216
I0905 01:04:50.547684 73306 layer_factory.hpp:77] Creating layer Scale8
I0905 01:04:50.547694 73306 net.cpp:100] Creating Layer Scale8
I0905 01:04:50.547703 73306 net.cpp:434] Scale8 <- BatchNorm8
I0905 01:04:50.547713 73306 net.cpp:395] Scale8 -> BatchNorm8 (in-place)
I0905 01:04:50.547874 73306 net.cpp:150] Setting up Scale8
I0905 01:04:50.547900 73306 net.cpp:157] Top shape: 16 100 64 64 (6553600)
I0905 01:04:50.547919 73306 net.cpp:165] Memory required for data: 752615616
I0905 01:04:50.547929 73306 layer_factory.hpp:77] Creating layer ReLU8
I0905 01:04:50.547938 73306 net.cpp:100] Creating Layer ReLU8
I0905 01:04:50.547946 73306 net.cpp:434] ReLU8 <- BatchNorm8
I0905 01:04:50.547956 73306 net.cpp:395] ReLU8 -> BatchNorm8 (in-place)
I0905 01:04:50.548334 73306 net.cpp:150] Setting up ReLU8
I0905 01:04:50.548364 73306 net.cpp:157] Top shape: 16 100 64 64 (6553600)
I0905 01:04:50.548372 73306 net.cpp:165] Memory required for data: 778830016
I0905 01:04:50.548382 73306 layer_factory.hpp:77] Creating layer Convolution9
I0905 01:04:50.548395 73306 net.cpp:100] Creating Layer Convolution9
I0905 01:04:50.548404 73306 net.cpp:434] Convolution9 <- BatchNorm8
I0905 01:04:50.548416 73306 net.cpp:408] Convolution9 -> Convolution9
I0905 01:04:50.549875 73306 net.cpp:150] Setting up Convolution9
I0905 01:04:50.549906 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.549913 73306 net.cpp:165] Memory required for data: 781975744
I0905 01:04:50.549924 73306 layer_factory.hpp:77] Creating layer Dropout9
I0905 01:04:50.549937 73306 net.cpp:100] Creating Layer Dropout9
I0905 01:04:50.549945 73306 net.cpp:434] Dropout9 <- Convolution9
I0905 01:04:50.549955 73306 net.cpp:408] Dropout9 -> Dropout9
I0905 01:04:50.550037 73306 net.cpp:150] Setting up Dropout9
I0905 01:04:50.550062 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.550082 73306 net.cpp:165] Memory required for data: 785121472
I0905 01:04:50.550091 73306 layer_factory.hpp:77] Creating layer Concat8
I0905 01:04:50.550110 73306 net.cpp:100] Creating Layer Concat8
I0905 01:04:50.550118 73306 net.cpp:434] Concat8 <- Concat7_Concat7_0_split_1
I0905 01:04:50.550139 73306 net.cpp:434] Concat8 <- Dropout9
I0905 01:04:50.550150 73306 net.cpp:408] Concat8 -> Concat8
I0905 01:04:50.550182 73306 net.cpp:150] Setting up Concat8
I0905 01:04:50.550206 73306 net.cpp:157] Top shape: 16 112 64 64 (7340032)
I0905 01:04:50.550228 73306 net.cpp:165] Memory required for data: 814481600
I0905 01:04:50.550236 73306 layer_factory.hpp:77] Creating layer Concat8_Concat8_0_split
I0905 01:04:50.550259 73306 net.cpp:100] Creating Layer Concat8_Concat8_0_split
I0905 01:04:50.550268 73306 net.cpp:434] Concat8_Concat8_0_split <- Concat8
I0905 01:04:50.550278 73306 net.cpp:408] Concat8_Concat8_0_split -> Concat8_Concat8_0_split_0
I0905 01:04:50.550289 73306 net.cpp:408] Concat8_Concat8_0_split -> Concat8_Concat8_0_split_1
I0905 01:04:50.550397 73306 net.cpp:150] Setting up Concat8_Concat8_0_split
I0905 01:04:50.550421 73306 net.cpp:157] Top shape: 16 112 64 64 (7340032)
I0905 01:04:50.550443 73306 net.cpp:157] Top shape: 16 112 64 64 (7340032)
I0905 01:04:50.550462 73306 net.cpp:165] Memory required for data: 873201856
I0905 01:04:50.550469 73306 layer_factory.hpp:77] Creating layer BatchNorm9
I0905 01:04:50.550484 73306 net.cpp:100] Creating Layer BatchNorm9
I0905 01:04:50.550493 73306 net.cpp:434] BatchNorm9 <- Concat8_Concat8_0_split_0
I0905 01:04:50.550503 73306 net.cpp:408] BatchNorm9 -> BatchNorm9
I0905 01:04:50.551378 73306 net.cpp:150] Setting up BatchNorm9
I0905 01:04:50.551408 73306 net.cpp:157] Top shape: 16 112 64 64 (7340032)
I0905 01:04:50.551416 73306 net.cpp:165] Memory required for data: 902561984
I0905 01:04:50.551429 73306 layer_factory.hpp:77] Creating layer Scale9
I0905 01:04:50.551441 73306 net.cpp:100] Creating Layer Scale9
I0905 01:04:50.551450 73306 net.cpp:434] Scale9 <- BatchNorm9
I0905 01:04:50.551461 73306 net.cpp:395] Scale9 -> BatchNorm9 (in-place)
I0905 01:04:50.551600 73306 net.cpp:150] Setting up Scale9
I0905 01:04:50.551612 73306 net.cpp:157] Top shape: 16 112 64 64 (7340032)
I0905 01:04:50.551632 73306 net.cpp:165] Memory required for data: 931922112
I0905 01:04:50.551656 73306 layer_factory.hpp:77] Creating layer ReLU9
I0905 01:04:50.551666 73306 net.cpp:100] Creating Layer ReLU9
I0905 01:04:50.551674 73306 net.cpp:434] ReLU9 <- BatchNorm9
I0905 01:04:50.551682 73306 net.cpp:395] ReLU9 -> BatchNorm9 (in-place)
I0905 01:04:50.551923 73306 net.cpp:150] Setting up ReLU9
I0905 01:04:50.551947 73306 net.cpp:157] Top shape: 16 112 64 64 (7340032)
I0905 01:04:50.551955 73306 net.cpp:165] Memory required for data: 961282240
I0905 01:04:50.551964 73306 layer_factory.hpp:77] Creating layer Convolution10
I0905 01:04:50.551985 73306 net.cpp:100] Creating Layer Convolution10
I0905 01:04:50.551992 73306 net.cpp:434] Convolution10 <- BatchNorm9
I0905 01:04:50.552002 73306 net.cpp:408] Convolution10 -> Convolution10
I0905 01:04:50.553581 73306 net.cpp:150] Setting up Convolution10
I0905 01:04:50.553611 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.553619 73306 net.cpp:165] Memory required for data: 964427968
I0905 01:04:50.553630 73306 layer_factory.hpp:77] Creating layer Dropout10
I0905 01:04:50.553643 73306 net.cpp:100] Creating Layer Dropout10
I0905 01:04:50.553653 73306 net.cpp:434] Dropout10 <- Convolution10
I0905 01:04:50.553661 73306 net.cpp:408] Dropout10 -> Dropout10
I0905 01:04:50.553735 73306 net.cpp:150] Setting up Dropout10
I0905 01:04:50.553748 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.553756 73306 net.cpp:165] Memory required for data: 967573696
I0905 01:04:50.553764 73306 layer_factory.hpp:77] Creating layer Concat9
I0905 01:04:50.553776 73306 net.cpp:100] Creating Layer Concat9
I0905 01:04:50.553783 73306 net.cpp:434] Concat9 <- Concat8_Concat8_0_split_1
I0905 01:04:50.553803 73306 net.cpp:434] Concat9 <- Dropout10
I0905 01:04:50.553824 73306 net.cpp:408] Concat9 -> Concat9
I0905 01:04:50.553853 73306 net.cpp:150] Setting up Concat9
I0905 01:04:50.553865 73306 net.cpp:157] Top shape: 16 124 64 64 (8126464)
I0905 01:04:50.553874 73306 net.cpp:165] Memory required for data: 1000079552
I0905 01:04:50.553881 73306 layer_factory.hpp:77] Creating layer Concat9_Concat9_0_split
I0905 01:04:50.553892 73306 net.cpp:100] Creating Layer Concat9_Concat9_0_split
I0905 01:04:50.553901 73306 net.cpp:434] Concat9_Concat9_0_split <- Concat9
I0905 01:04:50.553910 73306 net.cpp:408] Concat9_Concat9_0_split -> Concat9_Concat9_0_split_0
I0905 01:04:50.553935 73306 net.cpp:408] Concat9_Concat9_0_split -> Concat9_Concat9_0_split_1
I0905 01:04:50.553977 73306 net.cpp:150] Setting up Concat9_Concat9_0_split
I0905 01:04:50.553989 73306 net.cpp:157] Top shape: 16 124 64 64 (8126464)
I0905 01:04:50.553998 73306 net.cpp:157] Top shape: 16 124 64 64 (8126464)
I0905 01:04:50.554006 73306 net.cpp:165] Memory required for data: 1065091264
I0905 01:04:50.554028 73306 layer_factory.hpp:77] Creating layer BatchNorm10
I0905 01:04:50.554051 73306 net.cpp:100] Creating Layer BatchNorm10
I0905 01:04:50.554059 73306 net.cpp:434] BatchNorm10 <- Concat9_Concat9_0_split_0
I0905 01:04:50.554080 73306 net.cpp:408] BatchNorm10 -> BatchNorm10
I0905 01:04:50.554311 73306 net.cpp:150] Setting up BatchNorm10
I0905 01:04:50.554322 73306 net.cpp:157] Top shape: 16 124 64 64 (8126464)
I0905 01:04:50.554342 73306 net.cpp:165] Memory required for data: 1097597120
I0905 01:04:50.554353 73306 layer_factory.hpp:77] Creating layer Scale10
I0905 01:04:50.554363 73306 net.cpp:100] Creating Layer Scale10
I0905 01:04:50.554371 73306 net.cpp:434] Scale10 <- BatchNorm10
I0905 01:04:50.554381 73306 net.cpp:395] Scale10 -> BatchNorm10 (in-place)
I0905 01:04:50.554518 73306 net.cpp:150] Setting up Scale10
I0905 01:04:50.554540 73306 net.cpp:157] Top shape: 16 124 64 64 (8126464)
I0905 01:04:50.554559 73306 net.cpp:165] Memory required for data: 1130102976
I0905 01:04:50.554569 73306 layer_factory.hpp:77] Creating layer ReLU10
I0905 01:04:50.554577 73306 net.cpp:100] Creating Layer ReLU10
I0905 01:04:50.554585 73306 net.cpp:434] ReLU10 <- BatchNorm10
I0905 01:04:50.554595 73306 net.cpp:395] ReLU10 -> BatchNorm10 (in-place)
I0905 01:04:50.554971 73306 net.cpp:150] Setting up ReLU10
I0905 01:04:50.555001 73306 net.cpp:157] Top shape: 16 124 64 64 (8126464)
I0905 01:04:50.555008 73306 net.cpp:165] Memory required for data: 1162608832
I0905 01:04:50.555017 73306 layer_factory.hpp:77] Creating layer Convolution11
I0905 01:04:50.555032 73306 net.cpp:100] Creating Layer Convolution11
I0905 01:04:50.555040 73306 net.cpp:434] Convolution11 <- BatchNorm10
I0905 01:04:50.555050 73306 net.cpp:408] Convolution11 -> Convolution11
I0905 01:04:50.556318 73306 net.cpp:150] Setting up Convolution11
I0905 01:04:50.556347 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.556356 73306 net.cpp:165] Memory required for data: 1165754560
I0905 01:04:50.556366 73306 layer_factory.hpp:77] Creating layer Dropout11
I0905 01:04:50.556380 73306 net.cpp:100] Creating Layer Dropout11
I0905 01:04:50.556388 73306 net.cpp:434] Dropout11 <- Convolution11
I0905 01:04:50.556397 73306 net.cpp:408] Dropout11 -> Dropout11
I0905 01:04:50.556478 73306 net.cpp:150] Setting up Dropout11
I0905 01:04:50.556491 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.556499 73306 net.cpp:165] Memory required for data: 1168900288
I0905 01:04:50.556507 73306 layer_factory.hpp:77] Creating layer Concat10
I0905 01:04:50.556545 73306 net.cpp:100] Creating Layer Concat10
I0905 01:04:50.556552 73306 net.cpp:434] Concat10 <- Concat9_Concat9_0_split_1
I0905 01:04:50.556561 73306 net.cpp:434] Concat10 <- Dropout11
I0905 01:04:50.556584 73306 net.cpp:408] Concat10 -> Concat10
I0905 01:04:50.556622 73306 net.cpp:150] Setting up Concat10
I0905 01:04:50.556637 73306 net.cpp:157] Top shape: 16 136 64 64 (8912896)
I0905 01:04:50.556644 73306 net.cpp:165] Memory required for data: 1204551872
I0905 01:04:50.556653 73306 layer_factory.hpp:77] Creating layer Concat10_Concat10_0_split
I0905 01:04:50.556664 73306 net.cpp:100] Creating Layer Concat10_Concat10_0_split
I0905 01:04:50.556673 73306 net.cpp:434] Concat10_Concat10_0_split <- Concat10
I0905 01:04:50.556681 73306 net.cpp:408] Concat10_Concat10_0_split -> Concat10_Concat10_0_split_0
I0905 01:04:50.556691 73306 net.cpp:408] Concat10_Concat10_0_split -> Concat10_Concat10_0_split_1
I0905 01:04:50.556731 73306 net.cpp:150] Setting up Concat10_Concat10_0_split
I0905 01:04:50.556743 73306 net.cpp:157] Top shape: 16 136 64 64 (8912896)
I0905 01:04:50.556766 73306 net.cpp:157] Top shape: 16 136 64 64 (8912896)
I0905 01:04:50.556774 73306 net.cpp:165] Memory required for data: 1275855040
I0905 01:04:50.556782 73306 layer_factory.hpp:77] Creating layer BatchNorm11
I0905 01:04:50.556795 73306 net.cpp:100] Creating Layer BatchNorm11
I0905 01:04:50.556803 73306 net.cpp:434] BatchNorm11 <- Concat10_Concat10_0_split_0
I0905 01:04:50.556813 73306 net.cpp:408] BatchNorm11 -> BatchNorm11
I0905 01:04:50.557013 73306 net.cpp:150] Setting up BatchNorm11
I0905 01:04:50.557026 73306 net.cpp:157] Top shape: 16 136 64 64 (8912896)
I0905 01:04:50.557046 73306 net.cpp:165] Memory required for data: 1311506624
I0905 01:04:50.557067 73306 layer_factory.hpp:77] Creating layer Scale11
I0905 01:04:50.557080 73306 net.cpp:100] Creating Layer Scale11
I0905 01:04:50.557090 73306 net.cpp:434] Scale11 <- BatchNorm11
I0905 01:04:50.557099 73306 net.cpp:395] Scale11 -> BatchNorm11 (in-place)
I0905 01:04:50.557200 73306 net.cpp:150] Setting up Scale11
I0905 01:04:50.557224 73306 net.cpp:157] Top shape: 16 136 64 64 (8912896)
I0905 01:04:50.557232 73306 net.cpp:165] Memory required for data: 1347158208
I0905 01:04:50.557242 73306 layer_factory.hpp:77] Creating layer ReLU11
I0905 01:04:50.557252 73306 net.cpp:100] Creating Layer ReLU11
I0905 01:04:50.557260 73306 net.cpp:434] ReLU11 <- BatchNorm11
I0905 01:04:50.557271 73306 net.cpp:395] ReLU11 -> BatchNorm11 (in-place)
I0905 01:04:50.557611 73306 net.cpp:150] Setting up ReLU11
I0905 01:04:50.557641 73306 net.cpp:157] Top shape: 16 136 64 64 (8912896)
I0905 01:04:50.557651 73306 net.cpp:165] Memory required for data: 1382809792
I0905 01:04:50.557659 73306 layer_factory.hpp:77] Creating layer Convolution12
I0905 01:04:50.557677 73306 net.cpp:100] Creating Layer Convolution12
I0905 01:04:50.557685 73306 net.cpp:434] Convolution12 <- BatchNorm11
I0905 01:04:50.557696 73306 net.cpp:408] Convolution12 -> Convolution12
I0905 01:04:50.559156 73306 net.cpp:150] Setting up Convolution12
I0905 01:04:50.559187 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.559195 73306 net.cpp:165] Memory required for data: 1385955520
I0905 01:04:50.559206 73306 layer_factory.hpp:77] Creating layer Dropout12
I0905 01:04:50.559219 73306 net.cpp:100] Creating Layer Dropout12
I0905 01:04:50.559228 73306 net.cpp:434] Dropout12 <- Convolution12
I0905 01:04:50.559239 73306 net.cpp:408] Dropout12 -> Dropout12
I0905 01:04:50.559306 73306 net.cpp:150] Setting up Dropout12
I0905 01:04:50.559319 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.559327 73306 net.cpp:165] Memory required for data: 1389101248
I0905 01:04:50.559335 73306 layer_factory.hpp:77] Creating layer Concat11
I0905 01:04:50.559350 73306 net.cpp:100] Creating Layer Concat11
I0905 01:04:50.559360 73306 net.cpp:434] Concat11 <- Concat10_Concat10_0_split_1
I0905 01:04:50.559368 73306 net.cpp:434] Concat11 <- Dropout12
I0905 01:04:50.559378 73306 net.cpp:408] Concat11 -> Concat11
I0905 01:04:50.559407 73306 net.cpp:150] Setting up Concat11
I0905 01:04:50.559419 73306 net.cpp:157] Top shape: 16 148 64 64 (9699328)
I0905 01:04:50.559427 73306 net.cpp:165] Memory required for data: 1427898560
I0905 01:04:50.559435 73306 layer_factory.hpp:77] Creating layer Concat11_Concat11_0_split
I0905 01:04:50.559447 73306 net.cpp:100] Creating Layer Concat11_Concat11_0_split
I0905 01:04:50.559454 73306 net.cpp:434] Concat11_Concat11_0_split <- Concat11
I0905 01:04:50.559465 73306 net.cpp:408] Concat11_Concat11_0_split -> Concat11_Concat11_0_split_0
I0905 01:04:50.559476 73306 net.cpp:408] Concat11_Concat11_0_split -> Concat11_Concat11_0_split_1
I0905 01:04:50.559514 73306 net.cpp:150] Setting up Concat11_Concat11_0_split
I0905 01:04:50.559530 73306 net.cpp:157] Top shape: 16 148 64 64 (9699328)
I0905 01:04:50.559538 73306 net.cpp:157] Top shape: 16 148 64 64 (9699328)
I0905 01:04:50.559545 73306 net.cpp:165] Memory required for data: 1505493184
I0905 01:04:50.559553 73306 layer_factory.hpp:77] Creating layer BatchNorm12
I0905 01:04:50.559564 73306 net.cpp:100] Creating Layer BatchNorm12
I0905 01:04:50.559586 73306 net.cpp:434] BatchNorm12 <- Concat11_Concat11_0_split_0
I0905 01:04:50.559598 73306 net.cpp:408] BatchNorm12 -> BatchNorm12
I0905 01:04:50.559811 73306 net.cpp:150] Setting up BatchNorm12
I0905 01:04:50.559837 73306 net.cpp:157] Top shape: 16 148 64 64 (9699328)
I0905 01:04:50.559845 73306 net.cpp:165] Memory required for data: 1544290496
I0905 01:04:50.559859 73306 layer_factory.hpp:77] Creating layer Scale12
I0905 01:04:50.559871 73306 net.cpp:100] Creating Layer Scale12
I0905 01:04:50.559880 73306 net.cpp:434] Scale12 <- BatchNorm12
I0905 01:04:50.559890 73306 net.cpp:395] Scale12 -> BatchNorm12 (in-place)
I0905 01:04:50.559991 73306 net.cpp:150] Setting up Scale12
I0905 01:04:50.560015 73306 net.cpp:157] Top shape: 16 148 64 64 (9699328)
I0905 01:04:50.560024 73306 net.cpp:165] Memory required for data: 1583087808
I0905 01:04:50.560034 73306 layer_factory.hpp:77] Creating layer ReLU12
I0905 01:04:50.560045 73306 net.cpp:100] Creating Layer ReLU12
I0905 01:04:50.560053 73306 net.cpp:434] ReLU12 <- BatchNorm12
I0905 01:04:50.560062 73306 net.cpp:395] ReLU12 -> BatchNorm12 (in-place)
I0905 01:04:50.560266 73306 net.cpp:150] Setting up ReLU12
I0905 01:04:50.560293 73306 net.cpp:157] Top shape: 16 148 64 64 (9699328)
I0905 01:04:50.560302 73306 net.cpp:165] Memory required for data: 1621885120
I0905 01:04:50.560310 73306 layer_factory.hpp:77] Creating layer Convolution13
I0905 01:04:50.560325 73306 net.cpp:100] Creating Layer Convolution13
I0905 01:04:50.560333 73306 net.cpp:434] Convolution13 <- BatchNorm12
I0905 01:04:50.560346 73306 net.cpp:408] Convolution13 -> Convolution13
I0905 01:04:50.561797 73306 net.cpp:150] Setting up Convolution13
I0905 01:04:50.561827 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.561836 73306 net.cpp:165] Memory required for data: 1625030848
I0905 01:04:50.561847 73306 layer_factory.hpp:77] Creating layer Dropout13
I0905 01:04:50.561858 73306 net.cpp:100] Creating Layer Dropout13
I0905 01:04:50.561867 73306 net.cpp:434] Dropout13 <- Convolution13
I0905 01:04:50.561879 73306 net.cpp:408] Dropout13 -> Dropout13
I0905 01:04:50.561947 73306 net.cpp:150] Setting up Dropout13
I0905 01:04:50.561960 73306 net.cpp:157] Top shape: 16 12 64 64 (786432)
I0905 01:04:50.561969 73306 net.cpp:165] Memory required for data: 1628176576
I0905 01:04:50.561975 73306 layer_factory.hpp:77] Creating layer Concat12
I0905 01:04:50.562000 73306 net.cpp:100] Creating Layer Concat12
I0905 01:04:50.562021 73306 net.cpp:434] Concat12 <- Concat11_Concat11_0_split_1
I0905 01:04:50.562041 73306 net.cpp:434] Concat12 <- Dropout13
I0905 01:04:50.562053 73306 net.cpp:408] Concat12 -> Concat12
I0905 01:04:50.562090 73306 net.cpp:150] Setting up Concat12
I0905 01:04:50.562103 73306 net.cpp:157] Top shape: 16 160 64 64 (10485760)
I0905 01:04:50.562110 73306 net.cpp:165] Memory required for data: 1670119616
I0905 01:04:50.562119 73306 layer_factory.hpp:77] Creating layer BatchNorm13
I0905 01:04:50.562134 73306 net.cpp:100] Creating Layer BatchNorm13
I0905 01:04:50.562142 73306 net.cpp:434] BatchNorm13 <- Concat12
I0905 01:04:50.562153 73306 net.cpp:408] BatchNorm13 -> BatchNorm13
I0905 01:04:50.562376 73306 net.cpp:150] Setting up BatchNorm13
I0905 01:04:50.562400 73306 net.cpp:157] Top shape: 16 160 64 64 (10485760)
I0905 01:04:50.562408 73306 net.cpp:165] Memory required for data: 1712062656
I0905 01:04:50.562420 73306 layer_factory.hpp:77] Creating layer Scale13
I0905 01:04:50.562430 73306 net.cpp:100] Creating Layer Scale13
I0905 01:04:50.562438 73306 net.cpp:434] Scale13 <- BatchNorm13
I0905 01:04:50.562446 73306 net.cpp:395] Scale13 -> BatchNorm13 (in-place)
I0905 01:04:50.562582 73306 net.cpp:150] Setting up Scale13
I0905 01:04:50.562605 73306 net.cpp:157] Top shape: 16 160 64 64 (10485760)
I0905 01:04:50.562623 73306 net.cpp:165] Memory required for data: 1754005696
I0905 01:04:50.562649 73306 layer_factory.hpp:77] Creating layer ReLU13
I0905 01:04:50.562661 73306 net.cpp:100] Creating Layer ReLU13
I0905 01:04:50.562669 73306 net.cpp:434] ReLU13 <- BatchNorm13
I0905 01:04:50.562679 73306 net.cpp:395] ReLU13 -> BatchNorm13 (in-place)
I0905 01:04:50.563051 73306 net.cpp:150] Setting up ReLU13
I0905 01:04:50.563079 73306 net.cpp:157] Top shape: 16 160 64 64 (10485760)
I0905 01:04:50.563088 73306 net.cpp:165] Memory required for data: 1795948736
I0905 01:04:50.563097 73306 layer_factory.hpp:77] Creating layer Convolution14
I0905 01:04:50.563112 73306 net.cpp:100] Creating Layer Convolution14
I0905 01:04:50.563119 73306 net.cpp:434] Convolution14 <- BatchNorm13
I0905 01:04:50.563132 73306 net.cpp:408] Convolution14 -> Convolution14
I0905 01:04:50.564770 73306 net.cpp:150] Setting up Convolution14
I0905 01:04:50.564802 73306 net.cpp:157] Top shape: 16 160 64 64 (10485760)
I0905 01:04:50.564811 73306 net.cpp:165] Memory required for data: 1837891776
I0905 01:04:50.564822 73306 layer_factory.hpp:77] Creating layer Dropout14
I0905 01:04:50.564832 73306 net.cpp:100] Creating Layer Dropout14
I0905 01:04:50.564841 73306 net.cpp:434] Dropout14 <- Convolution14
I0905 01:04:50.564851 73306 net.cpp:408] Dropout14 -> Dropout14
I0905 01:04:50.564920 73306 net.cpp:150] Setting up Dropout14
I0905 01:04:50.564934 73306 net.cpp:157] Top shape: 16 160 64 64 (10485760)
I0905 01:04:50.564941 73306 net.cpp:165] Memory required for data: 1879834816
I0905 01:04:50.564949 73306 layer_factory.hpp:77] Creating layer Pooling1
I0905 01:04:50.564961 73306 net.cpp:100] Creating Layer Pooling1
I0905 01:04:50.564970 73306 net.cpp:434] Pooling1 <- Dropout14
I0905 01:04:50.564992 73306 net.cpp:408] Pooling1 -> Pooling1
I0905 01:04:50.565369 73306 net.cpp:150] Setting up Pooling1
I0905 01:04:50.565398 73306 net.cpp:157] Top shape: 16 160 32 32 (2621440)
I0905 01:04:50.565408 73306 net.cpp:165] Memory required for data: 1890320576
I0905 01:04:50.565417 73306 layer_factory.hpp:77] Creating layer Pooling1_Pooling1_0_split
I0905 01:04:50.565426 73306 net.cpp:100] Creating Layer Pooling1_Pooling1_0_split
I0905 01:04:50.565435 73306 net.cpp:434] Pooling1_Pooling1_0_split <- Pooling1
I0905 01:04:50.565444 73306 net.cpp:408] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_0
I0905 01:04:50.565454 73306 net.cpp:408] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_1
I0905 01:04:50.565522 73306 net.cpp:150] Setting up Pooling1_Pooling1_0_split
I0905 01:04:50.565536 73306 net.cpp:157] Top shape: 16 160 32 32 (2621440)
I0905 01:04:50.565543 73306 net.cpp:157] Top shape: 16 160 32 32 (2621440)
I0905 01:04:50.565551 73306 net.cpp:165] Memory required for data: 1911292096
I0905 01:04:50.565558 73306 layer_factory.hpp:77] Creating layer BatchNorm14
I0905 01:04:50.565572 73306 net.cpp:100] Creating Layer BatchNorm14
I0905 01:04:50.565579 73306 net.cpp:434] BatchNorm14 <- Pooling1_Pooling1_0_split_0
I0905 01:04:50.565589 73306 net.cpp:408] BatchNorm14 -> BatchNorm14
I0905 01:04:50.565822 73306 net.cpp:150] Setting up BatchNorm14
I0905 01:04:50.565850 73306 net.cpp:157] Top shape: 16 160 32 32 (2621440)
I0905 01:04:50.565857 73306 net.cpp:165] Memory required for data: 1921777856
I0905 01:04:50.565871 73306 layer_factory.hpp:77] Creating layer Scale14
I0905 01:04:50.565881 73306 net.cpp:100] Creating Layer Scale14
I0905 01:04:50.565889 73306 net.cpp:434] Scale14 <- BatchNorm14
I0905 01:04:50.565899 73306 net.cpp:395] Scale14 -> BatchNorm14 (in-place)
I0905 01:04:50.566036 73306 net.cpp:150] Setting up Scale14
I0905 01:04:50.566049 73306 net.cpp:157] Top shape: 16 160 32 32 (2621440)
I0905 01:04:50.566069 73306 net.cpp:165] Memory required for data: 1932263616
I0905 01:04:50.566078 73306 layer_factory.hpp:77] Creating layer ReLU14
I0905 01:04:50.566090 73306 net.cpp:100] Creating Layer ReLU14
I0905 01:04:50.566098 73306 net.cpp:434] ReLU14 <- BatchNorm14
I0905 01:04:50.566107 73306 net.cpp:395] ReLU14 -> BatchNorm14 (in-place)
I0905 01:04:50.566488 73306 net.cpp:150] Setting up ReLU14
I0905 01:04:50.566517 73306 net.cpp:157] Top shape: 16 160 32 32 (2621440)
I0905 01:04:50.566525 73306 net.cpp:165] Memory required for data: 1942749376
I0905 01:04:50.566534 73306 layer_factory.hpp:77] Creating layer Convolution15
I0905 01:04:50.566550 73306 net.cpp:100] Creating Layer Convolution15
I0905 01:04:50.566584 73306 net.cpp:434] Convolution15 <- BatchNorm14
I0905 01:04:50.566612 73306 net.cpp:408] Convolution15 -> Convolution15
I0905 01:04:50.568097 73306 net.cpp:150] Setting up Convolution15
I0905 01:04:50.568130 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.568138 73306 net.cpp:165] Memory required for data: 1943535808
I0905 01:04:50.568150 73306 layer_factory.hpp:77] Creating layer Dropout15
I0905 01:04:50.568166 73306 net.cpp:100] Creating Layer Dropout15
I0905 01:04:50.568174 73306 net.cpp:434] Dropout15 <- Convolution15
I0905 01:04:50.568184 73306 net.cpp:408] Dropout15 -> Dropout15
I0905 01:04:50.568255 73306 net.cpp:150] Setting up Dropout15
I0905 01:04:50.568267 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.568287 73306 net.cpp:165] Memory required for data: 1944322240
I0905 01:04:50.568295 73306 layer_factory.hpp:77] Creating layer Concat13
I0905 01:04:50.568318 73306 net.cpp:100] Creating Layer Concat13
I0905 01:04:50.568326 73306 net.cpp:434] Concat13 <- Pooling1_Pooling1_0_split_1
I0905 01:04:50.568336 73306 net.cpp:434] Concat13 <- Dropout15
I0905 01:04:50.568344 73306 net.cpp:408] Concat13 -> Concat13
I0905 01:04:50.568372 73306 net.cpp:150] Setting up Concat13
I0905 01:04:50.568395 73306 net.cpp:157] Top shape: 16 172 32 32 (2818048)
I0905 01:04:50.568403 73306 net.cpp:165] Memory required for data: 1955594432
I0905 01:04:50.568410 73306 layer_factory.hpp:77] Creating layer Concat13_Concat13_0_split
I0905 01:04:50.568423 73306 net.cpp:100] Creating Layer Concat13_Concat13_0_split
I0905 01:04:50.568431 73306 net.cpp:434] Concat13_Concat13_0_split <- Concat13
I0905 01:04:50.568444 73306 net.cpp:408] Concat13_Concat13_0_split -> Concat13_Concat13_0_split_0
I0905 01:04:50.568465 73306 net.cpp:408] Concat13_Concat13_0_split -> Concat13_Concat13_0_split_1
I0905 01:04:50.568513 73306 net.cpp:150] Setting up Concat13_Concat13_0_split
I0905 01:04:50.568526 73306 net.cpp:157] Top shape: 16 172 32 32 (2818048)
I0905 01:04:50.568534 73306 net.cpp:157] Top shape: 16 172 32 32 (2818048)
I0905 01:04:50.568542 73306 net.cpp:165] Memory required for data: 1978138816
I0905 01:04:50.568550 73306 layer_factory.hpp:77] Creating layer BatchNorm15
I0905 01:04:50.568562 73306 net.cpp:100] Creating Layer BatchNorm15
I0905 01:04:50.568583 73306 net.cpp:434] BatchNorm15 <- Concat13_Concat13_0_split_0
I0905 01:04:50.568593 73306 net.cpp:408] BatchNorm15 -> BatchNorm15
I0905 01:04:50.568821 73306 net.cpp:150] Setting up BatchNorm15
I0905 01:04:50.568835 73306 net.cpp:157] Top shape: 16 172 32 32 (2818048)
I0905 01:04:50.568855 73306 net.cpp:165] Memory required for data: 1989411008
I0905 01:04:50.568866 73306 layer_factory.hpp:77] Creating layer Scale15
I0905 01:04:50.568879 73306 net.cpp:100] Creating Layer Scale15
I0905 01:04:50.568887 73306 net.cpp:434] Scale15 <- BatchNorm15
I0905 01:04:50.568897 73306 net.cpp:395] Scale15 -> BatchNorm15 (in-place)
I0905 01:04:50.569031 73306 net.cpp:150] Setting up Scale15
I0905 01:04:50.569056 73306 net.cpp:157] Top shape: 16 172 32 32 (2818048)
I0905 01:04:50.569063 73306 net.cpp:165] Memory required for data: 2000683200
I0905 01:04:50.569072 73306 layer_factory.hpp:77] Creating layer ReLU15
I0905 01:04:50.569082 73306 net.cpp:100] Creating Layer ReLU15
I0905 01:04:50.569089 73306 net.cpp:434] ReLU15 <- BatchNorm15
I0905 01:04:50.569100 73306 net.cpp:395] ReLU15 -> BatchNorm15 (in-place)
I0905 01:04:50.569483 73306 net.cpp:150] Setting up ReLU15
I0905 01:04:50.569514 73306 net.cpp:157] Top shape: 16 172 32 32 (2818048)
I0905 01:04:50.569522 73306 net.cpp:165] Memory required for data: 2011955392
I0905 01:04:50.569530 73306 layer_factory.hpp:77] Creating layer Convolution16
I0905 01:04:50.569548 73306 net.cpp:100] Creating Layer Convolution16
I0905 01:04:50.569557 73306 net.cpp:434] Convolution16 <- BatchNorm15
I0905 01:04:50.569568 73306 net.cpp:408] Convolution16 -> Convolution16
I0905 01:04:50.571254 73306 net.cpp:150] Setting up Convolution16
I0905 01:04:50.571285 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.571310 73306 net.cpp:165] Memory required for data: 2012741824
I0905 01:04:50.571333 73306 layer_factory.hpp:77] Creating layer Dropout16
I0905 01:04:50.571357 73306 net.cpp:100] Creating Layer Dropout16
I0905 01:04:50.571377 73306 net.cpp:434] Dropout16 <- Convolution16
I0905 01:04:50.571390 73306 net.cpp:408] Dropout16 -> Dropout16
I0905 01:04:50.571473 73306 net.cpp:150] Setting up Dropout16
I0905 01:04:50.571501 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.571521 73306 net.cpp:165] Memory required for data: 2013528256
I0905 01:04:50.571529 73306 layer_factory.hpp:77] Creating layer Concat14
I0905 01:04:50.571550 73306 net.cpp:100] Creating Layer Concat14
I0905 01:04:50.571559 73306 net.cpp:434] Concat14 <- Concat13_Concat13_0_split_1
I0905 01:04:50.571579 73306 net.cpp:434] Concat14 <- Dropout16
I0905 01:04:50.571588 73306 net.cpp:408] Concat14 -> Concat14
I0905 01:04:50.571630 73306 net.cpp:150] Setting up Concat14
I0905 01:04:50.571655 73306 net.cpp:157] Top shape: 16 184 32 32 (3014656)
I0905 01:04:50.571665 73306 net.cpp:165] Memory required for data: 2025586880
I0905 01:04:50.571671 73306 layer_factory.hpp:77] Creating layer Concat14_Concat14_0_split
I0905 01:04:50.571681 73306 net.cpp:100] Creating Layer Concat14_Concat14_0_split
I0905 01:04:50.571708 73306 net.cpp:434] Concat14_Concat14_0_split <- Concat14
I0905 01:04:50.571729 73306 net.cpp:408] Concat14_Concat14_0_split -> Concat14_Concat14_0_split_0
I0905 01:04:50.571753 73306 net.cpp:408] Concat14_Concat14_0_split -> Concat14_Concat14_0_split_1
I0905 01:04:50.571802 73306 net.cpp:150] Setting up Concat14_Concat14_0_split
I0905 01:04:50.571815 73306 net.cpp:157] Top shape: 16 184 32 32 (3014656)
I0905 01:04:50.571822 73306 net.cpp:157] Top shape: 16 184 32 32 (3014656)
I0905 01:04:50.571842 73306 net.cpp:165] Memory required for data: 2049704128
I0905 01:04:50.571851 73306 layer_factory.hpp:77] Creating layer BatchNorm16
I0905 01:04:50.571861 73306 net.cpp:100] Creating Layer BatchNorm16
I0905 01:04:50.571880 73306 net.cpp:434] BatchNorm16 <- Concat14_Concat14_0_split_0
I0905 01:04:50.571905 73306 net.cpp:408] BatchNorm16 -> BatchNorm16
I0905 01:04:50.572155 73306 net.cpp:150] Setting up BatchNorm16
I0905 01:04:50.572178 73306 net.cpp:157] Top shape: 16 184 32 32 (3014656)
I0905 01:04:50.572186 73306 net.cpp:165] Memory required for data: 2061762752
I0905 01:04:50.572197 73306 layer_factory.hpp:77] Creating layer Scale16
I0905 01:04:50.572211 73306 net.cpp:100] Creating Layer Scale16
I0905 01:04:50.572218 73306 net.cpp:434] Scale16 <- BatchNorm16
I0905 01:04:50.572227 73306 net.cpp:395] Scale16 -> BatchNorm16 (in-place)
I0905 01:04:50.572377 73306 net.cpp:150] Setting up Scale16
I0905 01:04:50.572404 73306 net.cpp:157] Top shape: 16 184 32 32 (3014656)
I0905 01:04:50.572412 73306 net.cpp:165] Memory required for data: 2073821376
I0905 01:04:50.572433 73306 layer_factory.hpp:77] Creating layer ReLU16
I0905 01:04:50.572445 73306 net.cpp:100] Creating Layer ReLU16
I0905 01:04:50.572453 73306 net.cpp:434] ReLU16 <- BatchNorm16
I0905 01:04:50.572460 73306 net.cpp:395] ReLU16 -> BatchNorm16 (in-place)
I0905 01:04:50.572713 73306 net.cpp:150] Setting up ReLU16
I0905 01:04:50.572739 73306 net.cpp:157] Top shape: 16 184 32 32 (3014656)
I0905 01:04:50.572746 73306 net.cpp:165] Memory required for data: 2085880000
I0905 01:04:50.572756 73306 layer_factory.hpp:77] Creating layer Convolution17
I0905 01:04:50.572769 73306 net.cpp:100] Creating Layer Convolution17
I0905 01:04:50.572777 73306 net.cpp:434] Convolution17 <- BatchNorm16
I0905 01:04:50.572789 73306 net.cpp:408] Convolution17 -> Convolution17
I0905 01:04:50.574653 73306 net.cpp:150] Setting up Convolution17
I0905 01:04:50.574686 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.574694 73306 net.cpp:165] Memory required for data: 2086666432
I0905 01:04:50.574705 73306 layer_factory.hpp:77] Creating layer Dropout17
I0905 01:04:50.574715 73306 net.cpp:100] Creating Layer Dropout17
I0905 01:04:50.574724 73306 net.cpp:434] Dropout17 <- Convolution17
I0905 01:04:50.574760 73306 net.cpp:408] Dropout17 -> Dropout17
I0905 01:04:50.574822 73306 net.cpp:150] Setting up Dropout17
I0905 01:04:50.574853 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.574862 73306 net.cpp:165] Memory required for data: 2087452864
I0905 01:04:50.574882 73306 layer_factory.hpp:77] Creating layer Concat15
I0905 01:04:50.574894 73306 net.cpp:100] Creating Layer Concat15
I0905 01:04:50.574903 73306 net.cpp:434] Concat15 <- Concat14_Concat14_0_split_1
I0905 01:04:50.574911 73306 net.cpp:434] Concat15 <- Dropout17
I0905 01:04:50.574921 73306 net.cpp:408] Concat15 -> Concat15
I0905 01:04:50.574977 73306 net.cpp:150] Setting up Concat15
I0905 01:04:50.574990 73306 net.cpp:157] Top shape: 16 196 32 32 (3211264)
I0905 01:04:50.574997 73306 net.cpp:165] Memory required for data: 2100297920
I0905 01:04:50.575016 73306 layer_factory.hpp:77] Creating layer Concat15_Concat15_0_split
I0905 01:04:50.575027 73306 net.cpp:100] Creating Layer Concat15_Concat15_0_split
I0905 01:04:50.575047 73306 net.cpp:434] Concat15_Concat15_0_split <- Concat15
I0905 01:04:50.575058 73306 net.cpp:408] Concat15_Concat15_0_split -> Concat15_Concat15_0_split_0
I0905 01:04:50.575069 73306 net.cpp:408] Concat15_Concat15_0_split -> Concat15_Concat15_0_split_1
I0905 01:04:50.575106 73306 net.cpp:150] Setting up Concat15_Concat15_0_split
I0905 01:04:50.575117 73306 net.cpp:157] Top shape: 16 196 32 32 (3211264)
I0905 01:04:50.575126 73306 net.cpp:157] Top shape: 16 196 32 32 (3211264)
I0905 01:04:50.575134 73306 net.cpp:165] Memory required for data: 2125988032
I0905 01:04:50.575153 73306 layer_factory.hpp:77] Creating layer BatchNorm17
I0905 01:04:50.575166 73306 net.cpp:100] Creating Layer BatchNorm17
I0905 01:04:50.575186 73306 net.cpp:434] BatchNorm17 <- Concat15_Concat15_0_split_0
I0905 01:04:50.575197 73306 net.cpp:408] BatchNorm17 -> BatchNorm17
I0905 01:04:50.575485 73306 net.cpp:150] Setting up BatchNorm17
I0905 01:04:50.575510 73306 net.cpp:157] Top shape: 16 196 32 32 (3211264)
I0905 01:04:50.575517 73306 net.cpp:165] Memory required for data: 2138833088
I0905 01:04:50.575530 73306 layer_factory.hpp:77] Creating layer Scale17
I0905 01:04:50.575539 73306 net.cpp:100] Creating Layer Scale17
I0905 01:04:50.575547 73306 net.cpp:434] Scale17 <- BatchNorm17
I0905 01:04:50.575558 73306 net.cpp:395] Scale17 -> BatchNorm17 (in-place)
I0905 01:04:50.575711 73306 net.cpp:150] Setting up Scale17
I0905 01:04:50.575736 73306 net.cpp:157] Top shape: 16 196 32 32 (3211264)
I0905 01:04:50.575744 73306 net.cpp:165] Memory required for data: 2151678144
I0905 01:04:50.575767 73306 layer_factory.hpp:77] Creating layer ReLU17
I0905 01:04:50.575776 73306 net.cpp:100] Creating Layer ReLU17
I0905 01:04:50.575784 73306 net.cpp:434] ReLU17 <- BatchNorm17
I0905 01:04:50.575793 73306 net.cpp:395] ReLU17 -> BatchNorm17 (in-place)
I0905 01:04:50.576189 73306 net.cpp:150] Setting up ReLU17
I0905 01:04:50.576217 73306 net.cpp:157] Top shape: 16 196 32 32 (3211264)
I0905 01:04:50.576225 73306 net.cpp:165] Memory required for data: 2164523200
I0905 01:04:50.576234 73306 layer_factory.hpp:77] Creating layer Convolution18
I0905 01:04:50.576249 73306 net.cpp:100] Creating Layer Convolution18
I0905 01:04:50.576257 73306 net.cpp:434] Convolution18 <- BatchNorm17
I0905 01:04:50.576268 73306 net.cpp:408] Convolution18 -> Convolution18
I0905 01:04:50.577812 73306 net.cpp:150] Setting up Convolution18
I0905 01:04:50.577842 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.577850 73306 net.cpp:165] Memory required for data: 2165309632
I0905 01:04:50.577862 73306 layer_factory.hpp:77] Creating layer Dropout18
I0905 01:04:50.577875 73306 net.cpp:100] Creating Layer Dropout18
I0905 01:04:50.577884 73306 net.cpp:434] Dropout18 <- Convolution18
I0905 01:04:50.577894 73306 net.cpp:408] Dropout18 -> Dropout18
I0905 01:04:50.577951 73306 net.cpp:150] Setting up Dropout18
I0905 01:04:50.577965 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.577983 73306 net.cpp:165] Memory required for data: 2166096064
I0905 01:04:50.577991 73306 layer_factory.hpp:77] Creating layer Concat16
I0905 01:04:50.578042 73306 net.cpp:100] Creating Layer Concat16
I0905 01:04:50.578063 73306 net.cpp:434] Concat16 <- Concat15_Concat15_0_split_1
I0905 01:04:50.578074 73306 net.cpp:434] Concat16 <- Dropout18
I0905 01:04:50.578097 73306 net.cpp:408] Concat16 -> Concat16
I0905 01:04:50.578140 73306 net.cpp:150] Setting up Concat16
I0905 01:04:50.578176 73306 net.cpp:157] Top shape: 16 208 32 32 (3407872)
I0905 01:04:50.578186 73306 net.cpp:165] Memory required for data: 2179727552
I0905 01:04:50.578193 73306 layer_factory.hpp:77] Creating layer Concat16_Concat16_0_split
I0905 01:04:50.578215 73306 net.cpp:100] Creating Layer Concat16_Concat16_0_split
I0905 01:04:50.578223 73306 net.cpp:434] Concat16_Concat16_0_split <- Concat16
I0905 01:04:50.578243 73306 net.cpp:408] Concat16_Concat16_0_split -> Concat16_Concat16_0_split_0
I0905 01:04:50.578265 73306 net.cpp:408] Concat16_Concat16_0_split -> Concat16_Concat16_0_split_1
I0905 01:04:50.578308 73306 net.cpp:150] Setting up Concat16_Concat16_0_split
I0905 01:04:50.578332 73306 net.cpp:157] Top shape: 16 208 32 32 (3407872)
I0905 01:04:50.578353 73306 net.cpp:157] Top shape: 16 208 32 32 (3407872)
I0905 01:04:50.578361 73306 net.cpp:165] Memory required for data: 2206990528
I0905 01:04:50.578368 73306 layer_factory.hpp:77] Creating layer BatchNorm18
I0905 01:04:50.578392 73306 net.cpp:100] Creating Layer BatchNorm18
I0905 01:04:50.578399 73306 net.cpp:434] BatchNorm18 <- Concat16_Concat16_0_split_0
I0905 01:04:50.578421 73306 net.cpp:408] BatchNorm18 -> BatchNorm18
I0905 01:04:50.578711 73306 net.cpp:150] Setting up BatchNorm18
I0905 01:04:50.578737 73306 net.cpp:157] Top shape: 16 208 32 32 (3407872)
I0905 01:04:50.578757 73306 net.cpp:165] Memory required for data: 2220622016
I0905 01:04:50.578768 73306 layer_factory.hpp:77] Creating layer Scale18
I0905 01:04:50.578778 73306 net.cpp:100] Creating Layer Scale18
I0905 01:04:50.578786 73306 net.cpp:434] Scale18 <- BatchNorm18
I0905 01:04:50.578795 73306 net.cpp:395] Scale18 -> BatchNorm18 (in-place)
I0905 01:04:50.578946 73306 net.cpp:150] Setting up Scale18
I0905 01:04:50.578970 73306 net.cpp:157] Top shape: 16 208 32 32 (3407872)
I0905 01:04:50.578979 73306 net.cpp:165] Memory required for data: 2234253504
I0905 01:04:50.578987 73306 layer_factory.hpp:77] Creating layer ReLU18
I0905 01:04:50.578997 73306 net.cpp:100] Creating Layer ReLU18
I0905 01:04:50.579005 73306 net.cpp:434] ReLU18 <- BatchNorm18
I0905 01:04:50.579027 73306 net.cpp:395] ReLU18 -> BatchNorm18 (in-place)
I0905 01:04:50.579413 73306 net.cpp:150] Setting up ReLU18
I0905 01:04:50.579442 73306 net.cpp:157] Top shape: 16 208 32 32 (3407872)
I0905 01:04:50.579450 73306 net.cpp:165] Memory required for data: 2247884992
I0905 01:04:50.579458 73306 layer_factory.hpp:77] Creating layer Convolution19
I0905 01:04:50.579473 73306 net.cpp:100] Creating Layer Convolution19
I0905 01:04:50.579481 73306 net.cpp:434] Convolution19 <- BatchNorm18
I0905 01:04:50.579493 73306 net.cpp:408] Convolution19 -> Convolution19
I0905 01:04:50.581308 73306 net.cpp:150] Setting up Convolution19
I0905 01:04:50.581339 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.581347 73306 net.cpp:165] Memory required for data: 2248671424
I0905 01:04:50.581358 73306 layer_factory.hpp:77] Creating layer Dropout19
I0905 01:04:50.581369 73306 net.cpp:100] Creating Layer Dropout19
I0905 01:04:50.581377 73306 net.cpp:434] Dropout19 <- Convolution19
I0905 01:04:50.581389 73306 net.cpp:408] Dropout19 -> Dropout19
I0905 01:04:50.581445 73306 net.cpp:150] Setting up Dropout19
I0905 01:04:50.581459 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.581477 73306 net.cpp:165] Memory required for data: 2249457856
I0905 01:04:50.581485 73306 layer_factory.hpp:77] Creating layer Concat17
I0905 01:04:50.581519 73306 net.cpp:100] Creating Layer Concat17
I0905 01:04:50.581542 73306 net.cpp:434] Concat17 <- Concat16_Concat16_0_split_1
I0905 01:04:50.581552 73306 net.cpp:434] Concat17 <- Dropout19
I0905 01:04:50.581564 73306 net.cpp:408] Concat17 -> Concat17
I0905 01:04:50.581614 73306 net.cpp:150] Setting up Concat17
I0905 01:04:50.581627 73306 net.cpp:157] Top shape: 16 220 32 32 (3604480)
I0905 01:04:50.581636 73306 net.cpp:165] Memory required for data: 2263875776
I0905 01:04:50.581645 73306 layer_factory.hpp:77] Creating layer Concat17_Concat17_0_split
I0905 01:04:50.581671 73306 net.cpp:100] Creating Layer Concat17_Concat17_0_split
I0905 01:04:50.581679 73306 net.cpp:434] Concat17_Concat17_0_split <- Concat17
I0905 01:04:50.581701 73306 net.cpp:408] Concat17_Concat17_0_split -> Concat17_Concat17_0_split_0
I0905 01:04:50.581712 73306 net.cpp:408] Concat17_Concat17_0_split -> Concat17_Concat17_0_split_1
I0905 01:04:50.581763 73306 net.cpp:150] Setting up Concat17_Concat17_0_split
I0905 01:04:50.581789 73306 net.cpp:157] Top shape: 16 220 32 32 (3604480)
I0905 01:04:50.581807 73306 net.cpp:157] Top shape: 16 220 32 32 (3604480)
I0905 01:04:50.581828 73306 net.cpp:165] Memory required for data: 2292711616
I0905 01:04:50.581835 73306 layer_factory.hpp:77] Creating layer BatchNorm19
I0905 01:04:50.581858 73306 net.cpp:100] Creating Layer BatchNorm19
I0905 01:04:50.581867 73306 net.cpp:434] BatchNorm19 <- Concat17_Concat17_0_split_0
I0905 01:04:50.581878 73306 net.cpp:408] BatchNorm19 -> BatchNorm19
I0905 01:04:50.582149 73306 net.cpp:150] Setting up BatchNorm19
I0905 01:04:50.582172 73306 net.cpp:157] Top shape: 16 220 32 32 (3604480)
I0905 01:04:50.582180 73306 net.cpp:165] Memory required for data: 2307129536
I0905 01:04:50.582192 73306 layer_factory.hpp:77] Creating layer Scale19
I0905 01:04:50.582202 73306 net.cpp:100] Creating Layer Scale19
I0905 01:04:50.582211 73306 net.cpp:434] Scale19 <- BatchNorm19
I0905 01:04:50.582221 73306 net.cpp:395] Scale19 -> BatchNorm19 (in-place)
I0905 01:04:50.582366 73306 net.cpp:150] Setting up Scale19
I0905 01:04:50.582391 73306 net.cpp:157] Top shape: 16 220 32 32 (3604480)
I0905 01:04:50.582399 73306 net.cpp:165] Memory required for data: 2321547456
I0905 01:04:50.582420 73306 layer_factory.hpp:77] Creating layer ReLU19
I0905 01:04:50.582430 73306 net.cpp:100] Creating Layer ReLU19
I0905 01:04:50.582439 73306 net.cpp:434] ReLU19 <- BatchNorm19
I0905 01:04:50.582460 73306 net.cpp:395] ReLU19 -> BatchNorm19 (in-place)
I0905 01:04:50.582731 73306 net.cpp:150] Setting up ReLU19
I0905 01:04:50.582772 73306 net.cpp:157] Top shape: 16 220 32 32 (3604480)
I0905 01:04:50.582779 73306 net.cpp:165] Memory required for data: 2335965376
I0905 01:04:50.582787 73306 layer_factory.hpp:77] Creating layer Convolution20
I0905 01:04:50.582814 73306 net.cpp:100] Creating Layer Convolution20
I0905 01:04:50.582823 73306 net.cpp:434] Convolution20 <- BatchNorm19
I0905 01:04:50.582834 73306 net.cpp:408] Convolution20 -> Convolution20
I0905 01:04:50.584580 73306 net.cpp:150] Setting up Convolution20
I0905 01:04:50.584610 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.584619 73306 net.cpp:165] Memory required for data: 2336751808
I0905 01:04:50.584630 73306 layer_factory.hpp:77] Creating layer Dropout20
I0905 01:04:50.584640 73306 net.cpp:100] Creating Layer Dropout20
I0905 01:04:50.584652 73306 net.cpp:434] Dropout20 <- Convolution20
I0905 01:04:50.584662 73306 net.cpp:408] Dropout20 -> Dropout20
I0905 01:04:50.584718 73306 net.cpp:150] Setting up Dropout20
I0905 01:04:50.584745 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.584764 73306 net.cpp:165] Memory required for data: 2337538240
I0905 01:04:50.584784 73306 layer_factory.hpp:77] Creating layer Concat18
I0905 01:04:50.584794 73306 net.cpp:100] Creating Layer Concat18
I0905 01:04:50.584815 73306 net.cpp:434] Concat18 <- Concat17_Concat17_0_split_1
I0905 01:04:50.584825 73306 net.cpp:434] Concat18 <- Dropout20
I0905 01:04:50.584833 73306 net.cpp:408] Concat18 -> Concat18
I0905 01:04:50.584861 73306 net.cpp:150] Setting up Concat18
I0905 01:04:50.584887 73306 net.cpp:157] Top shape: 16 232 32 32 (3801088)
I0905 01:04:50.584893 73306 net.cpp:165] Memory required for data: 2352742592
I0905 01:04:50.584900 73306 layer_factory.hpp:77] Creating layer Concat18_Concat18_0_split
I0905 01:04:50.584950 73306 net.cpp:100] Creating Layer Concat18_Concat18_0_split
I0905 01:04:50.584974 73306 net.cpp:434] Concat18_Concat18_0_split <- Concat18
I0905 01:04:50.584995 73306 net.cpp:408] Concat18_Concat18_0_split -> Concat18_Concat18_0_split_0
I0905 01:04:50.585017 73306 net.cpp:408] Concat18_Concat18_0_split -> Concat18_Concat18_0_split_1
I0905 01:04:50.585086 73306 net.cpp:150] Setting up Concat18_Concat18_0_split
I0905 01:04:50.585099 73306 net.cpp:157] Top shape: 16 232 32 32 (3801088)
I0905 01:04:50.585108 73306 net.cpp:157] Top shape: 16 232 32 32 (3801088)
I0905 01:04:50.585116 73306 net.cpp:165] Memory required for data: 2383151296
I0905 01:04:50.585124 73306 layer_factory.hpp:77] Creating layer BatchNorm20
I0905 01:04:50.585135 73306 net.cpp:100] Creating Layer BatchNorm20
I0905 01:04:50.585144 73306 net.cpp:434] BatchNorm20 <- Concat18_Concat18_0_split_0
I0905 01:04:50.585156 73306 net.cpp:408] BatchNorm20 -> BatchNorm20
I0905 01:04:50.585425 73306 net.cpp:150] Setting up BatchNorm20
I0905 01:04:50.585449 73306 net.cpp:157] Top shape: 16 232 32 32 (3801088)
I0905 01:04:50.585458 73306 net.cpp:165] Memory required for data: 2398355648
I0905 01:04:50.585469 73306 layer_factory.hpp:77] Creating layer Scale20
I0905 01:04:50.585482 73306 net.cpp:100] Creating Layer Scale20
I0905 01:04:50.585491 73306 net.cpp:434] Scale20 <- BatchNorm20
I0905 01:04:50.585500 73306 net.cpp:395] Scale20 -> BatchNorm20 (in-place)
I0905 01:04:50.585625 73306 net.cpp:150] Setting up Scale20
I0905 01:04:50.585650 73306 net.cpp:157] Top shape: 16 232 32 32 (3801088)
I0905 01:04:50.585659 73306 net.cpp:165] Memory required for data: 2413560000
I0905 01:04:50.585678 73306 layer_factory.hpp:77] Creating layer ReLU20
I0905 01:04:50.585690 73306 net.cpp:100] Creating Layer ReLU20
I0905 01:04:50.585698 73306 net.cpp:434] ReLU20 <- BatchNorm20
I0905 01:04:50.585707 73306 net.cpp:395] ReLU20 -> BatchNorm20 (in-place)
I0905 01:04:50.586081 73306 net.cpp:150] Setting up ReLU20
I0905 01:04:50.586110 73306 net.cpp:157] Top shape: 16 232 32 32 (3801088)
I0905 01:04:50.586119 73306 net.cpp:165] Memory required for data: 2428764352
I0905 01:04:50.586127 73306 layer_factory.hpp:77] Creating layer Convolution21
I0905 01:04:50.586143 73306 net.cpp:100] Creating Layer Convolution21
I0905 01:04:50.586151 73306 net.cpp:434] Convolution21 <- BatchNorm20
I0905 01:04:50.586164 73306 net.cpp:408] Convolution21 -> Convolution21
I0905 01:04:50.587923 73306 net.cpp:150] Setting up Convolution21
I0905 01:04:50.587954 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.587963 73306 net.cpp:165] Memory required for data: 2429550784
I0905 01:04:50.587975 73306 layer_factory.hpp:77] Creating layer Dropout21
I0905 01:04:50.587985 73306 net.cpp:100] Creating Layer Dropout21
I0905 01:04:50.587996 73306 net.cpp:434] Dropout21 <- Convolution21
I0905 01:04:50.588006 73306 net.cpp:408] Dropout21 -> Dropout21
I0905 01:04:50.588064 73306 net.cpp:150] Setting up Dropout21
I0905 01:04:50.588080 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.588088 73306 net.cpp:165] Memory required for data: 2430337216
I0905 01:04:50.588096 73306 layer_factory.hpp:77] Creating layer Concat19
I0905 01:04:50.588105 73306 net.cpp:100] Creating Layer Concat19
I0905 01:04:50.588115 73306 net.cpp:434] Concat19 <- Concat18_Concat18_0_split_1
I0905 01:04:50.588124 73306 net.cpp:434] Concat19 <- Dropout21
I0905 01:04:50.588135 73306 net.cpp:408] Concat19 -> Concat19
I0905 01:04:50.588163 73306 net.cpp:150] Setting up Concat19
I0905 01:04:50.588176 73306 net.cpp:157] Top shape: 16 244 32 32 (3997696)
I0905 01:04:50.588184 73306 net.cpp:165] Memory required for data: 2446328000
I0905 01:04:50.588191 73306 layer_factory.hpp:77] Creating layer Concat19_Concat19_0_split
I0905 01:04:50.588203 73306 net.cpp:100] Creating Layer Concat19_Concat19_0_split
I0905 01:04:50.588212 73306 net.cpp:434] Concat19_Concat19_0_split <- Concat19
I0905 01:04:50.588222 73306 net.cpp:408] Concat19_Concat19_0_split -> Concat19_Concat19_0_split_0
I0905 01:04:50.588232 73306 net.cpp:408] Concat19_Concat19_0_split -> Concat19_Concat19_0_split_1
I0905 01:04:50.588292 73306 net.cpp:150] Setting up Concat19_Concat19_0_split
I0905 01:04:50.588306 73306 net.cpp:157] Top shape: 16 244 32 32 (3997696)
I0905 01:04:50.588315 73306 net.cpp:157] Top shape: 16 244 32 32 (3997696)
I0905 01:04:50.588322 73306 net.cpp:165] Memory required for data: 2478309568
I0905 01:04:50.588330 73306 layer_factory.hpp:77] Creating layer BatchNorm21
I0905 01:04:50.588340 73306 net.cpp:100] Creating Layer BatchNorm21
I0905 01:04:50.588348 73306 net.cpp:434] BatchNorm21 <- Concat19_Concat19_0_split_0
I0905 01:04:50.588361 73306 net.cpp:408] BatchNorm21 -> BatchNorm21
I0905 01:04:50.588580 73306 net.cpp:150] Setting up BatchNorm21
I0905 01:04:50.588593 73306 net.cpp:157] Top shape: 16 244 32 32 (3997696)
I0905 01:04:50.588613 73306 net.cpp:165] Memory required for data: 2494300352
I0905 01:04:50.588625 73306 layer_factory.hpp:77] Creating layer Scale21
I0905 01:04:50.588639 73306 net.cpp:100] Creating Layer Scale21
I0905 01:04:50.588649 73306 net.cpp:434] Scale21 <- BatchNorm21
I0905 01:04:50.588657 73306 net.cpp:395] Scale21 -> BatchNorm21 (in-place)
I0905 01:04:50.588770 73306 net.cpp:150] Setting up Scale21
I0905 01:04:50.588783 73306 net.cpp:157] Top shape: 16 244 32 32 (3997696)
I0905 01:04:50.588804 73306 net.cpp:165] Memory required for data: 2510291136
I0905 01:04:50.588812 73306 layer_factory.hpp:77] Creating layer ReLU21
I0905 01:04:50.588824 73306 net.cpp:100] Creating Layer ReLU21
I0905 01:04:50.588832 73306 net.cpp:434] ReLU21 <- BatchNorm21
I0905 01:04:50.588841 73306 net.cpp:395] ReLU21 -> BatchNorm21 (in-place)
I0905 01:04:50.589218 73306 net.cpp:150] Setting up ReLU21
I0905 01:04:50.589248 73306 net.cpp:157] Top shape: 16 244 32 32 (3997696)
I0905 01:04:50.589257 73306 net.cpp:165] Memory required for data: 2526281920
I0905 01:04:50.589267 73306 layer_factory.hpp:77] Creating layer Convolution22
I0905 01:04:50.589282 73306 net.cpp:100] Creating Layer Convolution22
I0905 01:04:50.589293 73306 net.cpp:434] Convolution22 <- BatchNorm21
I0905 01:04:50.589306 73306 net.cpp:408] Convolution22 -> Convolution22
I0905 01:04:50.591248 73306 net.cpp:150] Setting up Convolution22
I0905 01:04:50.591280 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.591290 73306 net.cpp:165] Memory required for data: 2527068352
I0905 01:04:50.591301 73306 layer_factory.hpp:77] Creating layer Dropout22
I0905 01:04:50.591312 73306 net.cpp:100] Creating Layer Dropout22
I0905 01:04:50.591333 73306 net.cpp:434] Dropout22 <- Convolution22
I0905 01:04:50.591344 73306 net.cpp:408] Dropout22 -> Dropout22
I0905 01:04:50.591406 73306 net.cpp:150] Setting up Dropout22
I0905 01:04:50.591441 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.591449 73306 net.cpp:165] Memory required for data: 2527854784
I0905 01:04:50.591457 73306 layer_factory.hpp:77] Creating layer Concat20
I0905 01:04:50.591480 73306 net.cpp:100] Creating Layer Concat20
I0905 01:04:50.591487 73306 net.cpp:434] Concat20 <- Concat19_Concat19_0_split_1
I0905 01:04:50.591496 73306 net.cpp:434] Concat20 <- Dropout22
I0905 01:04:50.591508 73306 net.cpp:408] Concat20 -> Concat20
I0905 01:04:50.591548 73306 net.cpp:150] Setting up Concat20
I0905 01:04:50.591562 73306 net.cpp:157] Top shape: 16 256 32 32 (4194304)
I0905 01:04:50.591570 73306 net.cpp:165] Memory required for data: 2544632000
I0905 01:04:50.591578 73306 layer_factory.hpp:77] Creating layer Concat20_Concat20_0_split
I0905 01:04:50.591588 73306 net.cpp:100] Creating Layer Concat20_Concat20_0_split
I0905 01:04:50.591596 73306 net.cpp:434] Concat20_Concat20_0_split <- Concat20
I0905 01:04:50.591605 73306 net.cpp:408] Concat20_Concat20_0_split -> Concat20_Concat20_0_split_0
I0905 01:04:50.591616 73306 net.cpp:408] Concat20_Concat20_0_split -> Concat20_Concat20_0_split_1
I0905 01:04:50.591657 73306 net.cpp:150] Setting up Concat20_Concat20_0_split
I0905 01:04:50.591670 73306 net.cpp:157] Top shape: 16 256 32 32 (4194304)
I0905 01:04:50.591678 73306 net.cpp:157] Top shape: 16 256 32 32 (4194304)
I0905 01:04:50.591699 73306 net.cpp:165] Memory required for data: 2578186432
I0905 01:04:50.591708 73306 layer_factory.hpp:77] Creating layer BatchNorm22
I0905 01:04:50.591720 73306 net.cpp:100] Creating Layer BatchNorm22
I0905 01:04:50.591729 73306 net.cpp:434] BatchNorm22 <- Concat20_Concat20_0_split_0
I0905 01:04:50.591739 73306 net.cpp:408] BatchNorm22 -> BatchNorm22
I0905 01:04:50.591967 73306 net.cpp:150] Setting up BatchNorm22
I0905 01:04:50.591992 73306 net.cpp:157] Top shape: 16 256 32 32 (4194304)
I0905 01:04:50.592000 73306 net.cpp:165] Memory required for data: 2594963648
I0905 01:04:50.592027 73306 layer_factory.hpp:77] Creating layer Scale22
I0905 01:04:50.592054 73306 net.cpp:100] Creating Layer Scale22
I0905 01:04:50.592063 73306 net.cpp:434] Scale22 <- BatchNorm22
I0905 01:04:50.592073 73306 net.cpp:395] Scale22 -> BatchNorm22 (in-place)
I0905 01:04:50.592200 73306 net.cpp:150] Setting up Scale22
I0905 01:04:50.592223 73306 net.cpp:157] Top shape: 16 256 32 32 (4194304)
I0905 01:04:50.592231 73306 net.cpp:165] Memory required for data: 2611740864
I0905 01:04:50.592241 73306 layer_factory.hpp:77] Creating layer ReLU22
I0905 01:04:50.592250 73306 net.cpp:100] Creating Layer ReLU22
I0905 01:04:50.592258 73306 net.cpp:434] ReLU22 <- BatchNorm22
I0905 01:04:50.592267 73306 net.cpp:395] ReLU22 -> BatchNorm22 (in-place)
I0905 01:04:50.592500 73306 net.cpp:150] Setting up ReLU22
I0905 01:04:50.592527 73306 net.cpp:157] Top shape: 16 256 32 32 (4194304)
I0905 01:04:50.592538 73306 net.cpp:165] Memory required for data: 2628518080
I0905 01:04:50.592545 73306 layer_factory.hpp:77] Creating layer Convolution23
I0905 01:04:50.592558 73306 net.cpp:100] Creating Layer Convolution23
I0905 01:04:50.592567 73306 net.cpp:434] Convolution23 <- BatchNorm22
I0905 01:04:50.592592 73306 net.cpp:408] Convolution23 -> Convolution23
I0905 01:04:50.595136 73306 net.cpp:150] Setting up Convolution23
I0905 01:04:50.595168 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.595178 73306 net.cpp:165] Memory required for data: 2629304512
I0905 01:04:50.595190 73306 layer_factory.hpp:77] Creating layer Dropout23
I0905 01:04:50.595202 73306 net.cpp:100] Creating Layer Dropout23
I0905 01:04:50.595223 73306 net.cpp:434] Dropout23 <- Convolution23
I0905 01:04:50.595234 73306 net.cpp:408] Dropout23 -> Dropout23
I0905 01:04:50.595286 73306 net.cpp:150] Setting up Dropout23
I0905 01:04:50.595299 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.595307 73306 net.cpp:165] Memory required for data: 2630090944
I0905 01:04:50.595315 73306 layer_factory.hpp:77] Creating layer Concat21
I0905 01:04:50.595326 73306 net.cpp:100] Creating Layer Concat21
I0905 01:04:50.595335 73306 net.cpp:434] Concat21 <- Concat20_Concat20_0_split_1
I0905 01:04:50.595345 73306 net.cpp:434] Concat21 <- Dropout23
I0905 01:04:50.595356 73306 net.cpp:408] Concat21 -> Concat21
I0905 01:04:50.595384 73306 net.cpp:150] Setting up Concat21
I0905 01:04:50.595396 73306 net.cpp:157] Top shape: 16 268 32 32 (4390912)
I0905 01:04:50.595404 73306 net.cpp:165] Memory required for data: 2647654592
I0905 01:04:50.595412 73306 layer_factory.hpp:77] Creating layer Concat21_Concat21_0_split
I0905 01:04:50.595424 73306 net.cpp:100] Creating Layer Concat21_Concat21_0_split
I0905 01:04:50.595432 73306 net.cpp:434] Concat21_Concat21_0_split <- Concat21
I0905 01:04:50.595443 73306 net.cpp:408] Concat21_Concat21_0_split -> Concat21_Concat21_0_split_0
I0905 01:04:50.595453 73306 net.cpp:408] Concat21_Concat21_0_split -> Concat21_Concat21_0_split_1
I0905 01:04:50.595495 73306 net.cpp:150] Setting up Concat21_Concat21_0_split
I0905 01:04:50.595509 73306 net.cpp:157] Top shape: 16 268 32 32 (4390912)
I0905 01:04:50.595517 73306 net.cpp:157] Top shape: 16 268 32 32 (4390912)
I0905 01:04:50.595525 73306 net.cpp:165] Memory required for data: 2682781888
I0905 01:04:50.595532 73306 layer_factory.hpp:77] Creating layer BatchNorm23
I0905 01:04:50.595544 73306 net.cpp:100] Creating Layer BatchNorm23
I0905 01:04:50.595552 73306 net.cpp:434] BatchNorm23 <- Concat21_Concat21_0_split_0
I0905 01:04:50.595576 73306 net.cpp:408] BatchNorm23 -> BatchNorm23
I0905 01:04:50.595820 73306 net.cpp:150] Setting up BatchNorm23
I0905 01:04:50.595834 73306 net.cpp:157] Top shape: 16 268 32 32 (4390912)
I0905 01:04:50.595854 73306 net.cpp:165] Memory required for data: 2700345536
I0905 01:04:50.595866 73306 layer_factory.hpp:77] Creating layer Scale23
I0905 01:04:50.595880 73306 net.cpp:100] Creating Layer Scale23
I0905 01:04:50.595890 73306 net.cpp:434] Scale23 <- BatchNorm23
I0905 01:04:50.595899 73306 net.cpp:395] Scale23 -> BatchNorm23 (in-place)
I0905 01:04:50.596026 73306 net.cpp:150] Setting up Scale23
I0905 01:04:50.596040 73306 net.cpp:157] Top shape: 16 268 32 32 (4390912)
I0905 01:04:50.596060 73306 net.cpp:165] Memory required for data: 2717909184
I0905 01:04:50.596070 73306 layer_factory.hpp:77] Creating layer ReLU23
I0905 01:04:50.596079 73306 net.cpp:100] Creating Layer ReLU23
I0905 01:04:50.596087 73306 net.cpp:434] ReLU23 <- BatchNorm23
I0905 01:04:50.596101 73306 net.cpp:395] ReLU23 -> BatchNorm23 (in-place)
I0905 01:04:50.596616 73306 net.cpp:150] Setting up ReLU23
I0905 01:04:50.596647 73306 net.cpp:157] Top shape: 16 268 32 32 (4390912)
I0905 01:04:50.596657 73306 net.cpp:165] Memory required for data: 2735472832
I0905 01:04:50.596664 73306 layer_factory.hpp:77] Creating layer Convolution24
I0905 01:04:50.596681 73306 net.cpp:100] Creating Layer Convolution24
I0905 01:04:50.596691 73306 net.cpp:434] Convolution24 <- BatchNorm23
I0905 01:04:50.596706 73306 net.cpp:408] Convolution24 -> Convolution24
I0905 01:04:50.598548 73306 net.cpp:150] Setting up Convolution24
I0905 01:04:50.598579 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.598588 73306 net.cpp:165] Memory required for data: 2736259264
I0905 01:04:50.598600 73306 layer_factory.hpp:77] Creating layer Dropout24
I0905 01:04:50.598613 73306 net.cpp:100] Creating Layer Dropout24
I0905 01:04:50.598640 73306 net.cpp:434] Dropout24 <- Convolution24
I0905 01:04:50.598656 73306 net.cpp:408] Dropout24 -> Dropout24
I0905 01:04:50.598706 73306 net.cpp:150] Setting up Dropout24
I0905 01:04:50.598719 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.598738 73306 net.cpp:165] Memory required for data: 2737045696
I0905 01:04:50.598764 73306 layer_factory.hpp:77] Creating layer Concat22
I0905 01:04:50.598788 73306 net.cpp:100] Creating Layer Concat22
I0905 01:04:50.598795 73306 net.cpp:434] Concat22 <- Concat21_Concat21_0_split_1
I0905 01:04:50.598804 73306 net.cpp:434] Concat22 <- Dropout24
I0905 01:04:50.598814 73306 net.cpp:408] Concat22 -> Concat22
I0905 01:04:50.598855 73306 net.cpp:150] Setting up Concat22
I0905 01:04:50.598866 73306 net.cpp:157] Top shape: 16 280 32 32 (4587520)
I0905 01:04:50.598875 73306 net.cpp:165] Memory required for data: 2755395776
I0905 01:04:50.598883 73306 layer_factory.hpp:77] Creating layer Concat22_Concat22_0_split
I0905 01:04:50.598892 73306 net.cpp:100] Creating Layer Concat22_Concat22_0_split
I0905 01:04:50.598901 73306 net.cpp:434] Concat22_Concat22_0_split <- Concat22
I0905 01:04:50.598912 73306 net.cpp:408] Concat22_Concat22_0_split -> Concat22_Concat22_0_split_0
I0905 01:04:50.598923 73306 net.cpp:408] Concat22_Concat22_0_split -> Concat22_Concat22_0_split_1
I0905 01:04:50.598963 73306 net.cpp:150] Setting up Concat22_Concat22_0_split
I0905 01:04:50.598978 73306 net.cpp:157] Top shape: 16 280 32 32 (4587520)
I0905 01:04:50.598987 73306 net.cpp:157] Top shape: 16 280 32 32 (4587520)
I0905 01:04:50.598995 73306 net.cpp:165] Memory required for data: 2792095936
I0905 01:04:50.599004 73306 layer_factory.hpp:77] Creating layer BatchNorm24
I0905 01:04:50.599014 73306 net.cpp:100] Creating Layer BatchNorm24
I0905 01:04:50.599021 73306 net.cpp:434] BatchNorm24 <- Concat22_Concat22_0_split_0
I0905 01:04:50.599033 73306 net.cpp:408] BatchNorm24 -> BatchNorm24
I0905 01:04:50.599282 73306 net.cpp:150] Setting up BatchNorm24
I0905 01:04:50.599305 73306 net.cpp:157] Top shape: 16 280 32 32 (4587520)
I0905 01:04:50.599313 73306 net.cpp:165] Memory required for data: 2810446016
I0905 01:04:50.599337 73306 layer_factory.hpp:77] Creating layer Scale24
I0905 01:04:50.599364 73306 net.cpp:100] Creating Layer Scale24
I0905 01:04:50.599375 73306 net.cpp:434] Scale24 <- BatchNorm24
I0905 01:04:50.599385 73306 net.cpp:395] Scale24 -> BatchNorm24 (in-place)
I0905 01:04:50.599521 73306 net.cpp:150] Setting up Scale24
I0905 01:04:50.599534 73306 net.cpp:157] Top shape: 16 280 32 32 (4587520)
I0905 01:04:50.599553 73306 net.cpp:165] Memory required for data: 2828796096
I0905 01:04:50.599563 73306 layer_factory.hpp:77] Creating layer ReLU24
I0905 01:04:50.599578 73306 net.cpp:100] Creating Layer ReLU24
I0905 01:04:50.599586 73306 net.cpp:434] ReLU24 <- BatchNorm24
I0905 01:04:50.599608 73306 net.cpp:395] ReLU24 -> BatchNorm24 (in-place)
I0905 01:04:50.600010 73306 net.cpp:150] Setting up ReLU24
I0905 01:04:50.600040 73306 net.cpp:157] Top shape: 16 280 32 32 (4587520)
I0905 01:04:50.600049 73306 net.cpp:165] Memory required for data: 2847146176
I0905 01:04:50.600057 73306 layer_factory.hpp:77] Creating layer Convolution25
I0905 01:04:50.600075 73306 net.cpp:100] Creating Layer Convolution25
I0905 01:04:50.600095 73306 net.cpp:434] Convolution25 <- BatchNorm24
I0905 01:04:50.600107 73306 net.cpp:408] Convolution25 -> Convolution25
I0905 01:04:50.602114 73306 net.cpp:150] Setting up Convolution25
I0905 01:04:50.602145 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.602154 73306 net.cpp:165] Memory required for data: 2847932608
I0905 01:04:50.602165 73306 layer_factory.hpp:77] Creating layer Dropout25
I0905 01:04:50.602179 73306 net.cpp:100] Creating Layer Dropout25
I0905 01:04:50.602200 73306 net.cpp:434] Dropout25 <- Convolution25
I0905 01:04:50.602211 73306 net.cpp:408] Dropout25 -> Dropout25
I0905 01:04:50.602267 73306 net.cpp:150] Setting up Dropout25
I0905 01:04:50.602290 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.602311 73306 net.cpp:165] Memory required for data: 2848719040
I0905 01:04:50.602319 73306 layer_factory.hpp:77] Creating layer Concat23
I0905 01:04:50.602341 73306 net.cpp:100] Creating Layer Concat23
I0905 01:04:50.602349 73306 net.cpp:434] Concat23 <- Concat22_Concat22_0_split_1
I0905 01:04:50.602358 73306 net.cpp:434] Concat23 <- Dropout25
I0905 01:04:50.602370 73306 net.cpp:408] Concat23 -> Concat23
I0905 01:04:50.602398 73306 net.cpp:150] Setting up Concat23
I0905 01:04:50.602423 73306 net.cpp:157] Top shape: 16 292 32 32 (4784128)
I0905 01:04:50.602432 73306 net.cpp:165] Memory required for data: 2867855552
I0905 01:04:50.602439 73306 layer_factory.hpp:77] Creating layer Concat23_Concat23_0_split
I0905 01:04:50.602449 73306 net.cpp:100] Creating Layer Concat23_Concat23_0_split
I0905 01:04:50.602458 73306 net.cpp:434] Concat23_Concat23_0_split <- Concat23
I0905 01:04:50.602466 73306 net.cpp:408] Concat23_Concat23_0_split -> Concat23_Concat23_0_split_0
I0905 01:04:50.602478 73306 net.cpp:408] Concat23_Concat23_0_split -> Concat23_Concat23_0_split_1
I0905 01:04:50.602520 73306 net.cpp:150] Setting up Concat23_Concat23_0_split
I0905 01:04:50.602532 73306 net.cpp:157] Top shape: 16 292 32 32 (4784128)
I0905 01:04:50.602541 73306 net.cpp:157] Top shape: 16 292 32 32 (4784128)
I0905 01:04:50.602550 73306 net.cpp:165] Memory required for data: 2906128576
I0905 01:04:50.602556 73306 layer_factory.hpp:77] Creating layer BatchNorm25
I0905 01:04:50.602568 73306 net.cpp:100] Creating Layer BatchNorm25
I0905 01:04:50.602577 73306 net.cpp:434] BatchNorm25 <- Concat23_Concat23_0_split_0
I0905 01:04:50.602587 73306 net.cpp:408] BatchNorm25 -> BatchNorm25
I0905 01:04:50.602833 73306 net.cpp:150] Setting up BatchNorm25
I0905 01:04:50.602847 73306 net.cpp:157] Top shape: 16 292 32 32 (4784128)
I0905 01:04:50.602867 73306 net.cpp:165] Memory required for data: 2925265088
I0905 01:04:50.602880 73306 layer_factory.hpp:77] Creating layer Scale25
I0905 01:04:50.602891 73306 net.cpp:100] Creating Layer Scale25
I0905 01:04:50.602900 73306 net.cpp:434] Scale25 <- BatchNorm25
I0905 01:04:50.602910 73306 net.cpp:395] Scale25 -> BatchNorm25 (in-place)
I0905 01:04:50.603047 73306 net.cpp:150] Setting up Scale25
I0905 01:04:50.603085 73306 net.cpp:157] Top shape: 16 292 32 32 (4784128)
I0905 01:04:50.603092 73306 net.cpp:165] Memory required for data: 2944401600
I0905 01:04:50.603101 73306 layer_factory.hpp:77] Creating layer ReLU25
I0905 01:04:50.603112 73306 net.cpp:100] Creating Layer ReLU25
I0905 01:04:50.603121 73306 net.cpp:434] ReLU25 <- BatchNorm25
I0905 01:04:50.603143 73306 net.cpp:395] ReLU25 -> BatchNorm25 (in-place)
I0905 01:04:50.603366 73306 net.cpp:150] Setting up ReLU25
I0905 01:04:50.603394 73306 net.cpp:157] Top shape: 16 292 32 32 (4784128)
I0905 01:04:50.603401 73306 net.cpp:165] Memory required for data: 2963538112
I0905 01:04:50.603410 73306 layer_factory.hpp:77] Creating layer Convolution26
I0905 01:04:50.603426 73306 net.cpp:100] Creating Layer Convolution26
I0905 01:04:50.603435 73306 net.cpp:434] Convolution26 <- BatchNorm25
I0905 01:04:50.603457 73306 net.cpp:408] Convolution26 -> Convolution26
I0905 01:04:50.605574 73306 net.cpp:150] Setting up Convolution26
I0905 01:04:50.605607 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.605617 73306 net.cpp:165] Memory required for data: 2964324544
I0905 01:04:50.605628 73306 layer_factory.hpp:77] Creating layer Dropout26
I0905 01:04:50.605639 73306 net.cpp:100] Creating Layer Dropout26
I0905 01:04:50.605648 73306 net.cpp:434] Dropout26 <- Convolution26
I0905 01:04:50.605674 73306 net.cpp:408] Dropout26 -> Dropout26
I0905 01:04:50.605725 73306 net.cpp:150] Setting up Dropout26
I0905 01:04:50.605739 73306 net.cpp:157] Top shape: 16 12 32 32 (196608)
I0905 01:04:50.605749 73306 net.cpp:165] Memory required for data: 2965110976
I0905 01:04:50.605757 73306 layer_factory.hpp:77] Creating layer Concat24
I0905 01:04:50.605768 73306 net.cpp:100] Creating Layer Concat24
I0905 01:04:50.605787 73306 net.cpp:434] Concat24 <- Concat23_Concat23_0_split_1
I0905 01:04:50.605808 73306 net.cpp:434] Concat24 <- Dropout26
I0905 01:04:50.605819 73306 net.cpp:408] Concat24 -> Concat24
I0905 01:04:50.605850 73306 net.cpp:150] Setting up Concat24
I0905 01:04:50.605864 73306 net.cpp:157] Top shape: 16 304 32 32 (4980736)
I0905 01:04:50.605871 73306 net.cpp:165] Memory required for data: 2985033920
I0905 01:04:50.605880 73306 layer_factory.hpp:77] Creating layer BatchNorm26
I0905 01:04:50.605891 73306 net.cpp:100] Creating Layer BatchNorm26
I0905 01:04:50.605901 73306 net.cpp:434] BatchNorm26 <- Concat24
I0905 01:04:50.605911 73306 net.cpp:408] BatchNorm26 -> BatchNorm26
I0905 01:04:50.606169 73306 net.cpp:150] Setting up BatchNorm26
I0905 01:04:50.606195 73306 net.cpp:157] Top shape: 16 304 32 32 (4980736)
I0905 01:04:50.606204 73306 net.cpp:165] Memory required for data: 3004956864
I0905 01:04:50.606216 73306 layer_factory.hpp:77] Creating layer Scale26
I0905 01:04:50.606230 73306 net.cpp:100] Creating Layer Scale26
I0905 01:04:50.606251 73306 net.cpp:434] Scale26 <- BatchNorm26
I0905 01:04:50.606261 73306 net.cpp:395] Scale26 -> BatchNorm26 (in-place)
I0905 01:04:50.606401 73306 net.cpp:150] Setting up Scale26
I0905 01:04:50.606415 73306 net.cpp:157] Top shape: 16 304 32 32 (4980736)
I0905 01:04:50.606434 73306 net.cpp:165] Memory required for data: 3024879808
I0905 01:04:50.606444 73306 layer_factory.hpp:77] Creating layer ReLU26
I0905 01:04:50.606453 73306 net.cpp:100] Creating Layer ReLU26
I0905 01:04:50.606462 73306 net.cpp:434] ReLU26 <- BatchNorm26
I0905 01:04:50.606487 73306 net.cpp:395] ReLU26 -> BatchNorm26 (in-place)
I0905 01:04:50.606878 73306 net.cpp:150] Setting up ReLU26
I0905 01:04:50.606909 73306 net.cpp:157] Top shape: 16 304 32 32 (4980736)
I0905 01:04:50.606917 73306 net.cpp:165] Memory required for data: 3044802752
I0905 01:04:50.606926 73306 layer_factory.hpp:77] Creating layer Convolution27
I0905 01:04:50.606942 73306 net.cpp:100] Creating Layer Convolution27
I0905 01:04:50.606968 73306 net.cpp:434] Convolution27 <- BatchNorm26
I0905 01:04:50.606981 73306 net.cpp:408] Convolution27 -> Convolution27
I0905 01:04:50.610824 73306 net.cpp:150] Setting up Convolution27
I0905 01:04:50.610855 73306 net.cpp:157] Top shape: 16 304 32 32 (4980736)
I0905 01:04:50.610877 73306 net.cpp:165] Memory required for data: 3064725696
I0905 01:04:50.610900 73306 layer_factory.hpp:77] Creating layer Dropout27
I0905 01:04:50.610914 73306 net.cpp:100] Creating Layer Dropout27
I0905 01:04:50.610924 73306 net.cpp:434] Dropout27 <- Convolution27
I0905 01:04:50.610945 73306 net.cpp:408] Dropout27 -> Dropout27
I0905 01:04:50.610999 73306 net.cpp:150] Setting up Dropout27
I0905 01:04:50.611029 73306 net.cpp:157] Top shape: 16 304 32 32 (4980736)
I0905 01:04:50.611037 73306 net.cpp:165] Memory required for data: 3084648640
I0905 01:04:50.611045 73306 layer_factory.hpp:77] Creating layer Pooling2
I0905 01:04:50.611068 73306 net.cpp:100] Creating Layer Pooling2
I0905 01:04:50.611075 73306 net.cpp:434] Pooling2 <- Dropout27
I0905 01:04:50.611086 73306 net.cpp:408] Pooling2 -> Pooling2
I0905 01:04:50.611500 73306 net.cpp:150] Setting up Pooling2
I0905 01:04:50.611529 73306 net.cpp:157] Top shape: 16 304 16 16 (1245184)
I0905 01:04:50.611538 73306 net.cpp:165] Memory required for data: 3089629376
I0905 01:04:50.611546 73306 layer_factory.hpp:77] Creating layer Pooling2_Pooling2_0_split
I0905 01:04:50.611557 73306 net.cpp:100] Creating Layer Pooling2_Pooling2_0_split
I0905 01:04:50.611564 73306 net.cpp:434] Pooling2_Pooling2_0_split <- Pooling2
I0905 01:04:50.611577 73306 net.cpp:408] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_0
I0905 01:04:50.611588 73306 net.cpp:408] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_1
I0905 01:04:50.611644 73306 net.cpp:150] Setting up Pooling2_Pooling2_0_split
I0905 01:04:50.611668 73306 net.cpp:157] Top shape: 16 304 16 16 (1245184)
I0905 01:04:50.611678 73306 net.cpp:157] Top shape: 16 304 16 16 (1245184)
I0905 01:04:50.611686 73306 net.cpp:165] Memory required for data: 3099590848
I0905 01:04:50.611695 73306 layer_factory.hpp:77] Creating layer BatchNorm27
I0905 01:04:50.611708 73306 net.cpp:100] Creating Layer BatchNorm27
I0905 01:04:50.611717 73306 net.cpp:434] BatchNorm27 <- Pooling2_Pooling2_0_split_0
I0905 01:04:50.611728 73306 net.cpp:408] BatchNorm27 -> BatchNorm27
I0905 01:04:50.611996 73306 net.cpp:150] Setting up BatchNorm27
I0905 01:04:50.612022 73306 net.cpp:157] Top shape: 16 304 16 16 (1245184)
I0905 01:04:50.612030 73306 net.cpp:165] Memory required for data: 3104571584
I0905 01:04:50.612042 73306 layer_factory.hpp:77] Creating layer Scale27
I0905 01:04:50.612054 73306 net.cpp:100] Creating Layer Scale27
I0905 01:04:50.612062 73306 net.cpp:434] Scale27 <- BatchNorm27
I0905 01:04:50.612083 73306 net.cpp:395] Scale27 -> BatchNorm27 (in-place)
I0905 01:04:50.612244 73306 net.cpp:150] Setting up Scale27
I0905 01:04:50.612258 73306 net.cpp:157] Top shape: 16 304 16 16 (1245184)
I0905 01:04:50.612277 73306 net.cpp:165] Memory required for data: 3109552320
I0905 01:04:50.612287 73306 layer_factory.hpp:77] Creating layer ReLU27
I0905 01:04:50.612296 73306 net.cpp:100] Creating Layer ReLU27
I0905 01:04:50.612304 73306 net.cpp:434] ReLU27 <- BatchNorm27
I0905 01:04:50.612313 73306 net.cpp:395] ReLU27 -> BatchNorm27 (in-place)
I0905 01:04:50.612565 73306 net.cpp:150] Setting up ReLU27
I0905 01:04:50.612592 73306 net.cpp:157] Top shape: 16 304 16 16 (1245184)
I0905 01:04:50.612601 73306 net.cpp:165] Memory required for data: 3114533056
I0905 01:04:50.612608 73306 layer_factory.hpp:77] Creating layer Convolution28
I0905 01:04:50.612623 73306 net.cpp:100] Creating Layer Convolution28
I0905 01:04:50.612632 73306 net.cpp:434] Convolution28 <- BatchNorm27
I0905 01:04:50.612658 73306 net.cpp:408] Convolution28 -> Convolution28
I0905 01:04:50.614789 73306 net.cpp:150] Setting up Convolution28
I0905 01:04:50.614820 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.614827 73306 net.cpp:165] Memory required for data: 3114729664
I0905 01:04:50.614838 73306 layer_factory.hpp:77] Creating layer Dropout28
I0905 01:04:50.614851 73306 net.cpp:100] Creating Layer Dropout28
I0905 01:04:50.614873 73306 net.cpp:434] Dropout28 <- Convolution28
I0905 01:04:50.614884 73306 net.cpp:408] Dropout28 -> Dropout28
I0905 01:04:50.614940 73306 net.cpp:150] Setting up Dropout28
I0905 01:04:50.614979 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.615001 73306 net.cpp:165] Memory required for data: 3114926272
I0905 01:04:50.615023 73306 layer_factory.hpp:77] Creating layer Concat25
I0905 01:04:50.615046 73306 net.cpp:100] Creating Layer Concat25
I0905 01:04:50.615053 73306 net.cpp:434] Concat25 <- Pooling2_Pooling2_0_split_1
I0905 01:04:50.615062 73306 net.cpp:434] Concat25 <- Dropout28
I0905 01:04:50.615073 73306 net.cpp:408] Concat25 -> Concat25
I0905 01:04:50.615116 73306 net.cpp:150] Setting up Concat25
I0905 01:04:50.615128 73306 net.cpp:157] Top shape: 16 316 16 16 (1294336)
I0905 01:04:50.615136 73306 net.cpp:165] Memory required for data: 3120103616
I0905 01:04:50.615144 73306 layer_factory.hpp:77] Creating layer Concat25_Concat25_0_split
I0905 01:04:50.615169 73306 net.cpp:100] Creating Layer Concat25_Concat25_0_split
I0905 01:04:50.615177 73306 net.cpp:434] Concat25_Concat25_0_split <- Concat25
I0905 01:04:50.615187 73306 net.cpp:408] Concat25_Concat25_0_split -> Concat25_Concat25_0_split_0
I0905 01:04:50.615198 73306 net.cpp:408] Concat25_Concat25_0_split -> Concat25_Concat25_0_split_1
I0905 01:04:50.615242 73306 net.cpp:150] Setting up Concat25_Concat25_0_split
I0905 01:04:50.615255 73306 net.cpp:157] Top shape: 16 316 16 16 (1294336)
I0905 01:04:50.615264 73306 net.cpp:157] Top shape: 16 316 16 16 (1294336)
I0905 01:04:50.615272 73306 net.cpp:165] Memory required for data: 3130458304
I0905 01:04:50.615280 73306 layer_factory.hpp:77] Creating layer BatchNorm28
I0905 01:04:50.615290 73306 net.cpp:100] Creating Layer BatchNorm28
I0905 01:04:50.615310 73306 net.cpp:434] BatchNorm28 <- Concat25_Concat25_0_split_0
I0905 01:04:50.615336 73306 net.cpp:408] BatchNorm28 -> BatchNorm28
I0905 01:04:50.615599 73306 net.cpp:150] Setting up BatchNorm28
I0905 01:04:50.615623 73306 net.cpp:157] Top shape: 16 316 16 16 (1294336)
I0905 01:04:50.615631 73306 net.cpp:165] Memory required for data: 3135635648
I0905 01:04:50.615645 73306 layer_factory.hpp:77] Creating layer Scale28
I0905 01:04:50.615656 73306 net.cpp:100] Creating Layer Scale28
I0905 01:04:50.615664 73306 net.cpp:434] Scale28 <- BatchNorm28
I0905 01:04:50.615674 73306 net.cpp:395] Scale28 -> BatchNorm28 (in-place)
I0905 01:04:50.615838 73306 net.cpp:150] Setting up Scale28
I0905 01:04:50.615864 73306 net.cpp:157] Top shape: 16 316 16 16 (1294336)
I0905 01:04:50.615871 73306 net.cpp:165] Memory required for data: 3140812992
I0905 01:04:50.615892 73306 layer_factory.hpp:77] Creating layer ReLU28
I0905 01:04:50.615902 73306 net.cpp:100] Creating Layer ReLU28
I0905 01:04:50.615909 73306 net.cpp:434] ReLU28 <- BatchNorm28
I0905 01:04:50.615921 73306 net.cpp:395] ReLU28 -> BatchNorm28 (in-place)
I0905 01:04:50.616333 73306 net.cpp:150] Setting up ReLU28
I0905 01:04:50.616363 73306 net.cpp:157] Top shape: 16 316 16 16 (1294336)
I0905 01:04:50.616370 73306 net.cpp:165] Memory required for data: 3145990336
I0905 01:04:50.616379 73306 layer_factory.hpp:77] Creating layer Convolution29
I0905 01:04:50.616394 73306 net.cpp:100] Creating Layer Convolution29
I0905 01:04:50.616402 73306 net.cpp:434] Convolution29 <- BatchNorm28
I0905 01:04:50.616415 73306 net.cpp:408] Convolution29 -> Convolution29
I0905 01:04:50.618568 73306 net.cpp:150] Setting up Convolution29
I0905 01:04:50.618599 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.618608 73306 net.cpp:165] Memory required for data: 3146186944
I0905 01:04:50.618619 73306 layer_factory.hpp:77] Creating layer Dropout29
I0905 01:04:50.618635 73306 net.cpp:100] Creating Layer Dropout29
I0905 01:04:50.618657 73306 net.cpp:434] Dropout29 <- Convolution29
I0905 01:04:50.618670 73306 net.cpp:408] Dropout29 -> Dropout29
I0905 01:04:50.618729 73306 net.cpp:150] Setting up Dropout29
I0905 01:04:50.618743 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.618752 73306 net.cpp:165] Memory required for data: 3146383552
I0905 01:04:50.618759 73306 layer_factory.hpp:77] Creating layer Concat26
I0905 01:04:50.618770 73306 net.cpp:100] Creating Layer Concat26
I0905 01:04:50.618793 73306 net.cpp:434] Concat26 <- Concat25_Concat25_0_split_1
I0905 01:04:50.618806 73306 net.cpp:434] Concat26 <- Dropout29
I0905 01:04:50.618818 73306 net.cpp:408] Concat26 -> Concat26
I0905 01:04:50.618852 73306 net.cpp:150] Setting up Concat26
I0905 01:04:50.618865 73306 net.cpp:157] Top shape: 16 328 16 16 (1343488)
I0905 01:04:50.618873 73306 net.cpp:165] Memory required for data: 3151757504
I0905 01:04:50.618881 73306 layer_factory.hpp:77] Creating layer Concat26_Concat26_0_split
I0905 01:04:50.618904 73306 net.cpp:100] Creating Layer Concat26_Concat26_0_split
I0905 01:04:50.618912 73306 net.cpp:434] Concat26_Concat26_0_split <- Concat26
I0905 01:04:50.618924 73306 net.cpp:408] Concat26_Concat26_0_split -> Concat26_Concat26_0_split_0
I0905 01:04:50.618937 73306 net.cpp:408] Concat26_Concat26_0_split -> Concat26_Concat26_0_split_1
I0905 01:04:50.618989 73306 net.cpp:150] Setting up Concat26_Concat26_0_split
I0905 01:04:50.619002 73306 net.cpp:157] Top shape: 16 328 16 16 (1343488)
I0905 01:04:50.619011 73306 net.cpp:157] Top shape: 16 328 16 16 (1343488)
I0905 01:04:50.619019 73306 net.cpp:165] Memory required for data: 3162505408
I0905 01:04:50.619027 73306 layer_factory.hpp:77] Creating layer BatchNorm29
I0905 01:04:50.619040 73306 net.cpp:100] Creating Layer BatchNorm29
I0905 01:04:50.619048 73306 net.cpp:434] BatchNorm29 <- Concat26_Concat26_0_split_0
I0905 01:04:50.619061 73306 net.cpp:408] BatchNorm29 -> BatchNorm29
I0905 01:04:50.619349 73306 net.cpp:150] Setting up BatchNorm29
I0905 01:04:50.619364 73306 net.cpp:157] Top shape: 16 328 16 16 (1343488)
I0905 01:04:50.619384 73306 net.cpp:165] Memory required for data: 3167879360
I0905 01:04:50.619396 73306 layer_factory.hpp:77] Creating layer Scale29
I0905 01:04:50.619407 73306 net.cpp:100] Creating Layer Scale29
I0905 01:04:50.619416 73306 net.cpp:434] Scale29 <- BatchNorm29
I0905 01:04:50.619427 73306 net.cpp:395] Scale29 -> BatchNorm29 (in-place)
I0905 01:04:50.619556 73306 net.cpp:150] Setting up Scale29
I0905 01:04:50.619570 73306 net.cpp:157] Top shape: 16 328 16 16 (1343488)
I0905 01:04:50.619590 73306 net.cpp:165] Memory required for data: 3173253312
I0905 01:04:50.619598 73306 layer_factory.hpp:77] Creating layer ReLU29
I0905 01:04:50.619609 73306 net.cpp:100] Creating Layer ReLU29
I0905 01:04:50.619617 73306 net.cpp:434] ReLU29 <- BatchNorm29
I0905 01:04:50.619626 73306 net.cpp:395] ReLU29 -> BatchNorm29 (in-place)
I0905 01:04:50.619871 73306 net.cpp:150] Setting up ReLU29
I0905 01:04:50.619887 73306 net.cpp:157] Top shape: 16 328 16 16 (1343488)
I0905 01:04:50.619906 73306 net.cpp:165] Memory required for data: 3178627264
I0905 01:04:50.619915 73306 layer_factory.hpp:77] Creating layer Convolution30
I0905 01:04:50.619930 73306 net.cpp:100] Creating Layer Convolution30
I0905 01:04:50.619938 73306 net.cpp:434] Convolution30 <- BatchNorm29
I0905 01:04:50.619951 73306 net.cpp:408] Convolution30 -> Convolution30
I0905 01:04:50.622318 73306 net.cpp:150] Setting up Convolution30
I0905 01:04:50.622349 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.622359 73306 net.cpp:165] Memory required for data: 3178823872
I0905 01:04:50.622370 73306 layer_factory.hpp:77] Creating layer Dropout30
I0905 01:04:50.622381 73306 net.cpp:100] Creating Layer Dropout30
I0905 01:04:50.622401 73306 net.cpp:434] Dropout30 <- Convolution30
I0905 01:04:50.622414 73306 net.cpp:408] Dropout30 -> Dropout30
I0905 01:04:50.622473 73306 net.cpp:150] Setting up Dropout30
I0905 01:04:50.622488 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.622495 73306 net.cpp:165] Memory required for data: 3179020480
I0905 01:04:50.622503 73306 layer_factory.hpp:77] Creating layer Concat27
I0905 01:04:50.622531 73306 net.cpp:100] Creating Layer Concat27
I0905 01:04:50.622541 73306 net.cpp:434] Concat27 <- Concat26_Concat26_0_split_1
I0905 01:04:50.622550 73306 net.cpp:434] Concat27 <- Dropout30
I0905 01:04:50.622560 73306 net.cpp:408] Concat27 -> Concat27
I0905 01:04:50.622601 73306 net.cpp:150] Setting up Concat27
I0905 01:04:50.622634 73306 net.cpp:157] Top shape: 16 340 16 16 (1392640)
I0905 01:04:50.622645 73306 net.cpp:165] Memory required for data: 3184591040
I0905 01:04:50.622653 73306 layer_factory.hpp:77] Creating layer Concat27_Concat27_0_split
I0905 01:04:50.622663 73306 net.cpp:100] Creating Layer Concat27_Concat27_0_split
I0905 01:04:50.622673 73306 net.cpp:434] Concat27_Concat27_0_split <- Concat27
I0905 01:04:50.622685 73306 net.cpp:408] Concat27_Concat27_0_split -> Concat27_Concat27_0_split_0
I0905 01:04:50.622697 73306 net.cpp:408] Concat27_Concat27_0_split -> Concat27_Concat27_0_split_1
I0905 01:04:50.622743 73306 net.cpp:150] Setting up Concat27_Concat27_0_split
I0905 01:04:50.622757 73306 net.cpp:157] Top shape: 16 340 16 16 (1392640)
I0905 01:04:50.622768 73306 net.cpp:157] Top shape: 16 340 16 16 (1392640)
I0905 01:04:50.622776 73306 net.cpp:165] Memory required for data: 3195732160
I0905 01:04:50.622784 73306 layer_factory.hpp:77] Creating layer BatchNorm30
I0905 01:04:50.622807 73306 net.cpp:100] Creating Layer BatchNorm30
I0905 01:04:50.622828 73306 net.cpp:434] BatchNorm30 <- Concat27_Concat27_0_split_0
I0905 01:04:50.622843 73306 net.cpp:408] BatchNorm30 -> BatchNorm30
I0905 01:04:50.623109 73306 net.cpp:150] Setting up BatchNorm30
I0905 01:04:50.623136 73306 net.cpp:157] Top shape: 16 340 16 16 (1392640)
I0905 01:04:50.623144 73306 net.cpp:165] Memory required for data: 3201302720
I0905 01:04:50.623157 73306 layer_factory.hpp:77] Creating layer Scale30
I0905 01:04:50.623170 73306 net.cpp:100] Creating Layer Scale30
I0905 01:04:50.623179 73306 net.cpp:434] Scale30 <- BatchNorm30
I0905 01:04:50.623189 73306 net.cpp:395] Scale30 -> BatchNorm30 (in-place)
I0905 01:04:50.623328 73306 net.cpp:150] Setting up Scale30
I0905 01:04:50.623342 73306 net.cpp:157] Top shape: 16 340 16 16 (1392640)
I0905 01:04:50.623363 73306 net.cpp:165] Memory required for data: 3206873280
I0905 01:04:50.623373 73306 layer_factory.hpp:77] Creating layer ReLU30
I0905 01:04:50.623386 73306 net.cpp:100] Creating Layer ReLU30
I0905 01:04:50.623395 73306 net.cpp:434] ReLU30 <- BatchNorm30
I0905 01:04:50.623405 73306 net.cpp:395] ReLU30 -> BatchNorm30 (in-place)
I0905 01:04:50.623631 73306 net.cpp:150] Setting up ReLU30
I0905 01:04:50.623657 73306 net.cpp:157] Top shape: 16 340 16 16 (1392640)
I0905 01:04:50.623666 73306 net.cpp:165] Memory required for data: 3212443840
I0905 01:04:50.623673 73306 layer_factory.hpp:77] Creating layer Convolution31
I0905 01:04:50.623689 73306 net.cpp:100] Creating Layer Convolution31
I0905 01:04:50.623697 73306 net.cpp:434] Convolution31 <- BatchNorm30
I0905 01:04:50.623710 73306 net.cpp:408] Convolution31 -> Convolution31
I0905 01:04:50.626009 73306 net.cpp:150] Setting up Convolution31
I0905 01:04:50.626041 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.626050 73306 net.cpp:165] Memory required for data: 3212640448
I0905 01:04:50.626062 73306 layer_factory.hpp:77] Creating layer Dropout31
I0905 01:04:50.626075 73306 net.cpp:100] Creating Layer Dropout31
I0905 01:04:50.626085 73306 net.cpp:434] Dropout31 <- Convolution31
I0905 01:04:50.626096 73306 net.cpp:408] Dropout31 -> Dropout31
I0905 01:04:50.626152 73306 net.cpp:150] Setting up Dropout31
I0905 01:04:50.626166 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.626174 73306 net.cpp:165] Memory required for data: 3212837056
I0905 01:04:50.626183 73306 layer_factory.hpp:77] Creating layer Concat28
I0905 01:04:50.626195 73306 net.cpp:100] Creating Layer Concat28
I0905 01:04:50.626204 73306 net.cpp:434] Concat28 <- Concat27_Concat27_0_split_1
I0905 01:04:50.626214 73306 net.cpp:434] Concat28 <- Dropout31
I0905 01:04:50.626227 73306 net.cpp:408] Concat28 -> Concat28
I0905 01:04:50.626257 73306 net.cpp:150] Setting up Concat28
I0905 01:04:50.626269 73306 net.cpp:157] Top shape: 16 352 16 16 (1441792)
I0905 01:04:50.626278 73306 net.cpp:165] Memory required for data: 3218604224
I0905 01:04:50.626286 73306 layer_factory.hpp:77] Creating layer Concat28_Concat28_0_split
I0905 01:04:50.626298 73306 net.cpp:100] Creating Layer Concat28_Concat28_0_split
I0905 01:04:50.626320 73306 net.cpp:434] Concat28_Concat28_0_split <- Concat28
I0905 01:04:50.626332 73306 net.cpp:408] Concat28_Concat28_0_split -> Concat28_Concat28_0_split_0
I0905 01:04:50.626344 73306 net.cpp:408] Concat28_Concat28_0_split -> Concat28_Concat28_0_split_1
I0905 01:04:50.626391 73306 net.cpp:150] Setting up Concat28_Concat28_0_split
I0905 01:04:50.626405 73306 net.cpp:157] Top shape: 16 352 16 16 (1441792)
I0905 01:04:50.626415 73306 net.cpp:157] Top shape: 16 352 16 16 (1441792)
I0905 01:04:50.626421 73306 net.cpp:165] Memory required for data: 3230138560
I0905 01:04:50.626430 73306 layer_factory.hpp:77] Creating layer BatchNorm31
I0905 01:04:50.626442 73306 net.cpp:100] Creating Layer BatchNorm31
I0905 01:04:50.626451 73306 net.cpp:434] BatchNorm31 <- Concat28_Concat28_0_split_0
I0905 01:04:50.626462 73306 net.cpp:408] BatchNorm31 -> BatchNorm31
I0905 01:04:50.626720 73306 net.cpp:150] Setting up BatchNorm31
I0905 01:04:50.626734 73306 net.cpp:157] Top shape: 16 352 16 16 (1441792)
I0905 01:04:50.626755 73306 net.cpp:165] Memory required for data: 3235905728
I0905 01:04:50.626768 73306 layer_factory.hpp:77] Creating layer Scale31
I0905 01:04:50.626780 73306 net.cpp:100] Creating Layer Scale31
I0905 01:04:50.626788 73306 net.cpp:434] Scale31 <- BatchNorm31
I0905 01:04:50.626802 73306 net.cpp:395] Scale31 -> BatchNorm31 (in-place)
I0905 01:04:50.626932 73306 net.cpp:150] Setting up Scale31
I0905 01:04:50.626946 73306 net.cpp:157] Top shape: 16 352 16 16 (1441792)
I0905 01:04:50.626966 73306 net.cpp:165] Memory required for data: 3241672896
I0905 01:04:50.626976 73306 layer_factory.hpp:77] Creating layer ReLU31
I0905 01:04:50.626987 73306 net.cpp:100] Creating Layer ReLU31
I0905 01:04:50.626996 73306 net.cpp:434] ReLU31 <- BatchNorm31
I0905 01:04:50.627007 73306 net.cpp:395] ReLU31 -> BatchNorm31 (in-place)
I0905 01:04:50.627384 73306 net.cpp:150] Setting up ReLU31
I0905 01:04:50.627416 73306 net.cpp:157] Top shape: 16 352 16 16 (1441792)
I0905 01:04:50.627425 73306 net.cpp:165] Memory required for data: 3247440064
I0905 01:04:50.627435 73306 layer_factory.hpp:77] Creating layer Convolution32
I0905 01:04:50.627454 73306 net.cpp:100] Creating Layer Convolution32
I0905 01:04:50.627463 73306 net.cpp:434] Convolution32 <- BatchNorm31
I0905 01:04:50.627475 73306 net.cpp:408] Convolution32 -> Convolution32
I0905 01:04:50.631675 73306 net.cpp:150] Setting up Convolution32
I0905 01:04:50.631708 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.631717 73306 net.cpp:165] Memory required for data: 3247636672
I0905 01:04:50.631729 73306 layer_factory.hpp:77] Creating layer Dropout32
I0905 01:04:50.631741 73306 net.cpp:100] Creating Layer Dropout32
I0905 01:04:50.631749 73306 net.cpp:434] Dropout32 <- Convolution32
I0905 01:04:50.631759 73306 net.cpp:408] Dropout32 -> Dropout32
I0905 01:04:50.631819 73306 net.cpp:150] Setting up Dropout32
I0905 01:04:50.631834 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.631841 73306 net.cpp:165] Memory required for data: 3247833280
I0905 01:04:50.631850 73306 layer_factory.hpp:77] Creating layer Concat29
I0905 01:04:50.631860 73306 net.cpp:100] Creating Layer Concat29
I0905 01:04:50.631870 73306 net.cpp:434] Concat29 <- Concat28_Concat28_0_split_1
I0905 01:04:50.631880 73306 net.cpp:434] Concat29 <- Dropout32
I0905 01:04:50.631892 73306 net.cpp:408] Concat29 -> Concat29
I0905 01:04:50.631928 73306 net.cpp:150] Setting up Concat29
I0905 01:04:50.631942 73306 net.cpp:157] Top shape: 16 364 16 16 (1490944)
I0905 01:04:50.631949 73306 net.cpp:165] Memory required for data: 3253797056
I0905 01:04:50.631958 73306 layer_factory.hpp:77] Creating layer Concat29_Concat29_0_split
I0905 01:04:50.631968 73306 net.cpp:100] Creating Layer Concat29_Concat29_0_split
I0905 01:04:50.631976 73306 net.cpp:434] Concat29_Concat29_0_split <- Concat29
I0905 01:04:50.631988 73306 net.cpp:408] Concat29_Concat29_0_split -> Concat29_Concat29_0_split_0
I0905 01:04:50.631999 73306 net.cpp:408] Concat29_Concat29_0_split -> Concat29_Concat29_0_split_1
I0905 01:04:50.632060 73306 net.cpp:150] Setting up Concat29_Concat29_0_split
I0905 01:04:50.632073 73306 net.cpp:157] Top shape: 16 364 16 16 (1490944)
I0905 01:04:50.632083 73306 net.cpp:157] Top shape: 16 364 16 16 (1490944)
I0905 01:04:50.632091 73306 net.cpp:165] Memory required for data: 3265724608
I0905 01:04:50.632099 73306 layer_factory.hpp:77] Creating layer BatchNorm32
I0905 01:04:50.632113 73306 net.cpp:100] Creating Layer BatchNorm32
I0905 01:04:50.632122 73306 net.cpp:434] BatchNorm32 <- Concat29_Concat29_0_split_0
I0905 01:04:50.632135 73306 net.cpp:408] BatchNorm32 -> BatchNorm32
I0905 01:04:50.632395 73306 net.cpp:150] Setting up BatchNorm32
I0905 01:04:50.632410 73306 net.cpp:157] Top shape: 16 364 16 16 (1490944)
I0905 01:04:50.632429 73306 net.cpp:165] Memory required for data: 3271688384
I0905 01:04:50.632441 73306 layer_factory.hpp:77] Creating layer Scale32
I0905 01:04:50.632452 73306 net.cpp:100] Creating Layer Scale32
I0905 01:04:50.632462 73306 net.cpp:434] Scale32 <- BatchNorm32
I0905 01:04:50.632472 73306 net.cpp:395] Scale32 -> BatchNorm32 (in-place)
I0905 01:04:50.632596 73306 net.cpp:150] Setting up Scale32
I0905 01:04:50.632622 73306 net.cpp:157] Top shape: 16 364 16 16 (1490944)
I0905 01:04:50.632630 73306 net.cpp:165] Memory required for data: 3277652160
I0905 01:04:50.632640 73306 layer_factory.hpp:77] Creating layer ReLU32
I0905 01:04:50.632650 73306 net.cpp:100] Creating Layer ReLU32
I0905 01:04:50.632658 73306 net.cpp:434] ReLU32 <- BatchNorm32
I0905 01:04:50.632668 73306 net.cpp:395] ReLU32 -> BatchNorm32 (in-place)
I0905 01:04:50.632894 73306 net.cpp:150] Setting up ReLU32
I0905 01:04:50.632920 73306 net.cpp:157] Top shape: 16 364 16 16 (1490944)
I0905 01:04:50.632927 73306 net.cpp:165] Memory required for data: 3283615936
I0905 01:04:50.632936 73306 layer_factory.hpp:77] Creating layer Convolution33
I0905 01:04:50.632951 73306 net.cpp:100] Creating Layer Convolution33
I0905 01:04:50.632958 73306 net.cpp:434] Convolution33 <- BatchNorm32
I0905 01:04:50.632972 73306 net.cpp:408] Convolution33 -> Convolution33
I0905 01:04:50.638422 73306 net.cpp:150] Setting up Convolution33
I0905 01:04:50.638454 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.638463 73306 net.cpp:165] Memory required for data: 3283812544
I0905 01:04:50.638474 73306 layer_factory.hpp:77] Creating layer Dropout33
I0905 01:04:50.638485 73306 net.cpp:100] Creating Layer Dropout33
I0905 01:04:50.638494 73306 net.cpp:434] Dropout33 <- Convolution33
I0905 01:04:50.638504 73306 net.cpp:408] Dropout33 -> Dropout33
I0905 01:04:50.638561 73306 net.cpp:150] Setting up Dropout33
I0905 01:04:50.638574 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.638582 73306 net.cpp:165] Memory required for data: 3284009152
I0905 01:04:50.638592 73306 layer_factory.hpp:77] Creating layer Concat30
I0905 01:04:50.638602 73306 net.cpp:100] Creating Layer Concat30
I0905 01:04:50.638609 73306 net.cpp:434] Concat30 <- Concat29_Concat29_0_split_1
I0905 01:04:50.638618 73306 net.cpp:434] Concat30 <- Dropout33
I0905 01:04:50.638633 73306 net.cpp:408] Concat30 -> Concat30
I0905 01:04:50.638667 73306 net.cpp:150] Setting up Concat30
I0905 01:04:50.638680 73306 net.cpp:157] Top shape: 16 376 16 16 (1540096)
I0905 01:04:50.638691 73306 net.cpp:165] Memory required for data: 3290169536
I0905 01:04:50.638700 73306 layer_factory.hpp:77] Creating layer Concat30_Concat30_0_split
I0905 01:04:50.638710 73306 net.cpp:100] Creating Layer Concat30_Concat30_0_split
I0905 01:04:50.638718 73306 net.cpp:434] Concat30_Concat30_0_split <- Concat30
I0905 01:04:50.638728 73306 net.cpp:408] Concat30_Concat30_0_split -> Concat30_Concat30_0_split_0
I0905 01:04:50.638738 73306 net.cpp:408] Concat30_Concat30_0_split -> Concat30_Concat30_0_split_1
I0905 01:04:50.638787 73306 net.cpp:150] Setting up Concat30_Concat30_0_split
I0905 01:04:50.638798 73306 net.cpp:157] Top shape: 16 376 16 16 (1540096)
I0905 01:04:50.638808 73306 net.cpp:157] Top shape: 16 376 16 16 (1540096)
I0905 01:04:50.638814 73306 net.cpp:165] Memory required for data: 3302490304
I0905 01:04:50.638835 73306 layer_factory.hpp:77] Creating layer BatchNorm33
I0905 01:04:50.638849 73306 net.cpp:100] Creating Layer BatchNorm33
I0905 01:04:50.638856 73306 net.cpp:434] BatchNorm33 <- Concat30_Concat30_0_split_0
I0905 01:04:50.638866 73306 net.cpp:408] BatchNorm33 -> BatchNorm33
I0905 01:04:50.639147 73306 net.cpp:150] Setting up BatchNorm33
I0905 01:04:50.639173 73306 net.cpp:157] Top shape: 16 376 16 16 (1540096)
I0905 01:04:50.639183 73306 net.cpp:165] Memory required for data: 3308650688
I0905 01:04:50.639194 73306 layer_factory.hpp:77] Creating layer Scale33
I0905 01:04:50.639205 73306 net.cpp:100] Creating Layer Scale33
I0905 01:04:50.639214 73306 net.cpp:434] Scale33 <- BatchNorm33
I0905 01:04:50.639225 73306 net.cpp:395] Scale33 -> BatchNorm33 (in-place)
I0905 01:04:50.639348 73306 net.cpp:150] Setting up Scale33
I0905 01:04:50.639374 73306 net.cpp:157] Top shape: 16 376 16 16 (1540096)
I0905 01:04:50.639381 73306 net.cpp:165] Memory required for data: 3314811072
I0905 01:04:50.639391 73306 layer_factory.hpp:77] Creating layer ReLU33
I0905 01:04:50.639405 73306 net.cpp:100] Creating Layer ReLU33
I0905 01:04:50.639412 73306 net.cpp:434] ReLU33 <- BatchNorm33
I0905 01:04:50.639421 73306 net.cpp:395] ReLU33 -> BatchNorm33 (in-place)
I0905 01:04:50.641264 73306 net.cpp:150] Setting up ReLU33
I0905 01:04:50.641293 73306 net.cpp:157] Top shape: 16 376 16 16 (1540096)
I0905 01:04:50.641301 73306 net.cpp:165] Memory required for data: 3320971456
I0905 01:04:50.641310 73306 layer_factory.hpp:77] Creating layer Convolution34
I0905 01:04:50.641325 73306 net.cpp:100] Creating Layer Convolution34
I0905 01:04:50.641335 73306 net.cpp:434] Convolution34 <- BatchNorm33
I0905 01:04:50.641345 73306 net.cpp:408] Convolution34 -> Convolution34
I0905 01:04:50.643699 73306 net.cpp:150] Setting up Convolution34
I0905 01:04:50.643731 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.643740 73306 net.cpp:165] Memory required for data: 3321168064
I0905 01:04:50.643751 73306 layer_factory.hpp:77] Creating layer Dropout34
I0905 01:04:50.643764 73306 net.cpp:100] Creating Layer Dropout34
I0905 01:04:50.643774 73306 net.cpp:434] Dropout34 <- Convolution34
I0905 01:04:50.643784 73306 net.cpp:408] Dropout34 -> Dropout34
I0905 01:04:50.643843 73306 net.cpp:150] Setting up Dropout34
I0905 01:04:50.643857 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.643865 73306 net.cpp:165] Memory required for data: 3321364672
I0905 01:04:50.643873 73306 layer_factory.hpp:77] Creating layer Concat31
I0905 01:04:50.643885 73306 net.cpp:100] Creating Layer Concat31
I0905 01:04:50.643894 73306 net.cpp:434] Concat31 <- Concat30_Concat30_0_split_1
I0905 01:04:50.643904 73306 net.cpp:434] Concat31 <- Dropout34
I0905 01:04:50.643914 73306 net.cpp:408] Concat31 -> Concat31
I0905 01:04:50.643945 73306 net.cpp:150] Setting up Concat31
I0905 01:04:50.643959 73306 net.cpp:157] Top shape: 16 388 16 16 (1589248)
I0905 01:04:50.643966 73306 net.cpp:165] Memory required for data: 3327721664
I0905 01:04:50.643975 73306 layer_factory.hpp:77] Creating layer Concat31_Concat31_0_split
I0905 01:04:50.643985 73306 net.cpp:100] Creating Layer Concat31_Concat31_0_split
I0905 01:04:50.643992 73306 net.cpp:434] Concat31_Concat31_0_split <- Concat31
I0905 01:04:50.644004 73306 net.cpp:408] Concat31_Concat31_0_split -> Concat31_Concat31_0_split_0
I0905 01:04:50.644016 73306 net.cpp:408] Concat31_Concat31_0_split -> Concat31_Concat31_0_split_1
I0905 01:04:50.644057 73306 net.cpp:150] Setting up Concat31_Concat31_0_split
I0905 01:04:50.644071 73306 net.cpp:157] Top shape: 16 388 16 16 (1589248)
I0905 01:04:50.644081 73306 net.cpp:157] Top shape: 16 388 16 16 (1589248)
I0905 01:04:50.644088 73306 net.cpp:165] Memory required for data: 3340435648
I0905 01:04:50.644098 73306 layer_factory.hpp:77] Creating layer BatchNorm34
I0905 01:04:50.644109 73306 net.cpp:100] Creating Layer BatchNorm34
I0905 01:04:50.644116 73306 net.cpp:434] BatchNorm34 <- Concat31_Concat31_0_split_0
I0905 01:04:50.644129 73306 net.cpp:408] BatchNorm34 -> BatchNorm34
I0905 01:04:50.644369 73306 net.cpp:150] Setting up BatchNorm34
I0905 01:04:50.644407 73306 net.cpp:157] Top shape: 16 388 16 16 (1589248)
I0905 01:04:50.644415 73306 net.cpp:165] Memory required for data: 3346792640
I0905 01:04:50.644428 73306 layer_factory.hpp:77] Creating layer Scale34
I0905 01:04:50.644441 73306 net.cpp:100] Creating Layer Scale34
I0905 01:04:50.644450 73306 net.cpp:434] Scale34 <- BatchNorm34
I0905 01:04:50.644460 73306 net.cpp:395] Scale34 -> BatchNorm34 (in-place)
I0905 01:04:50.644588 73306 net.cpp:150] Setting up Scale34
I0905 01:04:50.644614 73306 net.cpp:157] Top shape: 16 388 16 16 (1589248)
I0905 01:04:50.644623 73306 net.cpp:165] Memory required for data: 3353149632
I0905 01:04:50.644632 73306 layer_factory.hpp:77] Creating layer ReLU34
I0905 01:04:50.644642 73306 net.cpp:100] Creating Layer ReLU34
I0905 01:04:50.644650 73306 net.cpp:434] ReLU34 <- BatchNorm34
I0905 01:04:50.644661 73306 net.cpp:395] ReLU34 -> BatchNorm34 (in-place)
I0905 01:04:50.645057 73306 net.cpp:150] Setting up ReLU34
I0905 01:04:50.645089 73306 net.cpp:157] Top shape: 16 388 16 16 (1589248)
I0905 01:04:50.645098 73306 net.cpp:165] Memory required for data: 3359506624
I0905 01:04:50.645107 73306 layer_factory.hpp:77] Creating layer Convolution35
I0905 01:04:50.645123 73306 net.cpp:100] Creating Layer Convolution35
I0905 01:04:50.645131 73306 net.cpp:434] Convolution35 <- BatchNorm34
I0905 01:04:50.645145 73306 net.cpp:408] Convolution35 -> Convolution35
I0905 01:04:50.647610 73306 net.cpp:150] Setting up Convolution35
I0905 01:04:50.647642 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.647652 73306 net.cpp:165] Memory required for data: 3359703232
I0905 01:04:50.647663 73306 layer_factory.hpp:77] Creating layer Dropout35
I0905 01:04:50.647675 73306 net.cpp:100] Creating Layer Dropout35
I0905 01:04:50.647683 73306 net.cpp:434] Dropout35 <- Convolution35
I0905 01:04:50.647693 73306 net.cpp:408] Dropout35 -> Dropout35
I0905 01:04:50.647754 73306 net.cpp:150] Setting up Dropout35
I0905 01:04:50.647768 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.647776 73306 net.cpp:165] Memory required for data: 3359899840
I0905 01:04:50.647784 73306 layer_factory.hpp:77] Creating layer Concat32
I0905 01:04:50.647794 73306 net.cpp:100] Creating Layer Concat32
I0905 01:04:50.647804 73306 net.cpp:434] Concat32 <- Concat31_Concat31_0_split_1
I0905 01:04:50.647825 73306 net.cpp:434] Concat32 <- Dropout35
I0905 01:04:50.647853 73306 net.cpp:408] Concat32 -> Concat32
I0905 01:04:50.647887 73306 net.cpp:150] Setting up Concat32
I0905 01:04:50.647900 73306 net.cpp:157] Top shape: 16 400 16 16 (1638400)
I0905 01:04:50.647908 73306 net.cpp:165] Memory required for data: 3366453440
I0905 01:04:50.647917 73306 layer_factory.hpp:77] Creating layer Concat32_Concat32_0_split
I0905 01:04:50.647925 73306 net.cpp:100] Creating Layer Concat32_Concat32_0_split
I0905 01:04:50.647934 73306 net.cpp:434] Concat32_Concat32_0_split <- Concat32
I0905 01:04:50.647945 73306 net.cpp:408] Concat32_Concat32_0_split -> Concat32_Concat32_0_split_0
I0905 01:04:50.647955 73306 net.cpp:408] Concat32_Concat32_0_split -> Concat32_Concat32_0_split_1
I0905 01:04:50.648001 73306 net.cpp:150] Setting up Concat32_Concat32_0_split
I0905 01:04:50.648015 73306 net.cpp:157] Top shape: 16 400 16 16 (1638400)
I0905 01:04:50.648023 73306 net.cpp:157] Top shape: 16 400 16 16 (1638400)
I0905 01:04:50.648030 73306 net.cpp:165] Memory required for data: 3379560640
I0905 01:04:50.648038 73306 layer_factory.hpp:77] Creating layer BatchNorm35
I0905 01:04:50.648051 73306 net.cpp:100] Creating Layer BatchNorm35
I0905 01:04:50.648061 73306 net.cpp:434] BatchNorm35 <- Concat32_Concat32_0_split_0
I0905 01:04:50.648072 73306 net.cpp:408] BatchNorm35 -> BatchNorm35
I0905 01:04:50.648326 73306 net.cpp:150] Setting up BatchNorm35
I0905 01:04:50.648340 73306 net.cpp:157] Top shape: 16 400 16 16 (1638400)
I0905 01:04:50.648361 73306 net.cpp:165] Memory required for data: 3386114240
I0905 01:04:50.648375 73306 layer_factory.hpp:77] Creating layer Scale35
I0905 01:04:50.648386 73306 net.cpp:100] Creating Layer Scale35
I0905 01:04:50.648408 73306 net.cpp:434] Scale35 <- BatchNorm35
I0905 01:04:50.648419 73306 net.cpp:395] Scale35 -> BatchNorm35 (in-place)
I0905 01:04:50.648550 73306 net.cpp:150] Setting up Scale35
I0905 01:04:50.648579 73306 net.cpp:157] Top shape: 16 400 16 16 (1638400)
I0905 01:04:50.648587 73306 net.cpp:165] Memory required for data: 3392667840
I0905 01:04:50.648598 73306 layer_factory.hpp:77] Creating layer ReLU35
I0905 01:04:50.648609 73306 net.cpp:100] Creating Layer ReLU35
I0905 01:04:50.648617 73306 net.cpp:434] ReLU35 <- BatchNorm35
I0905 01:04:50.648627 73306 net.cpp:395] ReLU35 -> BatchNorm35 (in-place)
I0905 01:04:50.648869 73306 net.cpp:150] Setting up ReLU35
I0905 01:04:50.648896 73306 net.cpp:157] Top shape: 16 400 16 16 (1638400)
I0905 01:04:50.648905 73306 net.cpp:165] Memory required for data: 3399221440
I0905 01:04:50.648912 73306 layer_factory.hpp:77] Creating layer Convolution36
I0905 01:04:50.648927 73306 net.cpp:100] Creating Layer Convolution36
I0905 01:04:50.648936 73306 net.cpp:434] Convolution36 <- BatchNorm35
I0905 01:04:50.648952 73306 net.cpp:408] Convolution36 -> Convolution36
I0905 01:04:50.651599 73306 net.cpp:150] Setting up Convolution36
I0905 01:04:50.651633 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.651643 73306 net.cpp:165] Memory required for data: 3399418048
I0905 01:04:50.651654 73306 layer_factory.hpp:77] Creating layer Dropout36
I0905 01:04:50.651665 73306 net.cpp:100] Creating Layer Dropout36
I0905 01:04:50.651674 73306 net.cpp:434] Dropout36 <- Convolution36
I0905 01:04:50.651687 73306 net.cpp:408] Dropout36 -> Dropout36
I0905 01:04:50.651733 73306 net.cpp:150] Setting up Dropout36
I0905 01:04:50.651749 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.651758 73306 net.cpp:165] Memory required for data: 3399614656
I0905 01:04:50.651765 73306 layer_factory.hpp:77] Creating layer Concat33
I0905 01:04:50.651778 73306 net.cpp:100] Creating Layer Concat33
I0905 01:04:50.651787 73306 net.cpp:434] Concat33 <- Concat32_Concat32_0_split_1
I0905 01:04:50.651798 73306 net.cpp:434] Concat33 <- Dropout36
I0905 01:04:50.651808 73306 net.cpp:408] Concat33 -> Concat33
I0905 01:04:50.651842 73306 net.cpp:150] Setting up Concat33
I0905 01:04:50.651856 73306 net.cpp:157] Top shape: 16 412 16 16 (1687552)
I0905 01:04:50.651865 73306 net.cpp:165] Memory required for data: 3406364864
I0905 01:04:50.651872 73306 layer_factory.hpp:77] Creating layer Concat33_Concat33_0_split
I0905 01:04:50.651882 73306 net.cpp:100] Creating Layer Concat33_Concat33_0_split
I0905 01:04:50.651891 73306 net.cpp:434] Concat33_Concat33_0_split <- Concat33
I0905 01:04:50.651904 73306 net.cpp:408] Concat33_Concat33_0_split -> Concat33_Concat33_0_split_0
I0905 01:04:50.651916 73306 net.cpp:408] Concat33_Concat33_0_split -> Concat33_Concat33_0_split_1
I0905 01:04:50.651962 73306 net.cpp:150] Setting up Concat33_Concat33_0_split
I0905 01:04:50.651974 73306 net.cpp:157] Top shape: 16 412 16 16 (1687552)
I0905 01:04:50.651983 73306 net.cpp:157] Top shape: 16 412 16 16 (1687552)
I0905 01:04:50.651991 73306 net.cpp:165] Memory required for data: 3419865280
I0905 01:04:50.651999 73306 layer_factory.hpp:77] Creating layer BatchNorm36
I0905 01:04:50.652011 73306 net.cpp:100] Creating Layer BatchNorm36
I0905 01:04:50.652020 73306 net.cpp:434] BatchNorm36 <- Concat33_Concat33_0_split_0
I0905 01:04:50.652032 73306 net.cpp:408] BatchNorm36 -> BatchNorm36
I0905 01:04:50.652278 73306 net.cpp:150] Setting up BatchNorm36
I0905 01:04:50.652293 73306 net.cpp:157] Top shape: 16 412 16 16 (1687552)
I0905 01:04:50.652313 73306 net.cpp:165] Memory required for data: 3426615488
I0905 01:04:50.652325 73306 layer_factory.hpp:77] Creating layer Scale36
I0905 01:04:50.652338 73306 net.cpp:100] Creating Layer Scale36
I0905 01:04:50.652346 73306 net.cpp:434] Scale36 <- BatchNorm36
I0905 01:04:50.652359 73306 net.cpp:395] Scale36 -> BatchNorm36 (in-place)
I0905 01:04:50.652489 73306 net.cpp:150] Setting up Scale36
I0905 01:04:50.652503 73306 net.cpp:157] Top shape: 16 412 16 16 (1687552)
I0905 01:04:50.652535 73306 net.cpp:165] Memory required for data: 3433365696
I0905 01:04:50.652546 73306 layer_factory.hpp:77] Creating layer ReLU36
I0905 01:04:50.652557 73306 net.cpp:100] Creating Layer ReLU36
I0905 01:04:50.652565 73306 net.cpp:434] ReLU36 <- BatchNorm36
I0905 01:04:50.652575 73306 net.cpp:395] ReLU36 -> BatchNorm36 (in-place)
I0905 01:04:50.652791 73306 net.cpp:150] Setting up ReLU36
I0905 01:04:50.652822 73306 net.cpp:157] Top shape: 16 412 16 16 (1687552)
I0905 01:04:50.652829 73306 net.cpp:165] Memory required for data: 3440115904
I0905 01:04:50.652837 73306 layer_factory.hpp:77] Creating layer Convolution37
I0905 01:04:50.652854 73306 net.cpp:100] Creating Layer Convolution37
I0905 01:04:50.652863 73306 net.cpp:434] Convolution37 <- BatchNorm36
I0905 01:04:50.652874 73306 net.cpp:408] Convolution37 -> Convolution37
I0905 01:04:50.655432 73306 net.cpp:150] Setting up Convolution37
I0905 01:04:50.655454 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.655463 73306 net.cpp:165] Memory required for data: 3440312512
I0905 01:04:50.655474 73306 layer_factory.hpp:77] Creating layer Dropout37
I0905 01:04:50.655488 73306 net.cpp:100] Creating Layer Dropout37
I0905 01:04:50.655498 73306 net.cpp:434] Dropout37 <- Convolution37
I0905 01:04:50.655508 73306 net.cpp:408] Dropout37 -> Dropout37
I0905 01:04:50.655553 73306 net.cpp:150] Setting up Dropout37
I0905 01:04:50.655565 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.655573 73306 net.cpp:165] Memory required for data: 3440509120
I0905 01:04:50.655581 73306 layer_factory.hpp:77] Creating layer Concat34
I0905 01:04:50.655593 73306 net.cpp:100] Creating Layer Concat34
I0905 01:04:50.655602 73306 net.cpp:434] Concat34 <- Concat33_Concat33_0_split_1
I0905 01:04:50.655611 73306 net.cpp:434] Concat34 <- Dropout37
I0905 01:04:50.655621 73306 net.cpp:408] Concat34 -> Concat34
I0905 01:04:50.655652 73306 net.cpp:150] Setting up Concat34
I0905 01:04:50.655663 73306 net.cpp:157] Top shape: 16 424 16 16 (1736704)
I0905 01:04:50.655671 73306 net.cpp:165] Memory required for data: 3447455936
I0905 01:04:50.655678 73306 layer_factory.hpp:77] Creating layer Concat34_Concat34_0_split
I0905 01:04:50.655689 73306 net.cpp:100] Creating Layer Concat34_Concat34_0_split
I0905 01:04:50.655696 73306 net.cpp:434] Concat34_Concat34_0_split <- Concat34
I0905 01:04:50.655709 73306 net.cpp:408] Concat34_Concat34_0_split -> Concat34_Concat34_0_split_0
I0905 01:04:50.655719 73306 net.cpp:408] Concat34_Concat34_0_split -> Concat34_Concat34_0_split_1
I0905 01:04:50.655771 73306 net.cpp:150] Setting up Concat34_Concat34_0_split
I0905 01:04:50.655782 73306 net.cpp:157] Top shape: 16 424 16 16 (1736704)
I0905 01:04:50.655791 73306 net.cpp:157] Top shape: 16 424 16 16 (1736704)
I0905 01:04:50.655798 73306 net.cpp:165] Memory required for data: 3461349568
I0905 01:04:50.655805 73306 layer_factory.hpp:77] Creating layer BatchNorm37
I0905 01:04:50.655818 73306 net.cpp:100] Creating Layer BatchNorm37
I0905 01:04:50.655827 73306 net.cpp:434] BatchNorm37 <- Concat34_Concat34_0_split_0
I0905 01:04:50.655839 73306 net.cpp:408] BatchNorm37 -> BatchNorm37
I0905 01:04:50.656078 73306 net.cpp:150] Setting up BatchNorm37
I0905 01:04:50.656092 73306 net.cpp:157] Top shape: 16 424 16 16 (1736704)
I0905 01:04:50.656100 73306 net.cpp:165] Memory required for data: 3468296384
I0905 01:04:50.656112 73306 layer_factory.hpp:77] Creating layer Scale37
I0905 01:04:50.656152 73306 net.cpp:100] Creating Layer Scale37
I0905 01:04:50.656162 73306 net.cpp:434] Scale37 <- BatchNorm37
I0905 01:04:50.656173 73306 net.cpp:395] Scale37 -> BatchNorm37 (in-place)
I0905 01:04:50.656294 73306 net.cpp:150] Setting up Scale37
I0905 01:04:50.656308 73306 net.cpp:157] Top shape: 16 424 16 16 (1736704)
I0905 01:04:50.656316 73306 net.cpp:165] Memory required for data: 3475243200
I0905 01:04:50.656325 73306 layer_factory.hpp:77] Creating layer ReLU37
I0905 01:04:50.656337 73306 net.cpp:100] Creating Layer ReLU37
I0905 01:04:50.656344 73306 net.cpp:434] ReLU37 <- BatchNorm37
I0905 01:04:50.656355 73306 net.cpp:395] ReLU37 -> BatchNorm37 (in-place)
I0905 01:04:50.656734 73306 net.cpp:150] Setting up ReLU37
I0905 01:04:50.656754 73306 net.cpp:157] Top shape: 16 424 16 16 (1736704)
I0905 01:04:50.656764 73306 net.cpp:165] Memory required for data: 3482190016
I0905 01:04:50.656772 73306 layer_factory.hpp:77] Creating layer Convolution38
I0905 01:04:50.656788 73306 net.cpp:100] Creating Layer Convolution38
I0905 01:04:50.656797 73306 net.cpp:434] Convolution38 <- BatchNorm37
I0905 01:04:50.656811 73306 net.cpp:408] Convolution38 -> Convolution38
I0905 01:04:50.660004 73306 net.cpp:150] Setting up Convolution38
I0905 01:04:50.660025 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.660034 73306 net.cpp:165] Memory required for data: 3482386624
I0905 01:04:50.660045 73306 layer_factory.hpp:77] Creating layer Dropout38
I0905 01:04:50.660058 73306 net.cpp:100] Creating Layer Dropout38
I0905 01:04:50.660068 73306 net.cpp:434] Dropout38 <- Convolution38
I0905 01:04:50.660079 73306 net.cpp:408] Dropout38 -> Dropout38
I0905 01:04:50.660125 73306 net.cpp:150] Setting up Dropout38
I0905 01:04:50.660136 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.660145 73306 net.cpp:165] Memory required for data: 3482583232
I0905 01:04:50.660152 73306 layer_factory.hpp:77] Creating layer Concat35
I0905 01:04:50.660162 73306 net.cpp:100] Creating Layer Concat35
I0905 01:04:50.660171 73306 net.cpp:434] Concat35 <- Concat34_Concat34_0_split_1
I0905 01:04:50.660181 73306 net.cpp:434] Concat35 <- Dropout38
I0905 01:04:50.660192 73306 net.cpp:408] Concat35 -> Concat35
I0905 01:04:50.660221 73306 net.cpp:150] Setting up Concat35
I0905 01:04:50.660233 73306 net.cpp:157] Top shape: 16 436 16 16 (1785856)
I0905 01:04:50.660239 73306 net.cpp:165] Memory required for data: 3489726656
I0905 01:04:50.660246 73306 layer_factory.hpp:77] Creating layer Concat35_Concat35_0_split
I0905 01:04:50.660259 73306 net.cpp:100] Creating Layer Concat35_Concat35_0_split
I0905 01:04:50.660266 73306 net.cpp:434] Concat35_Concat35_0_split <- Concat35
I0905 01:04:50.660275 73306 net.cpp:408] Concat35_Concat35_0_split -> Concat35_Concat35_0_split_0
I0905 01:04:50.660286 73306 net.cpp:408] Concat35_Concat35_0_split -> Concat35_Concat35_0_split_1
I0905 01:04:50.660328 73306 net.cpp:150] Setting up Concat35_Concat35_0_split
I0905 01:04:50.660339 73306 net.cpp:157] Top shape: 16 436 16 16 (1785856)
I0905 01:04:50.660348 73306 net.cpp:157] Top shape: 16 436 16 16 (1785856)
I0905 01:04:50.660356 73306 net.cpp:165] Memory required for data: 3504013504
I0905 01:04:50.660362 73306 layer_factory.hpp:77] Creating layer BatchNorm38
I0905 01:04:50.660375 73306 net.cpp:100] Creating Layer BatchNorm38
I0905 01:04:50.660383 73306 net.cpp:434] BatchNorm38 <- Concat35_Concat35_0_split_0
I0905 01:04:50.660393 73306 net.cpp:408] BatchNorm38 -> BatchNorm38
I0905 01:04:50.660625 73306 net.cpp:150] Setting up BatchNorm38
I0905 01:04:50.660640 73306 net.cpp:157] Top shape: 16 436 16 16 (1785856)
I0905 01:04:50.660647 73306 net.cpp:165] Memory required for data: 3511156928
I0905 01:04:50.660658 73306 layer_factory.hpp:77] Creating layer Scale38
I0905 01:04:50.660671 73306 net.cpp:100] Creating Layer Scale38
I0905 01:04:50.660681 73306 net.cpp:434] Scale38 <- BatchNorm38
I0905 01:04:50.660689 73306 net.cpp:395] Scale38 -> BatchNorm38 (in-place)
I0905 01:04:50.660805 73306 net.cpp:150] Setting up Scale38
I0905 01:04:50.660820 73306 net.cpp:157] Top shape: 16 436 16 16 (1785856)
I0905 01:04:50.660826 73306 net.cpp:165] Memory required for data: 3518300352
I0905 01:04:50.660835 73306 layer_factory.hpp:77] Creating layer ReLU38
I0905 01:04:50.660845 73306 net.cpp:100] Creating Layer ReLU38
I0905 01:04:50.660853 73306 net.cpp:434] ReLU38 <- BatchNorm38
I0905 01:04:50.660863 73306 net.cpp:395] ReLU38 -> BatchNorm38 (in-place)
I0905 01:04:50.661064 73306 net.cpp:150] Setting up ReLU38
I0905 01:04:50.661080 73306 net.cpp:157] Top shape: 16 436 16 16 (1785856)
I0905 01:04:50.661088 73306 net.cpp:165] Memory required for data: 3525443776
I0905 01:04:50.661095 73306 layer_factory.hpp:77] Creating layer Convolution39
I0905 01:04:50.661123 73306 net.cpp:100] Creating Layer Convolution39
I0905 01:04:50.661133 73306 net.cpp:434] Convolution39 <- BatchNorm38
I0905 01:04:50.661146 73306 net.cpp:408] Convolution39 -> Convolution39
I0905 01:04:50.663761 73306 net.cpp:150] Setting up Convolution39
I0905 01:04:50.663782 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.663790 73306 net.cpp:165] Memory required for data: 3525640384
I0905 01:04:50.663801 73306 layer_factory.hpp:77] Creating layer Dropout39
I0905 01:04:50.663812 73306 net.cpp:100] Creating Layer Dropout39
I0905 01:04:50.663821 73306 net.cpp:434] Dropout39 <- Convolution39
I0905 01:04:50.663833 73306 net.cpp:408] Dropout39 -> Dropout39
I0905 01:04:50.663878 73306 net.cpp:150] Setting up Dropout39
I0905 01:04:50.663889 73306 net.cpp:157] Top shape: 16 12 16 16 (49152)
I0905 01:04:50.663897 73306 net.cpp:165] Memory required for data: 3525836992
I0905 01:04:50.663904 73306 layer_factory.hpp:77] Creating layer Concat36
I0905 01:04:50.663918 73306 net.cpp:100] Creating Layer Concat36
I0905 01:04:50.663926 73306 net.cpp:434] Concat36 <- Concat35_Concat35_0_split_1
I0905 01:04:50.663934 73306 net.cpp:434] Concat36 <- Dropout39
I0905 01:04:50.663944 73306 net.cpp:408] Concat36 -> Concat36
I0905 01:04:50.663975 73306 net.cpp:150] Setting up Concat36
I0905 01:04:50.663986 73306 net.cpp:157] Top shape: 16 448 16 16 (1835008)
I0905 01:04:50.663993 73306 net.cpp:165] Memory required for data: 3533177024
I0905 01:04:50.664000 73306 layer_factory.hpp:77] Creating layer BatchNorm39
I0905 01:04:50.664012 73306 net.cpp:100] Creating Layer BatchNorm39
I0905 01:04:50.664021 73306 net.cpp:434] BatchNorm39 <- Concat36
I0905 01:04:50.664031 73306 net.cpp:408] BatchNorm39 -> BatchNorm39
I0905 01:04:50.664299 73306 net.cpp:150] Setting up BatchNorm39
I0905 01:04:50.664314 73306 net.cpp:157] Top shape: 16 448 16 16 (1835008)
I0905 01:04:50.664322 73306 net.cpp:165] Memory required for data: 3540517056
I0905 01:04:50.664333 73306 layer_factory.hpp:77] Creating layer Scale39
I0905 01:04:50.664345 73306 net.cpp:100] Creating Layer Scale39
I0905 01:04:50.664353 73306 net.cpp:434] Scale39 <- BatchNorm39
I0905 01:04:50.664361 73306 net.cpp:395] Scale39 -> BatchNorm39 (in-place)
I0905 01:04:50.664474 73306 net.cpp:150] Setting up Scale39
I0905 01:04:50.664489 73306 net.cpp:157] Top shape: 16 448 16 16 (1835008)
I0905 01:04:50.664496 73306 net.cpp:165] Memory required for data: 3547857088
I0905 01:04:50.664505 73306 layer_factory.hpp:77] Creating layer ReLU39
I0905 01:04:50.664517 73306 net.cpp:100] Creating Layer ReLU39
I0905 01:04:50.664525 73306 net.cpp:434] ReLU39 <- BatchNorm39
I0905 01:04:50.664533 73306 net.cpp:395] ReLU39 -> BatchNorm39 (in-place)
I0905 01:04:50.664741 73306 net.cpp:150] Setting up ReLU39
I0905 01:04:50.664757 73306 net.cpp:157] Top shape: 16 448 16 16 (1835008)
I0905 01:04:50.664764 73306 net.cpp:165] Memory required for data: 3555197120
I0905 01:04:50.664772 73306 layer_factory.hpp:77] Creating layer Pooling3
I0905 01:04:50.664783 73306 net.cpp:100] Creating Layer Pooling3
I0905 01:04:50.664791 73306 net.cpp:434] Pooling3 <- BatchNorm39
I0905 01:04:50.664803 73306 net.cpp:408] Pooling3 -> Pooling3
I0905 01:04:50.665166 73306 net.cpp:150] Setting up Pooling3
I0905 01:04:50.665185 73306 net.cpp:157] Top shape: 16 448 1 1 (7168)
I0905 01:04:50.665194 73306 net.cpp:165] Memory required for data: 3555225792
I0905 01:04:50.665202 73306 layer_factory.hpp:77] Creating layer InnerProduct1
I0905 01:04:50.665215 73306 net.cpp:100] Creating Layer InnerProduct1
I0905 01:04:50.665222 73306 net.cpp:434] InnerProduct1 <- Pooling3
I0905 01:04:50.665235 73306 net.cpp:408] InnerProduct1 -> InnerProduct1
I0905 01:04:50.665379 73306 net.cpp:150] Setting up InnerProduct1
I0905 01:04:50.665392 73306 net.cpp:157] Top shape: 16 2 (32)
I0905 01:04:50.665400 73306 net.cpp:165] Memory required for data: 3555225920
I0905 01:04:50.665410 73306 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I0905 01:04:50.665421 73306 net.cpp:100] Creating Layer InnerProduct1_InnerProduct1_0_split
I0905 01:04:50.665441 73306 net.cpp:434] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I0905 01:04:50.665454 73306 net.cpp:408] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I0905 01:04:50.665467 73306 net.cpp:408] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I0905 01:04:50.665513 73306 net.cpp:150] Setting up InnerProduct1_InnerProduct1_0_split
I0905 01:04:50.665526 73306 net.cpp:157] Top shape: 16 2 (32)
I0905 01:04:50.665534 73306 net.cpp:157] Top shape: 16 2 (32)
I0905 01:04:50.665541 73306 net.cpp:165] Memory required for data: 3555226176
I0905 01:04:50.665549 73306 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0905 01:04:50.665558 73306 net.cpp:100] Creating Layer SoftmaxWithLoss1
I0905 01:04:50.665567 73306 net.cpp:434] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I0905 01:04:50.665576 73306 net.cpp:434] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I0905 01:04:50.665585 73306 net.cpp:408] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0905 01:04:50.665597 73306 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0905 01:04:50.665887 73306 net.cpp:150] Setting up SoftmaxWithLoss1
I0905 01:04:50.665904 73306 net.cpp:157] Top shape: (1)
I0905 01:04:50.665911 73306 net.cpp:160]     with loss weight 1
I0905 01:04:50.665927 73306 net.cpp:165] Memory required for data: 3555226180
I0905 01:04:50.665935 73306 layer_factory.hpp:77] Creating layer Accuracy1
I0905 01:04:50.665956 73306 net.cpp:100] Creating Layer Accuracy1
I0905 01:04:50.665964 73306 net.cpp:434] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I0905 01:04:50.665973 73306 net.cpp:434] Accuracy1 <- Data2_Data1_1_split_1
I0905 01:04:50.665983 73306 net.cpp:408] Accuracy1 -> Accuracy1
I0905 01:04:50.665999 73306 net.cpp:150] Setting up Accuracy1
I0905 01:04:50.666008 73306 net.cpp:157] Top shape: (1)
I0905 01:04:50.666015 73306 net.cpp:165] Memory required for data: 3555226184
I0905 01:04:50.666023 73306 net.cpp:228] Accuracy1 does not need backward computation.
I0905 01:04:50.666031 73306 net.cpp:226] SoftmaxWithLoss1 needs backward computation.
I0905 01:04:50.666040 73306 net.cpp:226] InnerProduct1_InnerProduct1_0_split needs backward computation.
I0905 01:04:50.666048 73306 net.cpp:226] InnerProduct1 needs backward computation.
I0905 01:04:50.666055 73306 net.cpp:226] Pooling3 needs backward computation.
I0905 01:04:50.666062 73306 net.cpp:226] ReLU39 needs backward computation.
I0905 01:04:50.666070 73306 net.cpp:226] Scale39 needs backward computation.
I0905 01:04:50.666077 73306 net.cpp:226] BatchNorm39 needs backward computation.
I0905 01:04:50.666085 73306 net.cpp:226] Concat36 needs backward computation.
I0905 01:04:50.666092 73306 net.cpp:226] Dropout39 needs backward computation.
I0905 01:04:50.666100 73306 net.cpp:226] Convolution39 needs backward computation.
I0905 01:04:50.666107 73306 net.cpp:226] ReLU38 needs backward computation.
I0905 01:04:50.666115 73306 net.cpp:226] Scale38 needs backward computation.
I0905 01:04:50.666122 73306 net.cpp:226] BatchNorm38 needs backward computation.
I0905 01:04:50.666131 73306 net.cpp:226] Concat35_Concat35_0_split needs backward computation.
I0905 01:04:50.666137 73306 net.cpp:226] Concat35 needs backward computation.
I0905 01:04:50.666146 73306 net.cpp:226] Dropout38 needs backward computation.
I0905 01:04:50.666153 73306 net.cpp:226] Convolution38 needs backward computation.
I0905 01:04:50.666160 73306 net.cpp:226] ReLU37 needs backward computation.
I0905 01:04:50.666167 73306 net.cpp:226] Scale37 needs backward computation.
I0905 01:04:50.666174 73306 net.cpp:226] BatchNorm37 needs backward computation.
I0905 01:04:50.666182 73306 net.cpp:226] Concat34_Concat34_0_split needs backward computation.
I0905 01:04:50.666189 73306 net.cpp:226] Concat34 needs backward computation.
I0905 01:04:50.666198 73306 net.cpp:226] Dropout37 needs backward computation.
I0905 01:04:50.666204 73306 net.cpp:226] Convolution37 needs backward computation.
I0905 01:04:50.666211 73306 net.cpp:226] ReLU36 needs backward computation.
I0905 01:04:50.666230 73306 net.cpp:226] Scale36 needs backward computation.
I0905 01:04:50.666239 73306 net.cpp:226] BatchNorm36 needs backward computation.
I0905 01:04:50.666245 73306 net.cpp:226] Concat33_Concat33_0_split needs backward computation.
I0905 01:04:50.666254 73306 net.cpp:226] Concat33 needs backward computation.
I0905 01:04:50.666260 73306 net.cpp:226] Dropout36 needs backward computation.
I0905 01:04:50.666268 73306 net.cpp:226] Convolution36 needs backward computation.
I0905 01:04:50.666275 73306 net.cpp:226] ReLU35 needs backward computation.
I0905 01:04:50.666283 73306 net.cpp:226] Scale35 needs backward computation.
I0905 01:04:50.666290 73306 net.cpp:226] BatchNorm35 needs backward computation.
I0905 01:04:50.666299 73306 net.cpp:226] Concat32_Concat32_0_split needs backward computation.
I0905 01:04:50.666309 73306 net.cpp:226] Concat32 needs backward computation.
I0905 01:04:50.666317 73306 net.cpp:226] Dropout35 needs backward computation.
I0905 01:04:50.666324 73306 net.cpp:226] Convolution35 needs backward computation.
I0905 01:04:50.666332 73306 net.cpp:226] ReLU34 needs backward computation.
I0905 01:04:50.666339 73306 net.cpp:226] Scale34 needs backward computation.
I0905 01:04:50.666347 73306 net.cpp:226] BatchNorm34 needs backward computation.
I0905 01:04:50.666353 73306 net.cpp:226] Concat31_Concat31_0_split needs backward computation.
I0905 01:04:50.666360 73306 net.cpp:226] Concat31 needs backward computation.
I0905 01:04:50.666368 73306 net.cpp:226] Dropout34 needs backward computation.
I0905 01:04:50.666375 73306 net.cpp:226] Convolution34 needs backward computation.
I0905 01:04:50.666383 73306 net.cpp:226] ReLU33 needs backward computation.
I0905 01:04:50.666390 73306 net.cpp:226] Scale33 needs backward computation.
I0905 01:04:50.666398 73306 net.cpp:226] BatchNorm33 needs backward computation.
I0905 01:04:50.666404 73306 net.cpp:226] Concat30_Concat30_0_split needs backward computation.
I0905 01:04:50.666412 73306 net.cpp:226] Concat30 needs backward computation.
I0905 01:04:50.666419 73306 net.cpp:226] Dropout33 needs backward computation.
I0905 01:04:50.666426 73306 net.cpp:226] Convolution33 needs backward computation.
I0905 01:04:50.666435 73306 net.cpp:226] ReLU32 needs backward computation.
I0905 01:04:50.666441 73306 net.cpp:226] Scale32 needs backward computation.
I0905 01:04:50.666448 73306 net.cpp:226] BatchNorm32 needs backward computation.
I0905 01:04:50.666455 73306 net.cpp:226] Concat29_Concat29_0_split needs backward computation.
I0905 01:04:50.666463 73306 net.cpp:226] Concat29 needs backward computation.
I0905 01:04:50.666471 73306 net.cpp:226] Dropout32 needs backward computation.
I0905 01:04:50.666478 73306 net.cpp:226] Convolution32 needs backward computation.
I0905 01:04:50.666486 73306 net.cpp:226] ReLU31 needs backward computation.
I0905 01:04:50.666492 73306 net.cpp:226] Scale31 needs backward computation.
I0905 01:04:50.666499 73306 net.cpp:226] BatchNorm31 needs backward computation.
I0905 01:04:50.666507 73306 net.cpp:226] Concat28_Concat28_0_split needs backward computation.
I0905 01:04:50.666514 73306 net.cpp:226] Concat28 needs backward computation.
I0905 01:04:50.666522 73306 net.cpp:226] Dropout31 needs backward computation.
I0905 01:04:50.666529 73306 net.cpp:226] Convolution31 needs backward computation.
I0905 01:04:50.666537 73306 net.cpp:226] ReLU30 needs backward computation.
I0905 01:04:50.666543 73306 net.cpp:226] Scale30 needs backward computation.
I0905 01:04:50.666550 73306 net.cpp:226] BatchNorm30 needs backward computation.
I0905 01:04:50.666558 73306 net.cpp:226] Concat27_Concat27_0_split needs backward computation.
I0905 01:04:50.666565 73306 net.cpp:226] Concat27 needs backward computation.
I0905 01:04:50.666574 73306 net.cpp:226] Dropout30 needs backward computation.
I0905 01:04:50.666581 73306 net.cpp:226] Convolution30 needs backward computation.
I0905 01:04:50.666590 73306 net.cpp:226] ReLU29 needs backward computation.
I0905 01:04:50.666599 73306 net.cpp:226] Scale29 needs backward computation.
I0905 01:04:50.666612 73306 net.cpp:226] BatchNorm29 needs backward computation.
I0905 01:04:50.666620 73306 net.cpp:226] Concat26_Concat26_0_split needs backward computation.
I0905 01:04:50.666633 73306 net.cpp:226] Concat26 needs backward computation.
I0905 01:04:50.666642 73306 net.cpp:226] Dropout29 needs backward computation.
I0905 01:04:50.666651 73306 net.cpp:226] Convolution29 needs backward computation.
I0905 01:04:50.666658 73306 net.cpp:226] ReLU28 needs backward computation.
I0905 01:04:50.666666 73306 net.cpp:226] Scale28 needs backward computation.
I0905 01:04:50.666673 73306 net.cpp:226] BatchNorm28 needs backward computation.
I0905 01:04:50.666681 73306 net.cpp:226] Concat25_Concat25_0_split needs backward computation.
I0905 01:04:50.666688 73306 net.cpp:226] Concat25 needs backward computation.
I0905 01:04:50.666697 73306 net.cpp:226] Dropout28 needs backward computation.
I0905 01:04:50.666703 73306 net.cpp:226] Convolution28 needs backward computation.
I0905 01:04:50.666710 73306 net.cpp:226] ReLU27 needs backward computation.
I0905 01:04:50.666720 73306 net.cpp:226] Scale27 needs backward computation.
I0905 01:04:50.666728 73306 net.cpp:226] BatchNorm27 needs backward computation.
I0905 01:04:50.666735 73306 net.cpp:226] Pooling2_Pooling2_0_split needs backward computation.
I0905 01:04:50.666743 73306 net.cpp:226] Pooling2 needs backward computation.
I0905 01:04:50.666750 73306 net.cpp:226] Dropout27 needs backward computation.
I0905 01:04:50.666759 73306 net.cpp:226] Convolution27 needs backward computation.
I0905 01:04:50.666766 73306 net.cpp:226] ReLU26 needs backward computation.
I0905 01:04:50.666774 73306 net.cpp:226] Scale26 needs backward computation.
I0905 01:04:50.666780 73306 net.cpp:226] BatchNorm26 needs backward computation.
I0905 01:04:50.666788 73306 net.cpp:226] Concat24 needs backward computation.
I0905 01:04:50.666795 73306 net.cpp:226] Dropout26 needs backward computation.
I0905 01:04:50.666803 73306 net.cpp:226] Convolution26 needs backward computation.
I0905 01:04:50.666810 73306 net.cpp:226] ReLU25 needs backward computation.
I0905 01:04:50.666817 73306 net.cpp:226] Scale25 needs backward computation.
I0905 01:04:50.666824 73306 net.cpp:226] BatchNorm25 needs backward computation.
I0905 01:04:50.666832 73306 net.cpp:226] Concat23_Concat23_0_split needs backward computation.
I0905 01:04:50.666839 73306 net.cpp:226] Concat23 needs backward computation.
I0905 01:04:50.666848 73306 net.cpp:226] Dropout25 needs backward computation.
I0905 01:04:50.666856 73306 net.cpp:226] Convolution25 needs backward computation.
I0905 01:04:50.666862 73306 net.cpp:226] ReLU24 needs backward computation.
I0905 01:04:50.666870 73306 net.cpp:226] Scale24 needs backward computation.
I0905 01:04:50.666877 73306 net.cpp:226] BatchNorm24 needs backward computation.
I0905 01:04:50.666884 73306 net.cpp:226] Concat22_Concat22_0_split needs backward computation.
I0905 01:04:50.666893 73306 net.cpp:226] Concat22 needs backward computation.
I0905 01:04:50.666900 73306 net.cpp:226] Dropout24 needs backward computation.
I0905 01:04:50.666908 73306 net.cpp:226] Convolution24 needs backward computation.
I0905 01:04:50.666915 73306 net.cpp:226] ReLU23 needs backward computation.
I0905 01:04:50.666923 73306 net.cpp:226] Scale23 needs backward computation.
I0905 01:04:50.666929 73306 net.cpp:226] BatchNorm23 needs backward computation.
I0905 01:04:50.666937 73306 net.cpp:226] Concat21_Concat21_0_split needs backward computation.
I0905 01:04:50.666945 73306 net.cpp:226] Concat21 needs backward computation.
I0905 01:04:50.666952 73306 net.cpp:226] Dropout23 needs backward computation.
I0905 01:04:50.666960 73306 net.cpp:226] Convolution23 needs backward computation.
I0905 01:04:50.666967 73306 net.cpp:226] ReLU22 needs backward computation.
I0905 01:04:50.666975 73306 net.cpp:226] Scale22 needs backward computation.
I0905 01:04:50.666981 73306 net.cpp:226] BatchNorm22 needs backward computation.
I0905 01:04:50.666990 73306 net.cpp:226] Concat20_Concat20_0_split needs backward computation.
I0905 01:04:50.666996 73306 net.cpp:226] Concat20 needs backward computation.
I0905 01:04:50.667012 73306 net.cpp:226] Dropout22 needs backward computation.
I0905 01:04:50.667021 73306 net.cpp:226] Convolution22 needs backward computation.
I0905 01:04:50.667040 73306 net.cpp:226] ReLU21 needs backward computation.
I0905 01:04:50.667048 73306 net.cpp:226] Scale21 needs backward computation.
I0905 01:04:50.667057 73306 net.cpp:226] BatchNorm21 needs backward computation.
I0905 01:04:50.667064 73306 net.cpp:226] Concat19_Concat19_0_split needs backward computation.
I0905 01:04:50.667073 73306 net.cpp:226] Concat19 needs backward computation.
I0905 01:04:50.667079 73306 net.cpp:226] Dropout21 needs backward computation.
I0905 01:04:50.667089 73306 net.cpp:226] Convolution21 needs backward computation.
I0905 01:04:50.667098 73306 net.cpp:226] ReLU20 needs backward computation.
I0905 01:04:50.667105 73306 net.cpp:226] Scale20 needs backward computation.
I0905 01:04:50.667112 73306 net.cpp:226] BatchNorm20 needs backward computation.
I0905 01:04:50.667120 73306 net.cpp:226] Concat18_Concat18_0_split needs backward computation.
I0905 01:04:50.667129 73306 net.cpp:226] Concat18 needs backward computation.
I0905 01:04:50.667137 73306 net.cpp:226] Dropout20 needs backward computation.
I0905 01:04:50.667145 73306 net.cpp:226] Convolution20 needs backward computation.
I0905 01:04:50.667153 73306 net.cpp:226] ReLU19 needs backward computation.
I0905 01:04:50.667161 73306 net.cpp:226] Scale19 needs backward computation.
I0905 01:04:50.667168 73306 net.cpp:226] BatchNorm19 needs backward computation.
I0905 01:04:50.667177 73306 net.cpp:226] Concat17_Concat17_0_split needs backward computation.
I0905 01:04:50.667186 73306 net.cpp:226] Concat17 needs backward computation.
I0905 01:04:50.667191 73306 net.cpp:226] Dropout19 needs backward computation.
I0905 01:04:50.667199 73306 net.cpp:226] Convolution19 needs backward computation.
I0905 01:04:50.667207 73306 net.cpp:226] ReLU18 needs backward computation.
I0905 01:04:50.667215 73306 net.cpp:226] Scale18 needs backward computation.
I0905 01:04:50.667222 73306 net.cpp:226] BatchNorm18 needs backward computation.
I0905 01:04:50.667230 73306 net.cpp:226] Concat16_Concat16_0_split needs backward computation.
I0905 01:04:50.667238 73306 net.cpp:226] Concat16 needs backward computation.
I0905 01:04:50.667246 73306 net.cpp:226] Dropout18 needs backward computation.
I0905 01:04:50.667254 73306 net.cpp:226] Convolution18 needs backward computation.
I0905 01:04:50.667263 73306 net.cpp:226] ReLU17 needs backward computation.
I0905 01:04:50.667270 73306 net.cpp:226] Scale17 needs backward computation.
I0905 01:04:50.667278 73306 net.cpp:226] BatchNorm17 needs backward computation.
I0905 01:04:50.667285 73306 net.cpp:226] Concat15_Concat15_0_split needs backward computation.
I0905 01:04:50.667294 73306 net.cpp:226] Concat15 needs backward computation.
I0905 01:04:50.667301 73306 net.cpp:226] Dropout17 needs backward computation.
I0905 01:04:50.667309 73306 net.cpp:226] Convolution17 needs backward computation.
I0905 01:04:50.667317 73306 net.cpp:226] ReLU16 needs backward computation.
I0905 01:04:50.667325 73306 net.cpp:226] Scale16 needs backward computation.
I0905 01:04:50.667332 73306 net.cpp:226] BatchNorm16 needs backward computation.
I0905 01:04:50.667340 73306 net.cpp:226] Concat14_Concat14_0_split needs backward computation.
I0905 01:04:50.667347 73306 net.cpp:226] Concat14 needs backward computation.
I0905 01:04:50.667356 73306 net.cpp:226] Dropout16 needs backward computation.
I0905 01:04:50.667364 73306 net.cpp:226] Convolution16 needs backward computation.
I0905 01:04:50.667371 73306 net.cpp:226] ReLU15 needs backward computation.
I0905 01:04:50.667379 73306 net.cpp:226] Scale15 needs backward computation.
I0905 01:04:50.667387 73306 net.cpp:226] BatchNorm15 needs backward computation.
I0905 01:04:50.667394 73306 net.cpp:226] Concat13_Concat13_0_split needs backward computation.
I0905 01:04:50.667402 73306 net.cpp:226] Concat13 needs backward computation.
I0905 01:04:50.667410 73306 net.cpp:226] Dropout15 needs backward computation.
I0905 01:04:50.667428 73306 net.cpp:226] Convolution15 needs backward computation.
I0905 01:04:50.667436 73306 net.cpp:226] ReLU14 needs backward computation.
I0905 01:04:50.667443 73306 net.cpp:226] Scale14 needs backward computation.
I0905 01:04:50.667450 73306 net.cpp:226] BatchNorm14 needs backward computation.
I0905 01:04:50.667459 73306 net.cpp:226] Pooling1_Pooling1_0_split needs backward computation.
I0905 01:04:50.667466 73306 net.cpp:226] Pooling1 needs backward computation.
I0905 01:04:50.667474 73306 net.cpp:226] Dropout14 needs backward computation.
I0905 01:04:50.667482 73306 net.cpp:226] Convolution14 needs backward computation.
I0905 01:04:50.667490 73306 net.cpp:226] ReLU13 needs backward computation.
I0905 01:04:50.667497 73306 net.cpp:226] Scale13 needs backward computation.
I0905 01:04:50.667505 73306 net.cpp:226] BatchNorm13 needs backward computation.
I0905 01:04:50.667512 73306 net.cpp:226] Concat12 needs backward computation.
I0905 01:04:50.667520 73306 net.cpp:226] Dropout13 needs backward computation.
I0905 01:04:50.667528 73306 net.cpp:226] Convolution13 needs backward computation.
I0905 01:04:50.667536 73306 net.cpp:226] ReLU12 needs backward computation.
I0905 01:04:50.667543 73306 net.cpp:226] Scale12 needs backward computation.
I0905 01:04:50.667551 73306 net.cpp:226] BatchNorm12 needs backward computation.
I0905 01:04:50.667559 73306 net.cpp:226] Concat11_Concat11_0_split needs backward computation.
I0905 01:04:50.667567 73306 net.cpp:226] Concat11 needs backward computation.
I0905 01:04:50.667575 73306 net.cpp:226] Dropout12 needs backward computation.
I0905 01:04:50.667583 73306 net.cpp:226] Convolution12 needs backward computation.
I0905 01:04:50.667592 73306 net.cpp:226] ReLU11 needs backward computation.
I0905 01:04:50.667598 73306 net.cpp:226] Scale11 needs backward computation.
I0905 01:04:50.667609 73306 net.cpp:226] BatchNorm11 needs backward computation.
I0905 01:04:50.667618 73306 net.cpp:226] Concat10_Concat10_0_split needs backward computation.
I0905 01:04:50.667626 73306 net.cpp:226] Concat10 needs backward computation.
I0905 01:04:50.667634 73306 net.cpp:226] Dropout11 needs backward computation.
I0905 01:04:50.667642 73306 net.cpp:226] Convolution11 needs backward computation.
I0905 01:04:50.667650 73306 net.cpp:226] ReLU10 needs backward computation.
I0905 01:04:50.667659 73306 net.cpp:226] Scale10 needs backward computation.
I0905 01:04:50.667665 73306 net.cpp:226] BatchNorm10 needs backward computation.
I0905 01:04:50.667673 73306 net.cpp:226] Concat9_Concat9_0_split needs backward computation.
I0905 01:04:50.667681 73306 net.cpp:226] Concat9 needs backward computation.
I0905 01:04:50.667690 73306 net.cpp:226] Dropout10 needs backward computation.
I0905 01:04:50.667698 73306 net.cpp:226] Convolution10 needs backward computation.
I0905 01:04:50.667706 73306 net.cpp:226] ReLU9 needs backward computation.
I0905 01:04:50.667714 73306 net.cpp:226] Scale9 needs backward computation.
I0905 01:04:50.667721 73306 net.cpp:226] BatchNorm9 needs backward computation.
I0905 01:04:50.667729 73306 net.cpp:226] Concat8_Concat8_0_split needs backward computation.
I0905 01:04:50.667738 73306 net.cpp:226] Concat8 needs backward computation.
I0905 01:04:50.667747 73306 net.cpp:226] Dropout9 needs backward computation.
I0905 01:04:50.667754 73306 net.cpp:226] Convolution9 needs backward computation.
I0905 01:04:50.667762 73306 net.cpp:226] ReLU8 needs backward computation.
I0905 01:04:50.667770 73306 net.cpp:226] Scale8 needs backward computation.
I0905 01:04:50.667778 73306 net.cpp:226] BatchNorm8 needs backward computation.
I0905 01:04:50.667785 73306 net.cpp:226] Concat7_Concat7_0_split needs backward computation.
I0905 01:04:50.667794 73306 net.cpp:226] Concat7 needs backward computation.
I0905 01:04:50.667803 73306 net.cpp:226] Dropout8 needs backward computation.
I0905 01:04:50.667810 73306 net.cpp:226] Convolution8 needs backward computation.
I0905 01:04:50.667819 73306 net.cpp:226] ReLU7 needs backward computation.
I0905 01:04:50.667826 73306 net.cpp:226] Scale7 needs backward computation.
I0905 01:04:50.667840 73306 net.cpp:226] BatchNorm7 needs backward computation.
I0905 01:04:50.667850 73306 net.cpp:226] Concat6_Concat6_0_split needs backward computation.
I0905 01:04:50.667857 73306 net.cpp:226] Concat6 needs backward computation.
I0905 01:04:50.667866 73306 net.cpp:226] Dropout7 needs backward computation.
I0905 01:04:50.667875 73306 net.cpp:226] Convolution7 needs backward computation.
I0905 01:04:50.667882 73306 net.cpp:226] ReLU6 needs backward computation.
I0905 01:04:50.667889 73306 net.cpp:226] Scale6 needs backward computation.
I0905 01:04:50.667897 73306 net.cpp:226] BatchNorm6 needs backward computation.
I0905 01:04:50.667906 73306 net.cpp:226] Concat5_Concat5_0_split needs backward computation.
I0905 01:04:50.667917 73306 net.cpp:226] Concat5 needs backward computation.
I0905 01:04:50.667924 73306 net.cpp:226] Dropout6 needs backward computation.
I0905 01:04:50.667932 73306 net.cpp:226] Convolution6 needs backward computation.
I0905 01:04:50.667945 73306 net.cpp:226] ReLU5 needs backward computation.
I0905 01:04:50.667953 73306 net.cpp:226] Scale5 needs backward computation.
I0905 01:04:50.667960 73306 net.cpp:226] BatchNorm5 needs backward computation.
I0905 01:04:50.667969 73306 net.cpp:226] Concat4_Concat4_0_split needs backward computation.
I0905 01:04:50.667976 73306 net.cpp:226] Concat4 needs backward computation.
I0905 01:04:50.667985 73306 net.cpp:226] Dropout5 needs backward computation.
I0905 01:04:50.667994 73306 net.cpp:226] Convolution5 needs backward computation.
I0905 01:04:50.668001 73306 net.cpp:226] ReLU4 needs backward computation.
I0905 01:04:50.668009 73306 net.cpp:226] Scale4 needs backward computation.
I0905 01:04:50.668017 73306 net.cpp:226] BatchNorm4 needs backward computation.
I0905 01:04:50.668025 73306 net.cpp:226] Concat3_Concat3_0_split needs backward computation.
I0905 01:04:50.668035 73306 net.cpp:226] Concat3 needs backward computation.
I0905 01:04:50.668043 73306 net.cpp:226] Dropout4 needs backward computation.
I0905 01:04:50.668051 73306 net.cpp:226] Convolution4 needs backward computation.
I0905 01:04:50.668059 73306 net.cpp:226] ReLU3 needs backward computation.
I0905 01:04:50.668067 73306 net.cpp:226] Scale3 needs backward computation.
I0905 01:04:50.668076 73306 net.cpp:226] BatchNorm3 needs backward computation.
I0905 01:04:50.668083 73306 net.cpp:226] Concat2_Concat2_0_split needs backward computation.
I0905 01:04:50.668092 73306 net.cpp:226] Concat2 needs backward computation.
I0905 01:04:50.668099 73306 net.cpp:226] Dropout3 needs backward computation.
I0905 01:04:50.668107 73306 net.cpp:226] Convolution3 needs backward computation.
I0905 01:04:50.668115 73306 net.cpp:226] ReLU2 needs backward computation.
I0905 01:04:50.668123 73306 net.cpp:226] Scale2 needs backward computation.
I0905 01:04:50.668130 73306 net.cpp:226] BatchNorm2 needs backward computation.
I0905 01:04:50.668138 73306 net.cpp:226] Concat1_Concat1_0_split needs backward computation.
I0905 01:04:50.668148 73306 net.cpp:226] Concat1 needs backward computation.
I0905 01:04:50.668155 73306 net.cpp:226] Dropout2 needs backward computation.
I0905 01:04:50.668164 73306 net.cpp:226] Convolution2 needs backward computation.
I0905 01:04:50.668171 73306 net.cpp:226] ReLU1 needs backward computation.
I0905 01:04:50.668179 73306 net.cpp:226] Scale1 needs backward computation.
I0905 01:04:50.668186 73306 net.cpp:226] BatchNorm1 needs backward computation.
I0905 01:04:50.668195 73306 net.cpp:226] Dropout1_Dropout1_0_split needs backward computation.
I0905 01:04:50.668202 73306 net.cpp:226] Dropout1 needs backward computation.
I0905 01:04:50.668210 73306 net.cpp:226] Convolution1 needs backward computation.
I0905 01:04:50.668220 73306 net.cpp:228] Data2_Data1_1_split does not need backward computation.
I0905 01:04:50.668228 73306 net.cpp:228] Data1 does not need backward computation.
I0905 01:04:50.668238 73306 net.cpp:270] This network produces output Accuracy1
I0905 01:04:50.668246 73306 net.cpp:270] This network produces output SoftmaxWithLoss1
I0905 01:04:50.668408 73306 net.cpp:283] Network initialization done.
I0905 01:04:50.669268 73306 solver.cpp:60] Solver scaffolding done.
I0905 01:04:50.678032 73306 caffe.cpp:251] Starting Optimization
I0905 01:04:50.678063 73306 solver.cpp:279] Solving DenseNN
I0905 01:04:50.678072 73306 solver.cpp:280] Learning Rate Policy: multistep
I0905 01:04:50.684942 73306 solver.cpp:337] Iteration 0, Testing net (#0)
I0905 01:05:32.893559 73306 solver.cpp:404]     Test net output #0: Accuracy1 = 0.503438
I0905 01:05:32.893759 73306 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 87.3366 (* 1 = 87.3366 loss)
I0905 01:05:33.532405 73306 solver.cpp:228] Iteration 0, loss = 0.694921
I0905 01:05:33.532438 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.694921 (* 1 = 0.694921 loss)
I0905 01:05:33.532465 73306 sgd_solver.cpp:106] Iteration 0, lr = 0.1
I0905 01:05:39.561316 73306 solver.cpp:228] Iteration 10, loss = 0.640744
I0905 01:05:39.561378 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.640744 (* 1 = 0.640744 loss)
I0905 01:05:39.561391 73306 sgd_solver.cpp:106] Iteration 10, lr = 0.1
I0905 01:05:45.654927 73306 solver.cpp:228] Iteration 20, loss = 0.817305
I0905 01:05:45.654991 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.817305 (* 1 = 0.817305 loss)
I0905 01:05:45.655007 73306 sgd_solver.cpp:106] Iteration 20, lr = 0.1
I0905 01:05:51.739327 73306 solver.cpp:228] Iteration 30, loss = 1.19528
I0905 01:05:51.739399 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.19528 (* 1 = 1.19528 loss)
I0905 01:05:51.739413 73306 sgd_solver.cpp:106] Iteration 30, lr = 0.1
I0905 01:05:57.802809 73306 solver.cpp:228] Iteration 40, loss = 0.839169
I0905 01:05:57.802856 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.839168 (* 1 = 0.839168 loss)
I0905 01:05:57.802868 73306 sgd_solver.cpp:106] Iteration 40, lr = 0.1
I0905 01:06:03.957587 73306 solver.cpp:228] Iteration 50, loss = 0.235214
I0905 01:06:03.957731 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.235214 (* 1 = 0.235214 loss)
I0905 01:06:03.957773 73306 sgd_solver.cpp:106] Iteration 50, lr = 0.1
I0905 01:06:10.201105 73306 solver.cpp:228] Iteration 60, loss = 0.438659
I0905 01:06:10.201153 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.438659 (* 1 = 0.438659 loss)
I0905 01:06:10.201166 73306 sgd_solver.cpp:106] Iteration 60, lr = 0.1
I0905 01:06:15.164746 73306 solver.cpp:228] Iteration 70, loss = 1.04312
I0905 01:06:15.164786 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.04312 (* 1 = 1.04312 loss)
I0905 01:06:15.164798 73306 sgd_solver.cpp:106] Iteration 70, lr = 0.1
I0905 01:06:20.836729 73306 solver.cpp:228] Iteration 80, loss = 0.579607
I0905 01:06:20.836778 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.579607 (* 1 = 0.579607 loss)
I0905 01:06:20.836792 73306 sgd_solver.cpp:106] Iteration 80, lr = 0.1
I0905 01:06:26.900084 73306 solver.cpp:228] Iteration 90, loss = 0.413468
I0905 01:06:26.900128 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.413468 (* 1 = 0.413468 loss)
I0905 01:06:26.900144 73306 sgd_solver.cpp:106] Iteration 90, lr = 0.1
I0905 01:06:33.324128 73306 solver.cpp:228] Iteration 100, loss = 0.44003
I0905 01:06:33.324174 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.44003 (* 1 = 0.44003 loss)
I0905 01:06:33.324187 73306 sgd_solver.cpp:106] Iteration 100, lr = 0.1
I0905 01:06:39.717300 73306 solver.cpp:228] Iteration 110, loss = 0.744567
I0905 01:06:39.717437 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.744567 (* 1 = 0.744567 loss)
I0905 01:06:39.717463 73306 sgd_solver.cpp:106] Iteration 110, lr = 0.1
I0905 01:06:45.501883 73306 solver.cpp:228] Iteration 120, loss = 0.725784
I0905 01:06:45.501934 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.725783 (* 1 = 0.725783 loss)
I0905 01:06:45.501946 73306 sgd_solver.cpp:106] Iteration 120, lr = 0.1
I0905 01:06:51.937885 73306 solver.cpp:228] Iteration 130, loss = 0.360738
I0905 01:06:51.937928 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.360737 (* 1 = 0.360737 loss)
I0905 01:06:51.937939 73306 sgd_solver.cpp:106] Iteration 130, lr = 0.1
I0905 01:06:58.023109 73306 solver.cpp:228] Iteration 140, loss = 0.587211
I0905 01:06:58.023144 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.587211 (* 1 = 0.587211 loss)
I0905 01:06:58.023159 73306 sgd_solver.cpp:106] Iteration 140, lr = 0.1
I0905 01:07:04.111407 73306 solver.cpp:228] Iteration 150, loss = 0.830495
I0905 01:07:04.111455 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.830494 (* 1 = 0.830494 loss)
I0905 01:07:04.111467 73306 sgd_solver.cpp:106] Iteration 150, lr = 0.1
I0905 01:07:10.227193 73306 solver.cpp:228] Iteration 160, loss = 0.976589
I0905 01:07:10.227371 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.976589 (* 1 = 0.976589 loss)
I0905 01:07:10.227404 73306 sgd_solver.cpp:106] Iteration 160, lr = 0.1
I0905 01:07:16.345834 73306 solver.cpp:228] Iteration 170, loss = 0.832703
I0905 01:07:16.345885 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.832703 (* 1 = 0.832703 loss)
I0905 01:07:16.345899 73306 sgd_solver.cpp:106] Iteration 170, lr = 0.1
I0905 01:07:22.448202 73306 solver.cpp:228] Iteration 180, loss = 0.798213
I0905 01:07:22.448247 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.798212 (* 1 = 0.798212 loss)
I0905 01:07:22.448259 73306 sgd_solver.cpp:106] Iteration 180, lr = 0.1
I0905 01:07:28.717995 73306 solver.cpp:228] Iteration 190, loss = 0.603729
I0905 01:07:28.718040 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.603729 (* 1 = 0.603729 loss)
I0905 01:07:28.718053 73306 sgd_solver.cpp:106] Iteration 190, lr = 0.1
I0905 01:07:34.991451 73306 solver.cpp:228] Iteration 200, loss = 0.454489
I0905 01:07:34.991499 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.454489 (* 1 = 0.454489 loss)
I0905 01:07:34.991514 73306 sgd_solver.cpp:106] Iteration 200, lr = 0.1
I0905 01:07:40.791173 73306 solver.cpp:228] Iteration 210, loss = 0.4419
I0905 01:07:40.791304 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.4419 (* 1 = 0.4419 loss)
I0905 01:07:40.791332 73306 sgd_solver.cpp:106] Iteration 210, lr = 0.1
I0905 01:07:46.923091 73306 solver.cpp:228] Iteration 220, loss = 0.741502
I0905 01:07:46.923136 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.741502 (* 1 = 0.741502 loss)
I0905 01:07:46.923151 73306 sgd_solver.cpp:106] Iteration 220, lr = 0.1
I0905 01:07:53.015283 73306 solver.cpp:228] Iteration 230, loss = 0.60394
I0905 01:07:53.015327 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.603939 (* 1 = 0.603939 loss)
I0905 01:07:53.015339 73306 sgd_solver.cpp:106] Iteration 230, lr = 0.1
I0905 01:07:58.657534 73306 solver.cpp:228] Iteration 240, loss = 0.340776
I0905 01:07:58.657599 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.340776 (* 1 = 0.340776 loss)
I0905 01:07:58.657614 73306 sgd_solver.cpp:106] Iteration 240, lr = 0.1
I0905 01:08:04.144876 73306 solver.cpp:228] Iteration 250, loss = 0.658384
I0905 01:08:04.144927 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.658384 (* 1 = 0.658384 loss)
I0905 01:08:04.144942 73306 sgd_solver.cpp:106] Iteration 250, lr = 0.1
I0905 01:08:10.360231 73306 solver.cpp:228] Iteration 260, loss = 0.676678
I0905 01:08:10.360275 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.676677 (* 1 = 0.676677 loss)
I0905 01:08:10.360290 73306 sgd_solver.cpp:106] Iteration 260, lr = 0.1
I0905 01:08:16.466996 73306 solver.cpp:228] Iteration 270, loss = 0.768094
I0905 01:08:16.467164 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.768093 (* 1 = 0.768093 loss)
I0905 01:08:16.467209 73306 sgd_solver.cpp:106] Iteration 270, lr = 0.1
I0905 01:08:22.557258 73306 solver.cpp:228] Iteration 280, loss = 0.615666
I0905 01:08:22.557307 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.615666 (* 1 = 0.615666 loss)
I0905 01:08:22.557319 73306 sgd_solver.cpp:106] Iteration 280, lr = 0.1
I0905 01:08:28.677197 73306 solver.cpp:228] Iteration 290, loss = 0.489956
I0905 01:08:28.677264 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.489956 (* 1 = 0.489956 loss)
I0905 01:08:28.677278 73306 sgd_solver.cpp:106] Iteration 290, lr = 0.1
I0905 01:08:34.809685 73306 solver.cpp:228] Iteration 300, loss = 0.479896
I0905 01:08:34.809729 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.479896 (* 1 = 0.479896 loss)
I0905 01:08:34.809743 73306 sgd_solver.cpp:106] Iteration 300, lr = 0.1
I0905 01:08:40.900261 73306 solver.cpp:228] Iteration 310, loss = 0.673794
I0905 01:08:40.900310 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.673794 (* 1 = 0.673794 loss)
I0905 01:08:40.900324 73306 sgd_solver.cpp:106] Iteration 310, lr = 0.1
I0905 01:08:46.708809 73306 solver.cpp:228] Iteration 320, loss = 0.613292
I0905 01:08:46.709009 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.613292 (* 1 = 0.613292 loss)
I0905 01:08:46.709048 73306 sgd_solver.cpp:106] Iteration 320, lr = 0.1
I0905 01:08:53.174599 73306 solver.cpp:228] Iteration 330, loss = 0.511963
I0905 01:08:53.174674 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.511962 (* 1 = 0.511962 loss)
I0905 01:08:53.174688 73306 sgd_solver.cpp:106] Iteration 330, lr = 0.1
I0905 01:08:59.322806 73306 solver.cpp:228] Iteration 340, loss = 0.440825
I0905 01:08:59.322854 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.440825 (* 1 = 0.440825 loss)
I0905 01:08:59.322868 73306 sgd_solver.cpp:106] Iteration 340, lr = 0.1
I0905 01:09:05.453389 73306 solver.cpp:228] Iteration 350, loss = 0.683679
I0905 01:09:05.453439 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.683679 (* 1 = 0.683679 loss)
I0905 01:09:05.453452 73306 sgd_solver.cpp:106] Iteration 350, lr = 0.1
I0905 01:09:11.581017 73306 solver.cpp:228] Iteration 360, loss = 0.514638
I0905 01:09:11.581063 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.514638 (* 1 = 0.514638 loss)
I0905 01:09:11.581076 73306 sgd_solver.cpp:106] Iteration 360, lr = 0.1
I0905 01:09:17.708597 73306 solver.cpp:228] Iteration 370, loss = 0.3525
I0905 01:09:17.708746 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.352499 (* 1 = 0.352499 loss)
I0905 01:09:17.708806 73306 sgd_solver.cpp:106] Iteration 370, lr = 0.1
I0905 01:09:24.156822 73306 solver.cpp:228] Iteration 380, loss = 0.392298
I0905 01:09:24.156883 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.392298 (* 1 = 0.392298 loss)
I0905 01:09:24.156901 73306 sgd_solver.cpp:106] Iteration 380, lr = 0.1
I0905 01:09:30.306624 73306 solver.cpp:228] Iteration 390, loss = 0.625868
I0905 01:09:30.306689 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.625868 (* 1 = 0.625868 loss)
I0905 01:09:30.306700 73306 sgd_solver.cpp:106] Iteration 390, lr = 0.1
I0905 01:09:36.436056 73306 solver.cpp:228] Iteration 400, loss = 0.728611
I0905 01:09:36.436099 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.728611 (* 1 = 0.728611 loss)
I0905 01:09:36.436111 73306 sgd_solver.cpp:106] Iteration 400, lr = 0.1
I0905 01:09:42.641263 73306 solver.cpp:228] Iteration 410, loss = 0.787029
I0905 01:09:42.641315 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.787029 (* 1 = 0.787029 loss)
I0905 01:09:42.641330 73306 sgd_solver.cpp:106] Iteration 410, lr = 0.1
I0905 01:09:47.633548 73306 solver.cpp:228] Iteration 420, loss = 0.881563
I0905 01:09:47.633594 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.881563 (* 1 = 0.881563 loss)
I0905 01:09:47.633605 73306 sgd_solver.cpp:106] Iteration 420, lr = 0.1
I0905 01:09:53.771715 73306 solver.cpp:228] Iteration 430, loss = 0.54619
I0905 01:09:53.771883 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.546189 (* 1 = 0.546189 loss)
I0905 01:09:53.771914 73306 sgd_solver.cpp:106] Iteration 430, lr = 0.1
I0905 01:09:59.883661 73306 solver.cpp:228] Iteration 440, loss = 0.652453
I0905 01:09:59.883713 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.652453 (* 1 = 0.652453 loss)
I0905 01:09:59.883725 73306 sgd_solver.cpp:106] Iteration 440, lr = 0.1
I0905 01:10:06.307760 73306 solver.cpp:228] Iteration 450, loss = 0.56597
I0905 01:10:06.307803 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.56597 (* 1 = 0.56597 loss)
I0905 01:10:06.307817 73306 sgd_solver.cpp:106] Iteration 450, lr = 0.1
I0905 01:10:12.396417 73306 solver.cpp:228] Iteration 460, loss = 0.547375
I0905 01:10:12.396461 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.547375 (* 1 = 0.547375 loss)
I0905 01:10:12.396476 73306 sgd_solver.cpp:106] Iteration 460, lr = 0.1
I0905 01:10:18.501132 73306 solver.cpp:228] Iteration 470, loss = 0.597669
I0905 01:10:18.501180 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.597669 (* 1 = 0.597669 loss)
I0905 01:10:18.501196 73306 sgd_solver.cpp:106] Iteration 470, lr = 0.1
I0905 01:10:24.593348 73306 solver.cpp:228] Iteration 480, loss = 0.673815
I0905 01:10:24.593559 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.673815 (* 1 = 0.673815 loss)
I0905 01:10:24.593586 73306 sgd_solver.cpp:106] Iteration 480, lr = 0.1
I0905 01:10:30.683789 73306 solver.cpp:228] Iteration 490, loss = 1.01149
I0905 01:10:30.683833 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.01149 (* 1 = 1.01149 loss)
I0905 01:10:30.683847 73306 sgd_solver.cpp:106] Iteration 490, lr = 0.1
I0905 01:10:36.823231 73306 solver.cpp:228] Iteration 500, loss = 0.617854
I0905 01:10:36.823281 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.617854 (* 1 = 0.617854 loss)
I0905 01:10:36.823295 73306 sgd_solver.cpp:106] Iteration 500, lr = 0.1
I0905 01:10:42.904897 73306 solver.cpp:228] Iteration 510, loss = 0.549986
I0905 01:10:42.904938 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.549986 (* 1 = 0.549986 loss)
I0905 01:10:42.904950 73306 sgd_solver.cpp:106] Iteration 510, lr = 0.1
I0905 01:10:48.990898 73306 solver.cpp:228] Iteration 520, loss = 0.650042
I0905 01:10:48.990947 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.650042 (* 1 = 0.650042 loss)
I0905 01:10:48.990960 73306 sgd_solver.cpp:106] Iteration 520, lr = 0.1
I0905 01:10:55.385962 73306 solver.cpp:228] Iteration 530, loss = 0.41036
I0905 01:10:55.386081 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.41036 (* 1 = 0.41036 loss)
I0905 01:10:55.386108 73306 sgd_solver.cpp:106] Iteration 530, lr = 0.1
I0905 01:11:01.481858 73306 solver.cpp:228] Iteration 540, loss = 0.512613
I0905 01:11:01.481909 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.512613 (* 1 = 0.512613 loss)
I0905 01:11:01.481922 73306 sgd_solver.cpp:106] Iteration 540, lr = 0.1
I0905 01:11:07.630275 73306 solver.cpp:228] Iteration 550, loss = 0.44077
I0905 01:11:07.630319 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.44077 (* 1 = 0.44077 loss)
I0905 01:11:07.630331 73306 sgd_solver.cpp:106] Iteration 550, lr = 0.1
I0905 01:11:13.729816 73306 solver.cpp:228] Iteration 560, loss = 0.769765
I0905 01:11:13.729858 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.769764 (* 1 = 0.769764 loss)
I0905 01:11:13.729871 73306 sgd_solver.cpp:106] Iteration 560, lr = 0.1
I0905 01:11:19.844725 73306 solver.cpp:228] Iteration 570, loss = 0.617979
I0905 01:11:19.844784 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.617979 (* 1 = 0.617979 loss)
I0905 01:11:19.844795 73306 sgd_solver.cpp:106] Iteration 570, lr = 0.1
I0905 01:11:25.967787 73306 solver.cpp:228] Iteration 580, loss = 0.521624
I0905 01:11:25.968001 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.521624 (* 1 = 0.521624 loss)
I0905 01:11:25.968016 73306 sgd_solver.cpp:106] Iteration 580, lr = 0.1
I0905 01:11:31.821602 73306 solver.cpp:228] Iteration 590, loss = 0.918578
I0905 01:11:31.821648 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.918578 (* 1 = 0.918578 loss)
I0905 01:11:31.821660 73306 sgd_solver.cpp:106] Iteration 590, lr = 0.1
I0905 01:11:37.409162 73306 solver.cpp:228] Iteration 600, loss = 0.61895
I0905 01:11:37.409225 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.61895 (* 1 = 0.61895 loss)
I0905 01:11:37.409241 73306 sgd_solver.cpp:106] Iteration 600, lr = 0.1
I0905 01:11:43.127413 73306 solver.cpp:228] Iteration 610, loss = 0.775233
I0905 01:11:43.127454 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.775233 (* 1 = 0.775233 loss)
I0905 01:11:43.127467 73306 sgd_solver.cpp:106] Iteration 610, lr = 0.1
I0905 01:11:49.127375 73306 solver.cpp:228] Iteration 620, loss = 0.651836
I0905 01:11:49.127442 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.651835 (* 1 = 0.651835 loss)
I0905 01:11:49.127459 73306 sgd_solver.cpp:106] Iteration 620, lr = 0.1
I0905 01:11:55.221984 73306 solver.cpp:228] Iteration 630, loss = 0.513092
I0905 01:11:55.222023 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.513092 (* 1 = 0.513092 loss)
I0905 01:11:55.222034 73306 sgd_solver.cpp:106] Iteration 630, lr = 0.1
I0905 01:12:01.552103 73306 solver.cpp:228] Iteration 640, loss = 0.427719
I0905 01:12:01.552335 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.427719 (* 1 = 0.427719 loss)
I0905 01:12:01.552351 73306 sgd_solver.cpp:106] Iteration 640, lr = 0.1
I0905 01:12:07.737824 73306 solver.cpp:228] Iteration 650, loss = 0.877189
I0905 01:12:07.737870 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.877189 (* 1 = 0.877189 loss)
I0905 01:12:07.737884 73306 sgd_solver.cpp:106] Iteration 650, lr = 0.1
I0905 01:12:13.875319 73306 solver.cpp:228] Iteration 660, loss = 0.798345
I0905 01:12:13.875365 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.798345 (* 1 = 0.798345 loss)
I0905 01:12:13.875377 73306 sgd_solver.cpp:106] Iteration 660, lr = 0.1
I0905 01:12:19.992564 73306 solver.cpp:228] Iteration 670, loss = 0.637488
I0905 01:12:19.992626 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.637488 (* 1 = 0.637488 loss)
I0905 01:12:19.992641 73306 sgd_solver.cpp:106] Iteration 670, lr = 0.1
I0905 01:12:26.417011 73306 solver.cpp:228] Iteration 680, loss = 0.754494
I0905 01:12:26.417064 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.754493 (* 1 = 0.754493 loss)
I0905 01:12:26.417078 73306 sgd_solver.cpp:106] Iteration 680, lr = 0.1
I0905 01:12:32.479131 73306 solver.cpp:228] Iteration 690, loss = 0.621019
I0905 01:12:32.479320 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.621019 (* 1 = 0.621019 loss)
I0905 01:12:32.479348 73306 sgd_solver.cpp:106] Iteration 690, lr = 0.1
I0905 01:12:38.584578 73306 solver.cpp:228] Iteration 700, loss = 0.445721
I0905 01:12:38.584624 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.445721 (* 1 = 0.445721 loss)
I0905 01:12:38.584637 73306 sgd_solver.cpp:106] Iteration 700, lr = 0.1
I0905 01:12:44.683240 73306 solver.cpp:228] Iteration 710, loss = 0.542565
I0905 01:12:44.683301 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.542564 (* 1 = 0.542564 loss)
I0905 01:12:44.683316 73306 sgd_solver.cpp:106] Iteration 710, lr = 0.1
I0905 01:12:50.826330 73306 solver.cpp:228] Iteration 720, loss = 0.701038
I0905 01:12:50.826390 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.701038 (* 1 = 0.701038 loss)
I0905 01:12:50.826402 73306 sgd_solver.cpp:106] Iteration 720, lr = 0.1
I0905 01:12:56.920286 73306 solver.cpp:228] Iteration 730, loss = 0.49459
I0905 01:12:56.920333 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.49459 (* 1 = 0.49459 loss)
I0905 01:12:56.920346 73306 sgd_solver.cpp:106] Iteration 730, lr = 0.1
I0905 01:13:02.676357 73306 solver.cpp:228] Iteration 740, loss = 0.335735
I0905 01:13:02.676556 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.335735 (* 1 = 0.335735 loss)
I0905 01:13:02.676609 73306 sgd_solver.cpp:106] Iteration 740, lr = 0.1
I0905 01:13:09.187328 73306 solver.cpp:228] Iteration 750, loss = 0.815574
I0905 01:13:09.187369 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.815573 (* 1 = 0.815573 loss)
I0905 01:13:09.187382 73306 sgd_solver.cpp:106] Iteration 750, lr = 0.1
I0905 01:13:15.314931 73306 solver.cpp:228] Iteration 760, loss = 0.520994
I0905 01:13:15.314975 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.520994 (* 1 = 0.520994 loss)
I0905 01:13:15.314986 73306 sgd_solver.cpp:106] Iteration 760, lr = 0.1
I0905 01:13:20.660032 73306 solver.cpp:228] Iteration 770, loss = 0.612008
I0905 01:13:20.660082 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.612008 (* 1 = 0.612008 loss)
I0905 01:13:20.660095 73306 sgd_solver.cpp:106] Iteration 770, lr = 0.1
I0905 01:13:26.301376 73306 solver.cpp:228] Iteration 780, loss = 0.487582
I0905 01:13:26.301419 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.487582 (* 1 = 0.487582 loss)
I0905 01:13:26.301434 73306 sgd_solver.cpp:106] Iteration 780, lr = 0.1
I0905 01:13:32.059725 73306 solver.cpp:228] Iteration 790, loss = 0.525679
I0905 01:13:32.059772 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.525678 (* 1 = 0.525678 loss)
I0905 01:13:32.059785 73306 sgd_solver.cpp:106] Iteration 790, lr = 0.1
I0905 01:13:37.969873 73306 solver.cpp:337] Iteration 800, Testing net (#0)
I0905 01:14:20.939362 73306 solver.cpp:404]     Test net output #0: Accuracy1 = 0.597812
I0905 01:14:20.939532 73306 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.738194 (* 1 = 0.738194 loss)
I0905 01:14:21.146503 73306 solver.cpp:228] Iteration 800, loss = 0.790894
I0905 01:14:21.146533 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.790894 (* 1 = 0.790894 loss)
I0905 01:14:21.146549 73306 sgd_solver.cpp:106] Iteration 800, lr = 0.1
I0905 01:14:27.245353 73306 solver.cpp:228] Iteration 810, loss = 0.539216
I0905 01:14:27.245407 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.539216 (* 1 = 0.539216 loss)
I0905 01:14:27.245420 73306 sgd_solver.cpp:106] Iteration 810, lr = 0.1
I0905 01:14:33.313470 73306 solver.cpp:228] Iteration 820, loss = 0.816419
I0905 01:14:33.313524 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.816419 (* 1 = 0.816419 loss)
I0905 01:14:33.313537 73306 sgd_solver.cpp:106] Iteration 820, lr = 0.1
I0905 01:14:39.416646 73306 solver.cpp:228] Iteration 830, loss = 0.57396
I0905 01:14:39.416692 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.57396 (* 1 = 0.57396 loss)
I0905 01:14:39.416707 73306 sgd_solver.cpp:106] Iteration 830, lr = 0.1
I0905 01:14:45.567031 73306 solver.cpp:228] Iteration 840, loss = 0.701592
I0905 01:14:45.567075 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.701592 (* 1 = 0.701592 loss)
I0905 01:14:45.567085 73306 sgd_solver.cpp:106] Iteration 840, lr = 0.1
I0905 01:14:51.671144 73306 solver.cpp:228] Iteration 850, loss = 0.739676
I0905 01:14:51.671332 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.739676 (* 1 = 0.739676 loss)
I0905 01:14:51.671349 73306 sgd_solver.cpp:106] Iteration 850, lr = 0.1
I0905 01:14:57.763885 73306 solver.cpp:228] Iteration 860, loss = 0.633565
I0905 01:14:57.763943 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.633565 (* 1 = 0.633565 loss)
I0905 01:14:57.763958 73306 sgd_solver.cpp:106] Iteration 860, lr = 0.1
I0905 01:15:03.791348 73306 solver.cpp:228] Iteration 870, loss = 0.358363
I0905 01:15:03.791400 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.358363 (* 1 = 0.358363 loss)
I0905 01:15:03.791412 73306 sgd_solver.cpp:106] Iteration 870, lr = 0.1
I0905 01:15:09.268973 73306 solver.cpp:228] Iteration 880, loss = 0.5558
I0905 01:15:09.269023 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.5558 (* 1 = 0.5558 loss)
I0905 01:15:09.269035 73306 sgd_solver.cpp:106] Iteration 880, lr = 0.1
I0905 01:15:14.835444 73306 solver.cpp:228] Iteration 890, loss = 0.669539
I0905 01:15:14.835489 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.669539 (* 1 = 0.669539 loss)
I0905 01:15:14.835501 73306 sgd_solver.cpp:106] Iteration 890, lr = 0.1
I0905 01:15:20.928146 73306 solver.cpp:228] Iteration 900, loss = 0.680624
I0905 01:15:20.928200 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.680623 (* 1 = 0.680623 loss)
I0905 01:15:20.928215 73306 sgd_solver.cpp:106] Iteration 900, lr = 0.1
I0905 01:15:27.012011 73306 solver.cpp:228] Iteration 910, loss = 0.570025
I0905 01:15:27.012240 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.570025 (* 1 = 0.570025 loss)
I0905 01:15:27.012254 73306 sgd_solver.cpp:106] Iteration 910, lr = 0.1
I0905 01:15:33.083789 73306 solver.cpp:228] Iteration 920, loss = 0.98065
I0905 01:15:33.083853 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.98065 (* 1 = 0.98065 loss)
I0905 01:15:33.083869 73306 sgd_solver.cpp:106] Iteration 920, lr = 0.1
I0905 01:15:39.503430 73306 solver.cpp:228] Iteration 930, loss = 0.397916
I0905 01:15:39.503479 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.397916 (* 1 = 0.397916 loss)
I0905 01:15:39.503491 73306 sgd_solver.cpp:106] Iteration 930, lr = 0.1
I0905 01:15:45.596904 73306 solver.cpp:228] Iteration 940, loss = 0.788337
I0905 01:15:45.596956 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.788337 (* 1 = 0.788337 loss)
I0905 01:15:45.596982 73306 sgd_solver.cpp:106] Iteration 940, lr = 0.1
I0905 01:15:51.674547 73306 solver.cpp:228] Iteration 950, loss = 0.594976
I0905 01:15:51.674599 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.594976 (* 1 = 0.594976 loss)
I0905 01:15:51.674612 73306 sgd_solver.cpp:106] Iteration 950, lr = 0.1
I0905 01:15:57.851676 73306 solver.cpp:228] Iteration 960, loss = 0.63811
I0905 01:15:57.851824 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.63811 (* 1 = 0.63811 loss)
I0905 01:15:57.851855 73306 sgd_solver.cpp:106] Iteration 960, lr = 0.1
I0905 01:16:04.137249 73306 solver.cpp:228] Iteration 970, loss = 0.399881
I0905 01:16:04.137293 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.399881 (* 1 = 0.399881 loss)
I0905 01:16:04.137305 73306 sgd_solver.cpp:106] Iteration 970, lr = 0.1
I0905 01:16:09.908046 73306 solver.cpp:228] Iteration 980, loss = 0.413757
I0905 01:16:09.908088 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.413757 (* 1 = 0.413757 loss)
I0905 01:16:09.908099 73306 sgd_solver.cpp:106] Iteration 980, lr = 0.1
I0905 01:16:16.342945 73306 solver.cpp:228] Iteration 990, loss = 0.381292
I0905 01:16:16.342985 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.381292 (* 1 = 0.381292 loss)
I0905 01:16:16.342999 73306 sgd_solver.cpp:106] Iteration 990, lr = 0.1
I0905 01:16:22.111058 73306 solver.cpp:228] Iteration 1000, loss = 0.572216
I0905 01:16:22.111098 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.572215 (* 1 = 0.572215 loss)
I0905 01:16:22.111111 73306 sgd_solver.cpp:106] Iteration 1000, lr = 0.1
I0905 01:16:28.552897 73306 solver.cpp:228] Iteration 1010, loss = 0.448553
I0905 01:16:28.553104 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.448553 (* 1 = 0.448553 loss)
I0905 01:16:28.553145 73306 sgd_solver.cpp:106] Iteration 1010, lr = 0.1
I0905 01:16:34.616821 73306 solver.cpp:228] Iteration 1020, loss = 0.399137
I0905 01:16:34.616866 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.399137 (* 1 = 0.399137 loss)
I0905 01:16:34.616880 73306 sgd_solver.cpp:106] Iteration 1020, lr = 0.1
I0905 01:16:40.809679 73306 solver.cpp:228] Iteration 1030, loss = 0.677401
I0905 01:16:40.809722 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.677401 (* 1 = 0.677401 loss)
I0905 01:16:40.809736 73306 sgd_solver.cpp:106] Iteration 1030, lr = 0.1
I0905 01:16:47.113159 73306 solver.cpp:228] Iteration 1040, loss = 0.468939
I0905 01:16:47.113195 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.468938 (* 1 = 0.468938 loss)
I0905 01:16:47.113210 73306 sgd_solver.cpp:106] Iteration 1040, lr = 0.1
I0905 01:16:53.004045 73306 solver.cpp:228] Iteration 1050, loss = 0.497111
I0905 01:16:53.004087 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.49711 (* 1 = 0.49711 loss)
I0905 01:16:53.004099 73306 sgd_solver.cpp:106] Iteration 1050, lr = 0.1
I0905 01:16:58.591891 73306 solver.cpp:228] Iteration 1060, loss = 0.466408
I0905 01:16:58.592087 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.466408 (* 1 = 0.466408 loss)
I0905 01:16:58.592102 73306 sgd_solver.cpp:106] Iteration 1060, lr = 0.1
I0905 01:17:04.112241 73306 solver.cpp:228] Iteration 1070, loss = 0.523945
I0905 01:17:04.112287 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.523945 (* 1 = 0.523945 loss)
I0905 01:17:04.112298 73306 sgd_solver.cpp:106] Iteration 1070, lr = 0.1
I0905 01:17:10.228364 73306 solver.cpp:228] Iteration 1080, loss = 0.419505
I0905 01:17:10.228406 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.419505 (* 1 = 0.419505 loss)
I0905 01:17:10.228420 73306 sgd_solver.cpp:106] Iteration 1080, lr = 0.1
I0905 01:17:16.325088 73306 solver.cpp:228] Iteration 1090, loss = 0.399571
I0905 01:17:16.325134 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.399571 (* 1 = 0.399571 loss)
I0905 01:17:16.325155 73306 sgd_solver.cpp:106] Iteration 1090, lr = 0.1
I0905 01:17:22.450337 73306 solver.cpp:228] Iteration 1100, loss = 0.43315
I0905 01:17:22.450381 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.43315 (* 1 = 0.43315 loss)
I0905 01:17:22.450392 73306 sgd_solver.cpp:106] Iteration 1100, lr = 0.1
I0905 01:17:28.467622 73306 solver.cpp:228] Iteration 1110, loss = 0.66448
I0905 01:17:28.467679 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.664479 (* 1 = 0.664479 loss)
I0905 01:17:28.467692 73306 sgd_solver.cpp:106] Iteration 1110, lr = 0.1
I0905 01:17:34.884400 73306 solver.cpp:228] Iteration 1120, loss = 0.611024
I0905 01:17:34.884536 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.611024 (* 1 = 0.611024 loss)
I0905 01:17:34.884567 73306 sgd_solver.cpp:106] Iteration 1120, lr = 0.1
I0905 01:17:40.759337 73306 solver.cpp:228] Iteration 1130, loss = 0.50377
I0905 01:17:40.759392 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.503769 (* 1 = 0.503769 loss)
I0905 01:17:40.759404 73306 sgd_solver.cpp:106] Iteration 1130, lr = 0.1
I0905 01:17:46.867389 73306 solver.cpp:228] Iteration 1140, loss = 0.634265
I0905 01:17:46.867447 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.634265 (* 1 = 0.634265 loss)
I0905 01:17:46.867461 73306 sgd_solver.cpp:106] Iteration 1140, lr = 0.1
I0905 01:17:53.200022 73306 solver.cpp:228] Iteration 1150, loss = 0.450871
I0905 01:17:53.200074 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.450871 (* 1 = 0.450871 loss)
I0905 01:17:53.200088 73306 sgd_solver.cpp:106] Iteration 1150, lr = 0.1
I0905 01:17:59.395180 73306 solver.cpp:228] Iteration 1160, loss = 0.710329
I0905 01:17:59.395228 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.710329 (* 1 = 0.710329 loss)
I0905 01:17:59.395243 73306 sgd_solver.cpp:106] Iteration 1160, lr = 0.1
I0905 01:18:05.522327 73306 solver.cpp:228] Iteration 1170, loss = 0.39253
I0905 01:18:05.522577 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.39253 (* 1 = 0.39253 loss)
I0905 01:18:05.522593 73306 sgd_solver.cpp:106] Iteration 1170, lr = 0.1
I0905 01:18:11.609369 73306 solver.cpp:228] Iteration 1180, loss = 0.491011
I0905 01:18:11.609428 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.491011 (* 1 = 0.491011 loss)
I0905 01:18:11.609443 73306 sgd_solver.cpp:106] Iteration 1180, lr = 0.1
I0905 01:18:17.708801 73306 solver.cpp:228] Iteration 1190, loss = 0.258599
I0905 01:18:17.708848 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.258599 (* 1 = 0.258599 loss)
I0905 01:18:17.708860 73306 sgd_solver.cpp:106] Iteration 1190, lr = 0.1
I0905 01:18:23.841938 73306 solver.cpp:228] Iteration 1200, loss = 0.329845
I0905 01:18:23.841974 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.329845 (* 1 = 0.329845 loss)
I0905 01:18:23.841987 73306 sgd_solver.cpp:106] Iteration 1200, lr = 0.1
I0905 01:18:30.236058 73306 solver.cpp:228] Iteration 1210, loss = 0.496563
I0905 01:18:30.236100 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.496563 (* 1 = 0.496563 loss)
I0905 01:18:30.236114 73306 sgd_solver.cpp:106] Iteration 1210, lr = 0.1
I0905 01:18:36.296656 73306 solver.cpp:228] Iteration 1220, loss = 0.517092
I0905 01:18:36.296883 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.517092 (* 1 = 0.517092 loss)
I0905 01:18:36.296912 73306 sgd_solver.cpp:106] Iteration 1220, lr = 0.1
I0905 01:18:41.968669 73306 solver.cpp:228] Iteration 1230, loss = 0.533462
I0905 01:18:41.968729 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.533462 (* 1 = 0.533462 loss)
I0905 01:18:41.968744 73306 sgd_solver.cpp:106] Iteration 1230, lr = 0.1
I0905 01:18:47.251255 73306 solver.cpp:228] Iteration 1240, loss = 0.324818
I0905 01:18:47.251312 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.324818 (* 1 = 0.324818 loss)
I0905 01:18:47.251324 73306 sgd_solver.cpp:106] Iteration 1240, lr = 0.1
I0905 01:18:52.897300 73306 solver.cpp:228] Iteration 1250, loss = 0.453021
I0905 01:18:52.897335 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.453021 (* 1 = 0.453021 loss)
I0905 01:18:52.897346 73306 sgd_solver.cpp:106] Iteration 1250, lr = 0.1
I0905 01:18:57.966287 73306 solver.cpp:228] Iteration 1260, loss = 0.416711
I0905 01:18:57.966338 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.416711 (* 1 = 0.416711 loss)
I0905 01:18:57.966352 73306 sgd_solver.cpp:106] Iteration 1260, lr = 0.1
I0905 01:19:03.042227 73306 solver.cpp:228] Iteration 1270, loss = 0.401692
I0905 01:19:03.042270 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.401692 (* 1 = 0.401692 loss)
I0905 01:19:03.042284 73306 sgd_solver.cpp:106] Iteration 1270, lr = 0.1
I0905 01:19:08.111770 73306 solver.cpp:228] Iteration 1280, loss = 0.235835
I0905 01:19:08.111976 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.235835 (* 1 = 0.235835 loss)
I0905 01:19:08.111992 73306 sgd_solver.cpp:106] Iteration 1280, lr = 0.1
I0905 01:19:13.170116 73306 solver.cpp:228] Iteration 1290, loss = 0.460414
I0905 01:19:13.170162 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.460414 (* 1 = 0.460414 loss)
I0905 01:19:13.170176 73306 sgd_solver.cpp:106] Iteration 1290, lr = 0.1
I0905 01:19:18.267704 73306 solver.cpp:228] Iteration 1300, loss = 1.24956
I0905 01:19:18.267752 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.24956 (* 1 = 1.24956 loss)
I0905 01:19:18.267765 73306 sgd_solver.cpp:106] Iteration 1300, lr = 0.1
I0905 01:19:23.364094 73306 solver.cpp:228] Iteration 1310, loss = 0.573447
I0905 01:19:23.364140 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.573446 (* 1 = 0.573446 loss)
I0905 01:19:23.364152 73306 sgd_solver.cpp:106] Iteration 1310, lr = 0.1
I0905 01:19:28.440667 73306 solver.cpp:228] Iteration 1320, loss = 0.561047
I0905 01:19:28.440714 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.561047 (* 1 = 0.561047 loss)
I0905 01:19:28.440728 73306 sgd_solver.cpp:106] Iteration 1320, lr = 0.1
I0905 01:19:33.493593 73306 solver.cpp:228] Iteration 1330, loss = 0.211066
I0905 01:19:33.493640 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.211066 (* 1 = 0.211066 loss)
I0905 01:19:33.493651 73306 sgd_solver.cpp:106] Iteration 1330, lr = 0.1
I0905 01:19:38.554744 73306 solver.cpp:228] Iteration 1340, loss = 0.754807
I0905 01:19:38.554968 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.754807 (* 1 = 0.754807 loss)
I0905 01:19:38.554986 73306 sgd_solver.cpp:106] Iteration 1340, lr = 0.1
I0905 01:19:43.658291 73306 solver.cpp:228] Iteration 1350, loss = 0.63816
I0905 01:19:43.658329 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.63816 (* 1 = 0.63816 loss)
I0905 01:19:43.658340 73306 sgd_solver.cpp:106] Iteration 1350, lr = 0.1
I0905 01:19:48.714707 73306 solver.cpp:228] Iteration 1360, loss = 0.610282
I0905 01:19:48.714759 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.610282 (* 1 = 0.610282 loss)
I0905 01:19:48.714773 73306 sgd_solver.cpp:106] Iteration 1360, lr = 0.1
I0905 01:19:53.747464 73306 solver.cpp:228] Iteration 1370, loss = 0.692082
I0905 01:19:53.747526 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.692082 (* 1 = 0.692082 loss)
I0905 01:19:53.747541 73306 sgd_solver.cpp:106] Iteration 1370, lr = 0.1
I0905 01:19:58.810158 73306 solver.cpp:228] Iteration 1380, loss = 0.328199
I0905 01:19:58.810204 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.328199 (* 1 = 0.328199 loss)
I0905 01:19:58.810217 73306 sgd_solver.cpp:106] Iteration 1380, lr = 0.1
I0905 01:20:03.836768 73306 solver.cpp:228] Iteration 1390, loss = 0.563076
I0905 01:20:03.836813 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.563076 (* 1 = 0.563076 loss)
I0905 01:20:03.836828 73306 sgd_solver.cpp:106] Iteration 1390, lr = 0.1
I0905 01:20:08.897346 73306 solver.cpp:228] Iteration 1400, loss = 0.5827
I0905 01:20:08.897507 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.5827 (* 1 = 0.5827 loss)
I0905 01:20:08.897524 73306 sgd_solver.cpp:106] Iteration 1400, lr = 0.1
I0905 01:20:14.607545 73306 solver.cpp:228] Iteration 1410, loss = 0.331437
I0905 01:20:14.607589 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.331436 (* 1 = 0.331436 loss)
I0905 01:20:14.607600 73306 sgd_solver.cpp:106] Iteration 1410, lr = 0.1
I0905 01:20:20.684974 73306 solver.cpp:228] Iteration 1420, loss = 0.47719
I0905 01:20:20.685034 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.477189 (* 1 = 0.477189 loss)
I0905 01:20:20.685050 73306 sgd_solver.cpp:106] Iteration 1420, lr = 0.1
I0905 01:20:26.444881 73306 solver.cpp:228] Iteration 1430, loss = 0.383565
I0905 01:20:26.444933 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.383564 (* 1 = 0.383564 loss)
I0905 01:20:26.444948 73306 sgd_solver.cpp:106] Iteration 1430, lr = 0.1
I0905 01:20:32.709252 73306 solver.cpp:228] Iteration 1440, loss = 0.803201
I0905 01:20:32.709298 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.803201 (* 1 = 0.803201 loss)
I0905 01:20:32.709311 73306 sgd_solver.cpp:106] Iteration 1440, lr = 0.1
I0905 01:20:37.972882 73306 solver.cpp:228] Iteration 1450, loss = 0.46689
I0905 01:20:37.972929 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.466889 (* 1 = 0.466889 loss)
I0905 01:20:37.972940 73306 sgd_solver.cpp:106] Iteration 1450, lr = 0.1
I0905 01:20:43.764869 73306 solver.cpp:228] Iteration 1460, loss = 0.7928
I0905 01:20:43.764974 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.792799 (* 1 = 0.792799 loss)
I0905 01:20:43.765004 73306 sgd_solver.cpp:106] Iteration 1460, lr = 0.1
I0905 01:20:50.194308 73306 solver.cpp:228] Iteration 1470, loss = 0.276348
I0905 01:20:50.194349 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.276348 (* 1 = 0.276348 loss)
I0905 01:20:50.194360 73306 sgd_solver.cpp:106] Iteration 1470, lr = 0.1
I0905 01:20:56.279227 73306 solver.cpp:228] Iteration 1480, loss = 0.379478
I0905 01:20:56.279297 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.379478 (* 1 = 0.379478 loss)
I0905 01:20:56.279312 73306 sgd_solver.cpp:106] Iteration 1480, lr = 0.1
I0905 01:21:02.380296 73306 solver.cpp:228] Iteration 1490, loss = 1.43969
I0905 01:21:02.380350 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.43969 (* 1 = 1.43969 loss)
I0905 01:21:02.380363 73306 sgd_solver.cpp:106] Iteration 1490, lr = 0.1
I0905 01:21:08.438323 73306 solver.cpp:228] Iteration 1500, loss = 0.730969
I0905 01:21:08.438374 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.730969 (* 1 = 0.730969 loss)
I0905 01:21:08.438386 73306 sgd_solver.cpp:106] Iteration 1500, lr = 0.1
I0905 01:21:14.602675 73306 solver.cpp:228] Iteration 1510, loss = 0.5641
I0905 01:21:14.602924 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.5641 (* 1 = 0.5641 loss)
I0905 01:21:14.602943 73306 sgd_solver.cpp:106] Iteration 1510, lr = 0.1
I0905 01:21:20.930217 73306 solver.cpp:228] Iteration 1520, loss = 0.46773
I0905 01:21:20.930259 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.46773 (* 1 = 0.46773 loss)
I0905 01:21:20.930271 73306 sgd_solver.cpp:106] Iteration 1520, lr = 0.1
I0905 01:21:27.015769 73306 solver.cpp:228] Iteration 1530, loss = 0.493487
I0905 01:21:27.015816 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.493487 (* 1 = 0.493487 loss)
I0905 01:21:27.015830 73306 sgd_solver.cpp:106] Iteration 1530, lr = 0.1
I0905 01:21:32.757990 73306 solver.cpp:228] Iteration 1540, loss = 0.594908
I0905 01:21:32.758038 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.594908 (* 1 = 0.594908 loss)
I0905 01:21:32.758050 73306 sgd_solver.cpp:106] Iteration 1540, lr = 0.1
I0905 01:21:39.188225 73306 solver.cpp:228] Iteration 1550, loss = 0.378448
I0905 01:21:39.188277 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.378448 (* 1 = 0.378448 loss)
I0905 01:21:39.188289 73306 sgd_solver.cpp:106] Iteration 1550, lr = 0.1
I0905 01:21:45.259002 73306 solver.cpp:228] Iteration 1560, loss = 0.496257
I0905 01:21:45.259146 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.496257 (* 1 = 0.496257 loss)
I0905 01:21:45.259177 73306 sgd_solver.cpp:106] Iteration 1560, lr = 0.1
I0905 01:21:51.428180 73306 solver.cpp:228] Iteration 1570, loss = 0.39874
I0905 01:21:51.428225 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.398739 (* 1 = 0.398739 loss)
I0905 01:21:51.428237 73306 sgd_solver.cpp:106] Iteration 1570, lr = 0.1
I0905 01:21:57.753741 73306 solver.cpp:228] Iteration 1580, loss = 0.28711
I0905 01:21:57.753793 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.28711 (* 1 = 0.28711 loss)
I0905 01:21:57.753806 73306 sgd_solver.cpp:106] Iteration 1580, lr = 0.1
I0905 01:22:03.714134 73306 solver.cpp:228] Iteration 1590, loss = 0.392277
I0905 01:22:03.714184 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.392277 (* 1 = 0.392277 loss)
I0905 01:22:03.714197 73306 sgd_solver.cpp:106] Iteration 1590, lr = 0.1
I0905 01:22:09.646826 73306 solver.cpp:337] Iteration 1600, Testing net (#0)
I0905 01:22:50.639339 73306 solver.cpp:404]     Test net output #0: Accuracy1 = 0.492188
I0905 01:22:50.639487 73306 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 1.37544 (* 1 = 1.37544 loss)
I0905 01:22:51.103926 73306 solver.cpp:228] Iteration 1600, loss = 0.493622
I0905 01:22:51.103957 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.493621 (* 1 = 0.493621 loss)
I0905 01:22:51.103973 73306 sgd_solver.cpp:106] Iteration 1600, lr = 0.1
I0905 01:22:57.166524 73306 solver.cpp:228] Iteration 1610, loss = 0.453971
I0905 01:22:57.166573 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.453971 (* 1 = 0.453971 loss)
I0905 01:22:57.166585 73306 sgd_solver.cpp:106] Iteration 1610, lr = 0.1
I0905 01:23:02.939893 73306 solver.cpp:228] Iteration 1620, loss = 0.423704
I0905 01:23:02.939939 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.423704 (* 1 = 0.423704 loss)
I0905 01:23:02.939954 73306 sgd_solver.cpp:106] Iteration 1620, lr = 0.1
I0905 01:23:09.176262 73306 solver.cpp:228] Iteration 1630, loss = 0.513694
I0905 01:23:09.176324 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.513693 (* 1 = 0.513693 loss)
I0905 01:23:09.176339 73306 sgd_solver.cpp:106] Iteration 1630, lr = 0.1
I0905 01:23:15.451302 73306 solver.cpp:228] Iteration 1640, loss = 0.504525
I0905 01:23:15.451346 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.504525 (* 1 = 0.504525 loss)
I0905 01:23:15.451359 73306 sgd_solver.cpp:106] Iteration 1640, lr = 0.1
I0905 01:23:21.534322 73306 solver.cpp:228] Iteration 1650, loss = 0.726459
I0905 01:23:21.534519 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.726459 (* 1 = 0.726459 loss)
I0905 01:23:21.534539 73306 sgd_solver.cpp:106] Iteration 1650, lr = 0.1
I0905 01:23:27.602032 73306 solver.cpp:228] Iteration 1660, loss = 0.506222
I0905 01:23:27.602085 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.506221 (* 1 = 0.506221 loss)
I0905 01:23:27.602099 73306 sgd_solver.cpp:106] Iteration 1660, lr = 0.1
I0905 01:23:33.691629 73306 solver.cpp:228] Iteration 1670, loss = 0.376844
I0905 01:23:33.691675 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.376844 (* 1 = 0.376844 loss)
I0905 01:23:33.691687 73306 sgd_solver.cpp:106] Iteration 1670, lr = 0.1
I0905 01:23:39.769842 73306 solver.cpp:228] Iteration 1680, loss = 0.3715
I0905 01:23:39.769887 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.3715 (* 1 = 0.3715 loss)
I0905 01:23:39.769901 73306 sgd_solver.cpp:106] Iteration 1680, lr = 0.1
I0905 01:23:45.857498 73306 solver.cpp:228] Iteration 1690, loss = 0.221282
I0905 01:23:45.857568 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.221282 (* 1 = 0.221282 loss)
I0905 01:23:45.857580 73306 sgd_solver.cpp:106] Iteration 1690, lr = 0.1
I0905 01:23:52.135035 73306 solver.cpp:228] Iteration 1700, loss = 0.489116
I0905 01:23:52.135223 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.489116 (* 1 = 0.489116 loss)
I0905 01:23:52.135241 73306 sgd_solver.cpp:106] Iteration 1700, lr = 0.1
I0905 01:23:58.341624 73306 solver.cpp:228] Iteration 1710, loss = 0.724512
I0905 01:23:58.341673 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.724511 (* 1 = 0.724511 loss)
I0905 01:23:58.341689 73306 sgd_solver.cpp:106] Iteration 1710, lr = 0.1
I0905 01:24:04.113144 73306 solver.cpp:228] Iteration 1720, loss = 0.422956
I0905 01:24:04.113196 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.422955 (* 1 = 0.422955 loss)
I0905 01:24:04.113210 73306 sgd_solver.cpp:106] Iteration 1720, lr = 0.1
I0905 01:24:10.014199 73306 solver.cpp:228] Iteration 1730, loss = 0.245003
I0905 01:24:10.014257 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.245003 (* 1 = 0.245003 loss)
I0905 01:24:10.014273 73306 sgd_solver.cpp:106] Iteration 1730, lr = 0.1
I0905 01:24:15.591769 73306 solver.cpp:228] Iteration 1740, loss = 0.298271
I0905 01:24:15.591817 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.298271 (* 1 = 0.298271 loss)
I0905 01:24:15.591830 73306 sgd_solver.cpp:106] Iteration 1740, lr = 0.1
I0905 01:24:21.408768 73306 solver.cpp:228] Iteration 1750, loss = 0.642942
I0905 01:24:21.408821 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.642942 (* 1 = 0.642942 loss)
I0905 01:24:21.408834 73306 sgd_solver.cpp:106] Iteration 1750, lr = 0.1
I0905 01:24:27.501803 73306 solver.cpp:228] Iteration 1760, loss = 0.38676
I0905 01:24:27.501929 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.38676 (* 1 = 0.38676 loss)
I0905 01:24:27.501956 73306 sgd_solver.cpp:106] Iteration 1760, lr = 0.1
I0905 01:24:33.405467 73306 solver.cpp:228] Iteration 1770, loss = 0.440401
I0905 01:24:33.405515 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.4404 (* 1 = 0.4404 loss)
I0905 01:24:33.405529 73306 sgd_solver.cpp:106] Iteration 1770, lr = 0.1
I0905 01:24:39.358387 73306 solver.cpp:228] Iteration 1780, loss = 0.53013
I0905 01:24:39.358427 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.53013 (* 1 = 0.53013 loss)
I0905 01:24:39.358438 73306 sgd_solver.cpp:106] Iteration 1780, lr = 0.1
I0905 01:24:45.468619 73306 solver.cpp:228] Iteration 1790, loss = 0.502158
I0905 01:24:45.468668 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.502158 (* 1 = 0.502158 loss)
I0905 01:24:45.468680 73306 sgd_solver.cpp:106] Iteration 1790, lr = 0.1
I0905 01:24:51.878239 73306 solver.cpp:228] Iteration 1800, loss = 0.401421
I0905 01:24:51.878284 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.401421 (* 1 = 0.401421 loss)
I0905 01:24:51.878298 73306 sgd_solver.cpp:106] Iteration 1800, lr = 0.1
I0905 01:24:57.941112 73306 solver.cpp:228] Iteration 1810, loss = 0.360485
I0905 01:24:57.941397 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.360485 (* 1 = 0.360485 loss)
I0905 01:24:57.941411 73306 sgd_solver.cpp:106] Iteration 1810, lr = 0.1
I0905 01:25:04.220165 73306 solver.cpp:228] Iteration 1820, loss = 0.346243
I0905 01:25:04.220212 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.346242 (* 1 = 0.346242 loss)
I0905 01:25:04.220224 73306 sgd_solver.cpp:106] Iteration 1820, lr = 0.1
I0905 01:25:10.404042 73306 solver.cpp:228] Iteration 1830, loss = 0.301811
I0905 01:25:10.404090 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.301811 (* 1 = 0.301811 loss)
I0905 01:25:10.404101 73306 sgd_solver.cpp:106] Iteration 1830, lr = 0.1
I0905 01:25:16.516255 73306 solver.cpp:228] Iteration 1840, loss = 0.407691
I0905 01:25:16.516306 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.40769 (* 1 = 0.40769 loss)
I0905 01:25:16.516322 73306 sgd_solver.cpp:106] Iteration 1840, lr = 0.1
I0905 01:25:22.605041 73306 solver.cpp:228] Iteration 1850, loss = 0.643185
I0905 01:25:22.605087 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.643184 (* 1 = 0.643184 loss)
I0905 01:25:22.605101 73306 sgd_solver.cpp:106] Iteration 1850, lr = 0.1
I0905 01:25:28.722311 73306 solver.cpp:228] Iteration 1860, loss = 0.54082
I0905 01:25:28.722420 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.54082 (* 1 = 0.54082 loss)
I0905 01:25:28.722434 73306 sgd_solver.cpp:106] Iteration 1860, lr = 0.1
I0905 01:25:34.831092 73306 solver.cpp:228] Iteration 1870, loss = 0.507438
I0905 01:25:34.831146 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.507438 (* 1 = 0.507438 loss)
I0905 01:25:34.831159 73306 sgd_solver.cpp:106] Iteration 1870, lr = 0.1
I0905 01:25:40.909874 73306 solver.cpp:228] Iteration 1880, loss = 0.461993
I0905 01:25:40.909921 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.461993 (* 1 = 0.461993 loss)
I0905 01:25:40.909935 73306 sgd_solver.cpp:106] Iteration 1880, lr = 0.1
I0905 01:25:46.984736 73306 solver.cpp:228] Iteration 1890, loss = 0.370145
I0905 01:25:46.984786 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.370145 (* 1 = 0.370145 loss)
I0905 01:25:46.984798 73306 sgd_solver.cpp:106] Iteration 1890, lr = 0.1
I0905 01:25:53.084347 73306 solver.cpp:228] Iteration 1900, loss = 0.452054
I0905 01:25:53.084393 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.452054 (* 1 = 0.452054 loss)
I0905 01:25:53.084405 73306 sgd_solver.cpp:106] Iteration 1900, lr = 0.1
I0905 01:25:58.736124 73306 solver.cpp:228] Iteration 1910, loss = 0.544221
I0905 01:25:58.736302 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.544221 (* 1 = 0.544221 loss)
I0905 01:25:58.736349 73306 sgd_solver.cpp:106] Iteration 1910, lr = 0.1
I0905 01:26:04.334657 73306 solver.cpp:228] Iteration 1920, loss = 0.880157
I0905 01:26:04.334702 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.880156 (* 1 = 0.880156 loss)
I0905 01:26:04.334717 73306 sgd_solver.cpp:106] Iteration 1920, lr = 0.1
I0905 01:26:10.429975 73306 solver.cpp:228] Iteration 1930, loss = 0.720792
I0905 01:26:10.430027 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.720792 (* 1 = 0.720792 loss)
I0905 01:26:10.430040 73306 sgd_solver.cpp:106] Iteration 1930, lr = 0.1
I0905 01:26:16.492594 73306 solver.cpp:228] Iteration 1940, loss = 0.50839
I0905 01:26:16.492643 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.508389 (* 1 = 0.508389 loss)
I0905 01:26:16.492657 73306 sgd_solver.cpp:106] Iteration 1940, lr = 0.1
I0905 01:26:22.585489 73306 solver.cpp:228] Iteration 1950, loss = 0.226506
I0905 01:26:22.585535 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.226506 (* 1 = 0.226506 loss)
I0905 01:26:22.585546 73306 sgd_solver.cpp:106] Iteration 1950, lr = 0.1
I0905 01:26:28.957758 73306 solver.cpp:228] Iteration 1960, loss = 0.248823
I0905 01:26:28.958015 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.248822 (* 1 = 0.248822 loss)
I0905 01:26:28.958036 73306 sgd_solver.cpp:106] Iteration 1960, lr = 0.1
I0905 01:26:35.027796 73306 solver.cpp:228] Iteration 1970, loss = 1.02614
I0905 01:26:35.027845 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.02614 (* 1 = 1.02614 loss)
I0905 01:26:35.027858 73306 sgd_solver.cpp:106] Iteration 1970, lr = 0.1
I0905 01:26:41.179775 73306 solver.cpp:228] Iteration 1980, loss = 0.434641
I0905 01:26:41.179826 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.434641 (* 1 = 0.434641 loss)
I0905 01:26:41.179838 73306 sgd_solver.cpp:106] Iteration 1980, lr = 0.1
I0905 01:26:47.177980 73306 solver.cpp:228] Iteration 1990, loss = 0.377548
I0905 01:26:47.178028 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.377548 (* 1 = 0.377548 loss)
I0905 01:26:47.178040 73306 sgd_solver.cpp:106] Iteration 1990, lr = 0.1
I0905 01:26:53.236302 73306 solver.cpp:228] Iteration 2000, loss = 0.593899
I0905 01:26:53.236354 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.593898 (* 1 = 0.593898 loss)
I0905 01:26:53.236366 73306 sgd_solver.cpp:106] Iteration 2000, lr = 0.1
I0905 01:26:59.102474 73306 solver.cpp:228] Iteration 2010, loss = 0.395232
I0905 01:26:59.102685 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.395231 (* 1 = 0.395231 loss)
I0905 01:26:59.102710 73306 sgd_solver.cpp:106] Iteration 2010, lr = 0.1
I0905 01:27:05.724042 73306 solver.cpp:228] Iteration 2020, loss = 0.376068
I0905 01:27:05.724086 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.376068 (* 1 = 0.376068 loss)
I0905 01:27:05.724098 73306 sgd_solver.cpp:106] Iteration 2020, lr = 0.1
I0905 01:27:11.472630 73306 solver.cpp:228] Iteration 2030, loss = 0.287115
I0905 01:27:11.472673 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.287114 (* 1 = 0.287114 loss)
I0905 01:27:11.472686 73306 sgd_solver.cpp:106] Iteration 2030, lr = 0.1
I0905 01:27:17.857209 73306 solver.cpp:228] Iteration 2040, loss = 0.539636
I0905 01:27:17.857254 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.539635 (* 1 = 0.539635 loss)
I0905 01:27:17.857265 73306 sgd_solver.cpp:106] Iteration 2040, lr = 0.1
I0905 01:27:24.241299 73306 solver.cpp:228] Iteration 2050, loss = 0.573955
I0905 01:27:24.241354 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.573955 (* 1 = 0.573955 loss)
I0905 01:27:24.241367 73306 sgd_solver.cpp:106] Iteration 2050, lr = 0.1
I0905 01:27:30.306799 73306 solver.cpp:228] Iteration 2060, loss = 0.288904
I0905 01:27:30.306998 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.288904 (* 1 = 0.288904 loss)
I0905 01:27:30.307029 73306 sgd_solver.cpp:106] Iteration 2060, lr = 0.1
I0905 01:27:36.367913 73306 solver.cpp:228] Iteration 2070, loss = 0.346872
I0905 01:27:36.367961 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.346872 (* 1 = 0.346872 loss)
I0905 01:27:36.367974 73306 sgd_solver.cpp:106] Iteration 2070, lr = 0.1
I0905 01:27:42.633780 73306 solver.cpp:228] Iteration 2080, loss = 0.667045
I0905 01:27:42.633834 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.667045 (* 1 = 0.667045 loss)
I0905 01:27:42.633849 73306 sgd_solver.cpp:106] Iteration 2080, lr = 0.1
I0905 01:27:48.089113 73306 solver.cpp:228] Iteration 2090, loss = 0.577743
I0905 01:27:48.089159 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.577742 (* 1 = 0.577742 loss)
I0905 01:27:48.089170 73306 sgd_solver.cpp:106] Iteration 2090, lr = 0.1
I0905 01:27:53.667868 73306 solver.cpp:228] Iteration 2100, loss = 0.322185
I0905 01:27:53.667915 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.322184 (* 1 = 0.322184 loss)
I0905 01:27:53.667927 73306 sgd_solver.cpp:106] Iteration 2100, lr = 0.1
I0905 01:27:59.697396 73306 solver.cpp:228] Iteration 2110, loss = 0.315176
I0905 01:27:59.697444 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.315176 (* 1 = 0.315176 loss)
I0905 01:27:59.697455 73306 sgd_solver.cpp:106] Iteration 2110, lr = 0.1
I0905 01:28:06.119364 73306 solver.cpp:228] Iteration 2120, loss = 0.485575
I0905 01:28:06.119606 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.485575 (* 1 = 0.485575 loss)
I0905 01:28:06.119622 73306 sgd_solver.cpp:106] Iteration 2120, lr = 0.1
I0905 01:28:12.189354 73306 solver.cpp:228] Iteration 2130, loss = 0.632717
I0905 01:28:12.189399 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.632717 (* 1 = 0.632717 loss)
I0905 01:28:12.189414 73306 sgd_solver.cpp:106] Iteration 2130, lr = 0.1
I0905 01:28:18.283102 73306 solver.cpp:228] Iteration 2140, loss = 0.319258
I0905 01:28:18.283145 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.319258 (* 1 = 0.319258 loss)
I0905 01:28:18.283161 73306 sgd_solver.cpp:106] Iteration 2140, lr = 0.1
I0905 01:28:24.342149 73306 solver.cpp:228] Iteration 2150, loss = 0.33076
I0905 01:28:24.342196 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.33076 (* 1 = 0.33076 loss)
I0905 01:28:24.342207 73306 sgd_solver.cpp:106] Iteration 2150, lr = 0.1
I0905 01:28:30.411046 73306 solver.cpp:228] Iteration 2160, loss = 0.803074
I0905 01:28:30.411092 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.803074 (* 1 = 0.803074 loss)
I0905 01:28:30.411106 73306 sgd_solver.cpp:106] Iteration 2160, lr = 0.1
I0905 01:28:36.780783 73306 solver.cpp:228] Iteration 2170, loss = 0.437857
I0905 01:28:36.780946 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.437857 (* 1 = 0.437857 loss)
I0905 01:28:36.781000 73306 sgd_solver.cpp:106] Iteration 2170, lr = 0.1
I0905 01:28:42.793822 73306 solver.cpp:228] Iteration 2180, loss = 0.594246
I0905 01:28:42.793872 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.594246 (* 1 = 0.594246 loss)
I0905 01:28:42.793889 73306 sgd_solver.cpp:106] Iteration 2180, lr = 0.1
I0905 01:28:48.980103 73306 solver.cpp:228] Iteration 2190, loss = 0.520589
I0905 01:28:48.980150 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.520589 (* 1 = 0.520589 loss)
I0905 01:28:48.980162 73306 sgd_solver.cpp:106] Iteration 2190, lr = 0.1
I0905 01:28:55.054271 73306 solver.cpp:228] Iteration 2200, loss = 0.484138
I0905 01:28:55.054312 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.484137 (* 1 = 0.484137 loss)
I0905 01:28:55.054327 73306 sgd_solver.cpp:106] Iteration 2200, lr = 0.1
I0905 01:29:01.378996 73306 solver.cpp:228] Iteration 2210, loss = 0.652098
I0905 01:29:01.379056 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.652098 (* 1 = 0.652098 loss)
I0905 01:29:01.379073 73306 sgd_solver.cpp:106] Iteration 2210, lr = 0.1
I0905 01:29:07.516856 73306 solver.cpp:228] Iteration 2220, loss = 0.317312
I0905 01:29:07.517007 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.317312 (* 1 = 0.317312 loss)
I0905 01:29:07.517060 73306 sgd_solver.cpp:106] Iteration 2220, lr = 0.1
I0905 01:29:13.576064 73306 solver.cpp:228] Iteration 2230, loss = 0.45036
I0905 01:29:13.576109 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.45036 (* 1 = 0.45036 loss)
I0905 01:29:13.576122 73306 sgd_solver.cpp:106] Iteration 2230, lr = 0.1
I0905 01:29:19.682497 73306 solver.cpp:228] Iteration 2240, loss = 0.642239
I0905 01:29:19.682548 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.642238 (* 1 = 0.642238 loss)
I0905 01:29:19.682559 73306 sgd_solver.cpp:106] Iteration 2240, lr = 0.1
I0905 01:29:25.746840 73306 solver.cpp:228] Iteration 2250, loss = 0.806526
I0905 01:29:25.746891 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.806526 (* 1 = 0.806526 loss)
I0905 01:29:25.746902 73306 sgd_solver.cpp:106] Iteration 2250, lr = 0.1
I0905 01:29:31.823096 73306 solver.cpp:228] Iteration 2260, loss = 0.402602
I0905 01:29:31.823137 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.402602 (* 1 = 0.402602 loss)
I0905 01:29:31.823149 73306 sgd_solver.cpp:106] Iteration 2260, lr = 0.1
I0905 01:29:37.449167 73306 solver.cpp:228] Iteration 2270, loss = 0.231783
I0905 01:29:37.449213 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.231782 (* 1 = 0.231782 loss)
I0905 01:29:37.449225 73306 sgd_solver.cpp:106] Iteration 2270, lr = 0.1
I0905 01:29:42.808575 73306 solver.cpp:228] Iteration 2280, loss = 0.933945
I0905 01:29:42.808820 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.933945 (* 1 = 0.933945 loss)
I0905 01:29:42.808850 73306 sgd_solver.cpp:106] Iteration 2280, lr = 0.1
I0905 01:29:48.932931 73306 solver.cpp:228] Iteration 2290, loss = 0.623669
I0905 01:29:48.932988 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.623669 (* 1 = 0.623669 loss)
I0905 01:29:48.933001 73306 sgd_solver.cpp:106] Iteration 2290, lr = 0.1
I0905 01:29:55.032894 73306 solver.cpp:228] Iteration 2300, loss = 0.540439
I0905 01:29:55.032964 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.540438 (* 1 = 0.540438 loss)
I0905 01:29:55.032979 73306 sgd_solver.cpp:106] Iteration 2300, lr = 0.1
I0905 01:30:00.916386 73306 solver.cpp:228] Iteration 2310, loss = 0.279908
I0905 01:30:00.916437 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.279908 (* 1 = 0.279908 loss)
I0905 01:30:00.916451 73306 sgd_solver.cpp:106] Iteration 2310, lr = 0.1
I0905 01:30:07.120812 73306 solver.cpp:228] Iteration 2320, loss = 0.297121
I0905 01:30:07.120859 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.29712 (* 1 = 0.29712 loss)
I0905 01:30:07.120872 73306 sgd_solver.cpp:106] Iteration 2320, lr = 0.1
I0905 01:30:13.216634 73306 solver.cpp:228] Iteration 2330, loss = 0.704038
I0905 01:30:13.216748 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.704038 (* 1 = 0.704038 loss)
I0905 01:30:13.216763 73306 sgd_solver.cpp:106] Iteration 2330, lr = 0.1
I0905 01:30:19.315474 73306 solver.cpp:228] Iteration 2340, loss = 0.268852
I0905 01:30:19.315521 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.268852 (* 1 = 0.268852 loss)
I0905 01:30:19.315534 73306 sgd_solver.cpp:106] Iteration 2340, lr = 0.1
I0905 01:30:25.414784 73306 solver.cpp:228] Iteration 2350, loss = 0.510169
I0905 01:30:25.414827 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.510169 (* 1 = 0.510169 loss)
I0905 01:30:25.414844 73306 sgd_solver.cpp:106] Iteration 2350, lr = 0.1
I0905 01:30:31.490694 73306 solver.cpp:228] Iteration 2360, loss = 0.365888
I0905 01:30:31.490784 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.365887 (* 1 = 0.365887 loss)
I0905 01:30:31.490804 73306 sgd_solver.cpp:106] Iteration 2360, lr = 0.1
I0905 01:30:37.872853 73306 solver.cpp:228] Iteration 2370, loss = 0.435616
I0905 01:30:37.872913 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.435616 (* 1 = 0.435616 loss)
I0905 01:30:37.872926 73306 sgd_solver.cpp:106] Iteration 2370, lr = 0.1
I0905 01:30:43.966429 73306 solver.cpp:228] Iteration 2380, loss = 0.43902
I0905 01:30:43.966573 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.43902 (* 1 = 0.43902 loss)
I0905 01:30:43.966620 73306 sgd_solver.cpp:106] Iteration 2380, lr = 0.1
I0905 01:30:50.062602 73306 solver.cpp:228] Iteration 2390, loss = 0.500168
I0905 01:30:50.062659 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.500168 (* 1 = 0.500168 loss)
I0905 01:30:50.062675 73306 sgd_solver.cpp:106] Iteration 2390, lr = 0.1
I0905 01:30:55.908905 73306 solver.cpp:337] Iteration 2400, Testing net (#0)
I0905 01:31:36.998772 73306 solver.cpp:404]     Test net output #0: Accuracy1 = 0.484375
I0905 01:31:36.998917 73306 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 2.33982 (* 1 = 2.33982 loss)
I0905 01:31:37.215677 73306 solver.cpp:228] Iteration 2400, loss = 0.322926
I0905 01:31:37.215705 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.322925 (* 1 = 0.322925 loss)
I0905 01:31:37.215724 73306 sgd_solver.cpp:106] Iteration 2400, lr = 0.1
I0905 01:31:43.274869 73306 solver.cpp:228] Iteration 2410, loss = 0.589312
I0905 01:31:43.274924 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.589312 (* 1 = 0.589312 loss)
I0905 01:31:43.274937 73306 sgd_solver.cpp:106] Iteration 2410, lr = 0.1
I0905 01:31:49.392398 73306 solver.cpp:228] Iteration 2420, loss = 0.728153
I0905 01:31:49.392453 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.728153 (* 1 = 0.728153 loss)
I0905 01:31:49.392468 73306 sgd_solver.cpp:106] Iteration 2420, lr = 0.1
I0905 01:31:55.472825 73306 solver.cpp:228] Iteration 2430, loss = 0.383888
I0905 01:31:55.472869 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.383888 (* 1 = 0.383888 loss)
I0905 01:31:55.472880 73306 sgd_solver.cpp:106] Iteration 2430, lr = 0.1
I0905 01:32:01.594218 73306 solver.cpp:228] Iteration 2440, loss = 0.510234
I0905 01:32:01.594264 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.510234 (* 1 = 0.510234 loss)
I0905 01:32:01.594275 73306 sgd_solver.cpp:106] Iteration 2440, lr = 0.1
I0905 01:32:07.682392 73306 solver.cpp:228] Iteration 2450, loss = 0.577378
I0905 01:32:07.682622 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.577378 (* 1 = 0.577378 loss)
I0905 01:32:07.682654 73306 sgd_solver.cpp:106] Iteration 2450, lr = 0.1
I0905 01:32:13.767144 73306 solver.cpp:228] Iteration 2460, loss = 0.202173
I0905 01:32:13.767189 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.202173 (* 1 = 0.202173 loss)
I0905 01:32:13.767202 73306 sgd_solver.cpp:106] Iteration 2460, lr = 0.1
I0905 01:32:19.917899 73306 solver.cpp:228] Iteration 2470, loss = 0.302183
I0905 01:32:19.917940 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.302183 (* 1 = 0.302183 loss)
I0905 01:32:19.917954 73306 sgd_solver.cpp:106] Iteration 2470, lr = 0.1
I0905 01:32:25.674543 73306 solver.cpp:228] Iteration 2480, loss = 0.6646
I0905 01:32:25.674581 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.664599 (* 1 = 0.664599 loss)
I0905 01:32:25.674595 73306 sgd_solver.cpp:106] Iteration 2480, lr = 0.1
I0905 01:32:32.081135 73306 solver.cpp:228] Iteration 2490, loss = 0.386923
I0905 01:32:32.081185 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.386923 (* 1 = 0.386923 loss)
I0905 01:32:32.081198 73306 sgd_solver.cpp:106] Iteration 2490, lr = 0.1
I0905 01:32:38.169301 73306 solver.cpp:228] Iteration 2500, loss = 0.513969
I0905 01:32:38.169451 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.513969 (* 1 = 0.513969 loss)
I0905 01:32:38.169507 73306 sgd_solver.cpp:106] Iteration 2500, lr = 0.1
I0905 01:32:44.560398 73306 solver.cpp:228] Iteration 2510, loss = 0.789019
I0905 01:32:44.560446 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.789019 (* 1 = 0.789019 loss)
I0905 01:32:44.560461 73306 sgd_solver.cpp:106] Iteration 2510, lr = 0.1
I0905 01:32:50.611898 73306 solver.cpp:228] Iteration 2520, loss = 0.614702
I0905 01:32:50.611939 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.614702 (* 1 = 0.614702 loss)
I0905 01:32:50.611953 73306 sgd_solver.cpp:106] Iteration 2520, lr = 0.1
I0905 01:32:56.657410 73306 solver.cpp:228] Iteration 2530, loss = 0.578364
I0905 01:32:56.657481 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.578364 (* 1 = 0.578364 loss)
I0905 01:32:56.657501 73306 sgd_solver.cpp:106] Iteration 2530, lr = 0.1
I0905 01:33:02.847885 73306 solver.cpp:228] Iteration 2540, loss = 0.345828
I0905 01:33:02.847944 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.345827 (* 1 = 0.345827 loss)
I0905 01:33:02.847959 73306 sgd_solver.cpp:106] Iteration 2540, lr = 0.1
I0905 01:33:08.635951 73306 solver.cpp:228] Iteration 2550, loss = 0.369018
I0905 01:33:08.636211 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.369018 (* 1 = 0.369018 loss)
I0905 01:33:08.636229 73306 sgd_solver.cpp:106] Iteration 2550, lr = 0.1
I0905 01:33:14.232434 73306 solver.cpp:228] Iteration 2560, loss = 0.715157
I0905 01:33:14.232482 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.715157 (* 1 = 0.715157 loss)
I0905 01:33:14.232496 73306 sgd_solver.cpp:106] Iteration 2560, lr = 0.1
I0905 01:33:19.788210 73306 solver.cpp:228] Iteration 2570, loss = 0.532125
I0905 01:33:19.788261 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.532124 (* 1 = 0.532124 loss)
I0905 01:33:19.788274 73306 sgd_solver.cpp:106] Iteration 2570, lr = 0.1
I0905 01:33:26.166445 73306 solver.cpp:228] Iteration 2580, loss = 0.34542
I0905 01:33:26.166489 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.34542 (* 1 = 0.34542 loss)
I0905 01:33:26.166507 73306 sgd_solver.cpp:106] Iteration 2580, lr = 0.1
I0905 01:33:32.238976 73306 solver.cpp:228] Iteration 2590, loss = 0.53885
I0905 01:33:32.239017 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.53885 (* 1 = 0.53885 loss)
I0905 01:33:32.239028 73306 sgd_solver.cpp:106] Iteration 2590, lr = 0.1
I0905 01:33:38.352110 73306 solver.cpp:228] Iteration 2600, loss = 0.571911
I0905 01:33:38.352155 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.57191 (* 1 = 0.57191 loss)
I0905 01:33:38.352169 73306 sgd_solver.cpp:106] Iteration 2600, lr = 0.1
I0905 01:33:44.417567 73306 solver.cpp:228] Iteration 2610, loss = 0.724089
I0905 01:33:44.417784 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.724089 (* 1 = 0.724089 loss)
I0905 01:33:44.417814 73306 sgd_solver.cpp:106] Iteration 2610, lr = 0.1
I0905 01:33:50.525051 73306 solver.cpp:228] Iteration 2620, loss = 0.321564
I0905 01:33:50.525099 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.321564 (* 1 = 0.321564 loss)
I0905 01:33:50.525113 73306 sgd_solver.cpp:106] Iteration 2620, lr = 0.1
I0905 01:33:56.947779 73306 solver.cpp:228] Iteration 2630, loss = 0.394171
I0905 01:33:56.947827 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.394171 (* 1 = 0.394171 loss)
I0905 01:33:56.947841 73306 sgd_solver.cpp:106] Iteration 2630, lr = 0.1
I0905 01:34:03.011353 73306 solver.cpp:228] Iteration 2640, loss = 0.577967
I0905 01:34:03.011394 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.577967 (* 1 = 0.577967 loss)
I0905 01:34:03.011406 73306 sgd_solver.cpp:106] Iteration 2640, lr = 0.1
I0905 01:34:09.080219 73306 solver.cpp:228] Iteration 2650, loss = 0.394977
I0905 01:34:09.080258 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.394977 (* 1 = 0.394977 loss)
I0905 01:34:09.080271 73306 sgd_solver.cpp:106] Iteration 2650, lr = 0.1
I0905 01:34:15.174288 73306 solver.cpp:228] Iteration 2660, loss = 0.528379
I0905 01:34:15.174465 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.528378 (* 1 = 0.528378 loss)
I0905 01:34:15.174479 73306 sgd_solver.cpp:106] Iteration 2660, lr = 0.1
I0905 01:34:21.547313 73306 solver.cpp:228] Iteration 2670, loss = 0.442636
I0905 01:34:21.547377 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.442636 (* 1 = 0.442636 loss)
I0905 01:34:21.547392 73306 sgd_solver.cpp:106] Iteration 2670, lr = 0.1
I0905 01:34:27.648452 73306 solver.cpp:228] Iteration 2680, loss = 0.413618
I0905 01:34:27.648495 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.413617 (* 1 = 0.413617 loss)
I0905 01:34:27.648507 73306 sgd_solver.cpp:106] Iteration 2680, lr = 0.1
I0905 01:34:33.714731 73306 solver.cpp:228] Iteration 2690, loss = 0.197006
I0905 01:34:33.714784 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.197005 (* 1 = 0.197005 loss)
I0905 01:34:33.714798 73306 sgd_solver.cpp:106] Iteration 2690, lr = 0.1
I0905 01:34:39.758146 73306 solver.cpp:228] Iteration 2700, loss = 0.294081
I0905 01:34:39.758199 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.29408 (* 1 = 0.29408 loss)
I0905 01:34:39.758213 73306 sgd_solver.cpp:106] Iteration 2700, lr = 0.1
I0905 01:34:45.837106 73306 solver.cpp:228] Iteration 2710, loss = 0.505824
I0905 01:34:45.837278 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.505823 (* 1 = 0.505823 loss)
I0905 01:34:45.837298 73306 sgd_solver.cpp:106] Iteration 2710, lr = 0.1
I0905 01:34:52.197624 73306 solver.cpp:228] Iteration 2720, loss = 0.419829
I0905 01:34:52.197665 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.419829 (* 1 = 0.419829 loss)
I0905 01:34:52.197679 73306 sgd_solver.cpp:106] Iteration 2720, lr = 0.1
I0905 01:34:57.964256 73306 solver.cpp:228] Iteration 2730, loss = 0.588664
I0905 01:34:57.964309 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.588664 (* 1 = 0.588664 loss)
I0905 01:34:57.964323 73306 sgd_solver.cpp:106] Iteration 2730, lr = 0.1
I0905 01:35:03.232110 73306 solver.cpp:228] Iteration 2740, loss = 0.680604
I0905 01:35:03.232161 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.680604 (* 1 = 0.680604 loss)
I0905 01:35:03.232175 73306 sgd_solver.cpp:106] Iteration 2740, lr = 0.1
I0905 01:35:08.996000 73306 solver.cpp:228] Iteration 2750, loss = 0.423467
I0905 01:35:08.996037 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.423467 (* 1 = 0.423467 loss)
I0905 01:35:08.996048 73306 sgd_solver.cpp:106] Iteration 2750, lr = 0.1
I0905 01:35:15.058360 73306 solver.cpp:228] Iteration 2760, loss = 0.842048
I0905 01:35:15.058415 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.842048 (* 1 = 0.842048 loss)
I0905 01:35:15.058429 73306 sgd_solver.cpp:106] Iteration 2760, lr = 0.1
I0905 01:35:21.450354 73306 solver.cpp:228] Iteration 2770, loss = 0.415103
I0905 01:35:21.450541 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.415103 (* 1 = 0.415103 loss)
I0905 01:35:21.450570 73306 sgd_solver.cpp:106] Iteration 2770, lr = 0.1
I0905 01:35:27.503368 73306 solver.cpp:228] Iteration 2780, loss = 0.466831
I0905 01:35:27.503422 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.466831 (* 1 = 0.466831 loss)
I0905 01:35:27.503433 73306 sgd_solver.cpp:106] Iteration 2780, lr = 0.1
I0905 01:35:33.589633 73306 solver.cpp:228] Iteration 2790, loss = 0.32553
I0905 01:35:33.589697 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.32553 (* 1 = 0.32553 loss)
I0905 01:35:33.589712 73306 sgd_solver.cpp:106] Iteration 2790, lr = 0.1
I0905 01:35:39.993791 73306 solver.cpp:228] Iteration 2800, loss = 1.10599
I0905 01:35:39.993847 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.10599 (* 1 = 1.10599 loss)
I0905 01:35:39.993861 73306 sgd_solver.cpp:106] Iteration 2800, lr = 0.1
I0905 01:35:45.742020 73306 solver.cpp:228] Iteration 2810, loss = 0.334103
I0905 01:35:45.742074 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.334103 (* 1 = 0.334103 loss)
I0905 01:35:45.742086 73306 sgd_solver.cpp:106] Iteration 2810, lr = 0.1
I0905 01:35:52.103356 73306 solver.cpp:228] Iteration 2820, loss = 0.589256
I0905 01:35:52.103552 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.589256 (* 1 = 0.589256 loss)
I0905 01:35:52.103581 73306 sgd_solver.cpp:106] Iteration 2820, lr = 0.1
I0905 01:35:58.234611 73306 solver.cpp:228] Iteration 2830, loss = 0.348038
I0905 01:35:58.234674 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.348037 (* 1 = 0.348037 loss)
I0905 01:35:58.234688 73306 sgd_solver.cpp:106] Iteration 2830, lr = 0.1
I0905 01:36:04.300986 73306 solver.cpp:228] Iteration 2840, loss = 0.455817
I0905 01:36:04.301033 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.455816 (* 1 = 0.455816 loss)
I0905 01:36:04.301046 73306 sgd_solver.cpp:106] Iteration 2840, lr = 0.1
I0905 01:36:10.367363 73306 solver.cpp:228] Iteration 2850, loss = 0.497366
I0905 01:36:10.367408 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.497365 (* 1 = 0.497365 loss)
I0905 01:36:10.367422 73306 sgd_solver.cpp:106] Iteration 2850, lr = 0.1
I0905 01:36:16.634706 73306 solver.cpp:228] Iteration 2860, loss = 0.366499
I0905 01:36:16.634794 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.366498 (* 1 = 0.366498 loss)
I0905 01:36:16.634812 73306 sgd_solver.cpp:106] Iteration 2860, lr = 0.1
I0905 01:36:22.818744 73306 solver.cpp:228] Iteration 2870, loss = 0.391592
I0905 01:36:22.818974 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.391592 (* 1 = 0.391592 loss)
I0905 01:36:22.819011 73306 sgd_solver.cpp:106] Iteration 2870, lr = 0.1
I0905 01:36:28.899369 73306 solver.cpp:228] Iteration 2880, loss = 1.37109
I0905 01:36:28.899407 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.37109 (* 1 = 1.37109 loss)
I0905 01:36:28.899418 73306 sgd_solver.cpp:106] Iteration 2880, lr = 0.1
I0905 01:36:34.303660 73306 solver.cpp:228] Iteration 2890, loss = 0.388597
I0905 01:36:34.303699 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.388597 (* 1 = 0.388597 loss)
I0905 01:36:34.303710 73306 sgd_solver.cpp:106] Iteration 2890, lr = 0.1
I0905 01:36:39.347138 73306 solver.cpp:228] Iteration 2900, loss = 0.269701
I0905 01:36:39.347178 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.269701 (* 1 = 0.269701 loss)
I0905 01:36:39.347190 73306 sgd_solver.cpp:106] Iteration 2900, lr = 0.1
I0905 01:36:44.186847 73306 solver.cpp:228] Iteration 2910, loss = 0.363032
I0905 01:36:44.186887 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.363031 (* 1 = 0.363031 loss)
I0905 01:36:44.186897 73306 sgd_solver.cpp:106] Iteration 2910, lr = 0.1
I0905 01:36:48.844319 73306 solver.cpp:228] Iteration 2920, loss = 0.679772
I0905 01:36:48.844359 73306 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.679772 (* 1 = 0.679772 loss)
I0905 01:36:48.844372 73306 sgd_solver.cpp:106] Iteration 2920, lr = 0.1
I0905 01:36:53.491986 73306 solver.cpp:228] Iteration 2930, loss = 0.404681
