Log file created at: 2016/07/26 14:01:02
Running on machine: deepath-01
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0726 14:01:02.819591 136306 caffe.cpp:184] Using GPUs 3
I0726 14:01:03.082664 136306 solver.cpp:47] Initializing solver from parameters: 
test_iter: 10
test_interval: 500
base_lr: 0.01
display: 10
max_iter: 80000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 40000
snapshot: 1000
snapshot_prefix: "models/gnet"
solver_mode: CPU
device_id: 3
net: "train_val.prototxt"
test_initialization: true
I0726 14:01:03.082913 136306 solver.cpp:90] Creating training net from net file: train_val.prototxt
I0726 14:01:03.083479 136306 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0726 14:01:03.083523 136306 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0726 14:01:03.083721 136306 net.cpp:49] Initializing net from parameters: 
name: "Net"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  python_param {
    module: "mask_image_layer"
    layer: "random_patches_from_images_withlabel"
    param_str: "{\'root_folder\': \'/home/dywang/Proliferation/data/mitoses\', \'image_list\': \'/home/dywang/00exp_wdy/stage04_mitosisDetection/step02_train_mitosis_detecotor/heatmap_mc13_tr/all_image_withlabel_mc13_tr.lst\', \'seed\': 8899, \'mean\': (128, 128, 128), \'size\': 64, \'batch\': 128, \'scale\':0.1, \'colorn\':20, \'classes\':\'1:0 2:1 3:0\', \'DEBUG\': True}"
  }
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "data"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv12"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv21"
  type: "Convolution"
  bottom: "pool1"
  top: "conv21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu21"
  type: "ReLU"
  bottom: "conv21"
  top: "conv21"
}
layer {
  name: "conv22"
  type: "Convolution"
  bottom: "conv21"
  top: "conv22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu22"
  type: "ReLU"
  bottom: "conv22"
  top: "conv22"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv22"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv31"
  type: "Convolution"
  bottom: "pool2"
  top: "conv31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu31"
  type: "ReLU"
  bottom: "conv31"
  top: "conv31"
}
layer {
  name: "conv32"
  type: "Convolution"
  bottom: "conv31"
  top: "conv32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu32"
  type: "ReLU"
  bottom: "conv32"
  top: "conv32"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv32"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "drop"
  type: "Dropout"
  bottom: "conv4"
  top: "conv4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv_c"
  type: "Convolution"
  bottom: "conv4"
  top: "conv_c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 1
    kernel_w: 1
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv_c"
  bottom: "label"
  top: "loss"
}
I0726 14:01:03.085304 136306 layer_factory.hpp:76] Creating layer data
I0726 14:01:08.483398 136306 net.cpp:106] Creating Layer data
I0726 14:01:08.483448 136306 net.cpp:411] data -> data
I0726 14:01:08.483477 136306 net.cpp:411] data -> label
I0726 14:01:11.183009 136306 net.cpp:150] Setting up data
I0726 14:01:11.183071 136306 net.cpp:157] Top shape: 128 3 64 64 (1572864)
I0726 14:01:11.183082 136306 net.cpp:157] Top shape: 128 1 1 1 (128)
I0726 14:01:11.183089 136306 net.cpp:165] Memory required for data: 6291968
I0726 14:01:11.183110 136306 layer_factory.hpp:76] Creating layer conv11
I0726 14:01:11.183146 136306 net.cpp:106] Creating Layer conv11
I0726 14:01:11.183158 136306 net.cpp:454] conv11 <- data
I0726 14:01:11.183178 136306 net.cpp:411] conv11 -> conv11
I0726 14:01:11.286567 136306 net.cpp:150] Setting up conv11
I0726 14:01:11.286613 136306 net.cpp:157] Top shape: 128 32 62 62 (15745024)
I0726 14:01:11.286623 136306 net.cpp:165] Memory required for data: 69272064
I0726 14:01:11.286654 136306 layer_factory.hpp:76] Creating layer relu11
I0726 14:01:11.286679 136306 net.cpp:106] Creating Layer relu11
I0726 14:01:11.286689 136306 net.cpp:454] relu11 <- conv11
I0726 14:01:11.286701 136306 net.cpp:397] relu11 -> conv11 (in-place)
I0726 14:01:11.286860 136306 net.cpp:150] Setting up relu11
I0726 14:01:11.286876 136306 net.cpp:157] Top shape: 128 32 62 62 (15745024)
I0726 14:01:11.286883 136306 net.cpp:165] Memory required for data: 132252160
I0726 14:01:11.286891 136306 layer_factory.hpp:76] Creating layer conv12
I0726 14:01:11.286906 136306 net.cpp:106] Creating Layer conv12
I0726 14:01:11.286914 136306 net.cpp:454] conv12 <- conv11
I0726 14:01:11.286926 136306 net.cpp:411] conv12 -> conv12
I0726 14:01:11.288427 136306 net.cpp:150] Setting up conv12
I0726 14:01:11.288449 136306 net.cpp:157] Top shape: 128 32 60 60 (14745600)
I0726 14:01:11.288457 136306 net.cpp:165] Memory required for data: 191234560
I0726 14:01:11.288470 136306 layer_factory.hpp:76] Creating layer relu12
I0726 14:01:11.288480 136306 net.cpp:106] Creating Layer relu12
I0726 14:01:11.288488 136306 net.cpp:454] relu12 <- conv12
I0726 14:01:11.288502 136306 net.cpp:397] relu12 -> conv12 (in-place)
I0726 14:01:11.288790 136306 net.cpp:150] Setting up relu12
I0726 14:01:11.288808 136306 net.cpp:157] Top shape: 128 32 60 60 (14745600)
I0726 14:01:11.288815 136306 net.cpp:165] Memory required for data: 250216960
I0726 14:01:11.288823 136306 layer_factory.hpp:76] Creating layer pool1
I0726 14:01:11.288836 136306 net.cpp:106] Creating Layer pool1
I0726 14:01:11.288842 136306 net.cpp:454] pool1 <- conv12
I0726 14:01:11.288883 136306 net.cpp:411] pool1 -> pool1
I0726 14:01:11.289075 136306 net.cpp:150] Setting up pool1
I0726 14:01:11.289091 136306 net.cpp:157] Top shape: 128 32 30 30 (3686400)
I0726 14:01:11.289098 136306 net.cpp:165] Memory required for data: 264962560
I0726 14:01:11.289105 136306 layer_factory.hpp:76] Creating layer conv21
I0726 14:01:11.289129 136306 net.cpp:106] Creating Layer conv21
I0726 14:01:11.289137 136306 net.cpp:454] conv21 <- pool1
I0726 14:01:11.289147 136306 net.cpp:411] conv21 -> conv21
I0726 14:01:11.290743 136306 net.cpp:150] Setting up conv21
I0726 14:01:11.290763 136306 net.cpp:157] Top shape: 128 64 28 28 (6422528)
I0726 14:01:11.290771 136306 net.cpp:165] Memory required for data: 290652672
I0726 14:01:11.290788 136306 layer_factory.hpp:76] Creating layer relu21
I0726 14:01:11.290801 136306 net.cpp:106] Creating Layer relu21
I0726 14:01:11.290807 136306 net.cpp:454] relu21 <- conv21
I0726 14:01:11.290815 136306 net.cpp:397] relu21 -> conv21 (in-place)
I0726 14:01:11.291081 136306 net.cpp:150] Setting up relu21
I0726 14:01:11.291110 136306 net.cpp:157] Top shape: 128 64 28 28 (6422528)
I0726 14:01:11.291118 136306 net.cpp:165] Memory required for data: 316342784
I0726 14:01:11.291126 136306 layer_factory.hpp:76] Creating layer conv22
I0726 14:01:11.291144 136306 net.cpp:106] Creating Layer conv22
I0726 14:01:11.291152 136306 net.cpp:454] conv22 <- conv21
I0726 14:01:11.291160 136306 net.cpp:411] conv22 -> conv22
I0726 14:01:11.292135 136306 net.cpp:150] Setting up conv22
I0726 14:01:11.292152 136306 net.cpp:157] Top shape: 128 64 26 26 (5537792)
I0726 14:01:11.292160 136306 net.cpp:165] Memory required for data: 338493952
I0726 14:01:11.292171 136306 layer_factory.hpp:76] Creating layer relu22
I0726 14:01:11.292182 136306 net.cpp:106] Creating Layer relu22
I0726 14:01:11.292191 136306 net.cpp:454] relu22 <- conv22
I0726 14:01:11.292198 136306 net.cpp:397] relu22 -> conv22 (in-place)
I0726 14:01:11.292465 136306 net.cpp:150] Setting up relu22
I0726 14:01:11.292482 136306 net.cpp:157] Top shape: 128 64 26 26 (5537792)
I0726 14:01:11.292490 136306 net.cpp:165] Memory required for data: 360645120
I0726 14:01:11.292497 136306 layer_factory.hpp:76] Creating layer pool2
I0726 14:01:11.292510 136306 net.cpp:106] Creating Layer pool2
I0726 14:01:11.292517 136306 net.cpp:454] pool2 <- conv22
I0726 14:01:11.292526 136306 net.cpp:411] pool2 -> pool2
I0726 14:01:11.292706 136306 net.cpp:150] Setting up pool2
I0726 14:01:11.292721 136306 net.cpp:157] Top shape: 128 64 13 13 (1384448)
I0726 14:01:11.292729 136306 net.cpp:165] Memory required for data: 366182912
I0726 14:01:11.292740 136306 layer_factory.hpp:76] Creating layer conv31
I0726 14:01:11.292757 136306 net.cpp:106] Creating Layer conv31
I0726 14:01:11.292763 136306 net.cpp:454] conv31 <- pool2
I0726 14:01:11.292771 136306 net.cpp:411] conv31 -> conv31
I0726 14:01:11.294651 136306 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 6912
I0726 14:01:11.294850 136306 net.cpp:150] Setting up conv31
I0726 14:01:11.294867 136306 net.cpp:157] Top shape: 128 128 11 11 (1982464)
I0726 14:01:11.294875 136306 net.cpp:165] Memory required for data: 374112768
I0726 14:01:11.294890 136306 layer_factory.hpp:76] Creating layer relu31
I0726 14:01:11.294899 136306 net.cpp:106] Creating Layer relu31
I0726 14:01:11.294911 136306 net.cpp:454] relu31 <- conv31
I0726 14:01:11.294919 136306 net.cpp:397] relu31 -> conv31 (in-place)
I0726 14:01:11.295197 136306 net.cpp:150] Setting up relu31
I0726 14:01:11.295213 136306 net.cpp:157] Top shape: 128 128 11 11 (1982464)
I0726 14:01:11.295220 136306 net.cpp:165] Memory required for data: 382042624
I0726 14:01:11.295228 136306 layer_factory.hpp:76] Creating layer conv32
I0726 14:01:11.295243 136306 net.cpp:106] Creating Layer conv32
I0726 14:01:11.295249 136306 net.cpp:454] conv32 <- conv31
I0726 14:01:11.295258 136306 net.cpp:411] conv32 -> conv32
I0726 14:01:11.296978 136306 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 13824
I0726 14:01:11.297006 136306 net.cpp:150] Setting up conv32
I0726 14:01:11.297034 136306 net.cpp:157] Top shape: 128 128 9 9 (1327104)
I0726 14:01:11.297041 136306 net.cpp:165] Memory required for data: 387351040
I0726 14:01:11.297050 136306 layer_factory.hpp:76] Creating layer relu32
I0726 14:01:11.297068 136306 net.cpp:106] Creating Layer relu32
I0726 14:01:11.297075 136306 net.cpp:454] relu32 <- conv32
I0726 14:01:11.297083 136306 net.cpp:397] relu32 -> conv32 (in-place)
I0726 14:01:11.297233 136306 net.cpp:150] Setting up relu32
I0726 14:01:11.297248 136306 net.cpp:157] Top shape: 128 128 9 9 (1327104)
I0726 14:01:11.297256 136306 net.cpp:165] Memory required for data: 392659456
I0726 14:01:11.297266 136306 layer_factory.hpp:76] Creating layer pool3
I0726 14:01:11.297277 136306 net.cpp:106] Creating Layer pool3
I0726 14:01:11.297286 136306 net.cpp:454] pool3 <- conv32
I0726 14:01:11.297296 136306 net.cpp:411] pool3 -> pool3
I0726 14:01:11.297616 136306 net.cpp:150] Setting up pool3
I0726 14:01:11.297641 136306 net.cpp:157] Top shape: 128 128 3 3 (147456)
I0726 14:01:11.297648 136306 net.cpp:165] Memory required for data: 393249280
I0726 14:01:11.297657 136306 layer_factory.hpp:76] Creating layer conv4
I0726 14:01:11.297672 136306 net.cpp:106] Creating Layer conv4
I0726 14:01:11.297678 136306 net.cpp:454] conv4 <- pool3
I0726 14:01:11.297689 136306 net.cpp:411] conv4 -> conv4
I0726 14:01:11.300779 136306 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 1769472
I0726 14:01:11.300973 136306 net.cpp:150] Setting up conv4
I0726 14:01:11.300992 136306 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0726 14:01:11.301000 136306 net.cpp:165] Memory required for data: 393380352
I0726 14:01:11.301012 136306 layer_factory.hpp:76] Creating layer relu4
I0726 14:01:11.301023 136306 net.cpp:106] Creating Layer relu4
I0726 14:01:11.301029 136306 net.cpp:454] relu4 <- conv4
I0726 14:01:11.301038 136306 net.cpp:397] relu4 -> conv4 (in-place)
I0726 14:01:11.301306 136306 net.cpp:150] Setting up relu4
I0726 14:01:11.301334 136306 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0726 14:01:11.301345 136306 net.cpp:165] Memory required for data: 393511424
I0726 14:01:11.301352 136306 layer_factory.hpp:76] Creating layer drop
I0726 14:01:11.301368 136306 net.cpp:106] Creating Layer drop
I0726 14:01:11.301375 136306 net.cpp:454] drop <- conv4
I0726 14:01:11.301383 136306 net.cpp:397] drop -> conv4 (in-place)
I0726 14:01:11.301416 136306 net.cpp:150] Setting up drop
I0726 14:01:11.301427 136306 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0726 14:01:11.301434 136306 net.cpp:165] Memory required for data: 393642496
I0726 14:01:11.301441 136306 layer_factory.hpp:76] Creating layer conv_c
I0726 14:01:11.301452 136306 net.cpp:106] Creating Layer conv_c
I0726 14:01:11.301460 136306 net.cpp:454] conv_c <- conv4
I0726 14:01:11.301470 136306 net.cpp:411] conv_c -> conv_c
I0726 14:01:11.302917 136306 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 3072
I0726 14:01:11.303102 136306 net.cpp:150] Setting up conv_c
I0726 14:01:11.303118 136306 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0726 14:01:11.303127 136306 net.cpp:165] Memory required for data: 393773568
I0726 14:01:11.303144 136306 layer_factory.hpp:76] Creating layer loss
I0726 14:01:11.303159 136306 net.cpp:106] Creating Layer loss
I0726 14:01:11.303166 136306 net.cpp:454] loss <- conv_c
I0726 14:01:11.303175 136306 net.cpp:454] loss <- label
I0726 14:01:11.303184 136306 net.cpp:411] loss -> loss
I0726 14:01:11.303201 136306 layer_factory.hpp:76] Creating layer loss
I0726 14:01:11.303454 136306 net.cpp:150] Setting up loss
I0726 14:01:11.303472 136306 net.cpp:157] Top shape: (1)
I0726 14:01:11.303478 136306 net.cpp:160]     with loss weight 1
I0726 14:01:11.303508 136306 net.cpp:165] Memory required for data: 393773572
I0726 14:01:11.303521 136306 net.cpp:226] loss needs backward computation.
I0726 14:01:11.303529 136306 net.cpp:226] conv_c needs backward computation.
I0726 14:01:11.303541 136306 net.cpp:226] drop needs backward computation.
I0726 14:01:11.303549 136306 net.cpp:226] relu4 needs backward computation.
I0726 14:01:11.303555 136306 net.cpp:226] conv4 needs backward computation.
I0726 14:01:11.303575 136306 net.cpp:226] pool3 needs backward computation.
I0726 14:01:11.303583 136306 net.cpp:226] relu32 needs backward computation.
I0726 14:01:11.303589 136306 net.cpp:226] conv32 needs backward computation.
I0726 14:01:11.303596 136306 net.cpp:226] relu31 needs backward computation.
I0726 14:01:11.303602 136306 net.cpp:226] conv31 needs backward computation.
I0726 14:01:11.303608 136306 net.cpp:226] pool2 needs backward computation.
I0726 14:01:11.303616 136306 net.cpp:226] relu22 needs backward computation.
I0726 14:01:11.303622 136306 net.cpp:226] conv22 needs backward computation.
I0726 14:01:11.303627 136306 net.cpp:226] relu21 needs backward computation.
I0726 14:01:11.303634 136306 net.cpp:226] conv21 needs backward computation.
I0726 14:01:11.303640 136306 net.cpp:226] pool1 needs backward computation.
I0726 14:01:11.303647 136306 net.cpp:226] relu12 needs backward computation.
I0726 14:01:11.303653 136306 net.cpp:226] conv12 needs backward computation.
I0726 14:01:11.303658 136306 net.cpp:226] relu11 needs backward computation.
I0726 14:01:11.303665 136306 net.cpp:226] conv11 needs backward computation.
I0726 14:01:11.303673 136306 net.cpp:228] data does not need backward computation.
I0726 14:01:11.303678 136306 net.cpp:270] This network produces output loss
I0726 14:01:11.303700 136306 net.cpp:283] Network initialization done.
I0726 14:01:11.304291 136306 solver.cpp:180] Creating test net (#0) specified by net file: train_val.prototxt
I0726 14:01:11.304345 136306 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0726 14:01:11.304502 136306 net.cpp:49] Initializing net from parameters: 
name: "Net"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  python_param {
    module: "mask_image_layer"
    layer: "random_patches_from_images_withlabel"
    param_str: "{\'root_folder\': \'/home/dywang/Proliferation/data/mitoses\', \'image_list\': \'/home/dywang/00exp_wdy/stage04_mitosisDetection/step02_train_mitosis_detecotor/heatmap_mc13_tr/all_image_withlabel_mc13_tr.lst\', \'seed\': 8899, \'mean\': (128, 128, 128), \'size\': 64, \'batch\': 128, \'scale\':0.1, \'colorn\':20, \'classes\':\'1:0 2:1 3:0\', \'DEBUG\': True}"
  }
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "data"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv12"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv21"
  type: "Convolution"
  bottom: "pool1"
  top: "conv21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu21"
  type: "ReLU"
  bottom: "conv21"
  top: "conv21"
}
layer {
  name: "conv22"
  type: "Convolution"
  bottom: "conv21"
  top: "conv22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu22"
  type: "ReLU"
  bottom: "conv22"
  top: "conv22"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv22"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv31"
  type: "Convolution"
  bottom: "pool2"
  top: "conv31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu31"
  type: "ReLU"
  bottom: "conv31"
  top: "conv31"
}
layer {
  name: "conv32"
  type: "Convolution"
  bottom: "conv31"
  top: "conv32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu32"
  type: "ReLU"
  bottom: "conv32"
  top: "conv32"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv32"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "drop"
  type: "Dropout"
  bottom: "conv4"
  top: "conv4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv_c"
  type: "Convolution"
  bottom: "conv4"
  top: "conv_c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 1
    kernel_w: 1
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "conv_c"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv_c"
  bottom: "label"
  top: "loss"
}
I0726 14:01:11.305572 136306 layer_factory.hpp:76] Creating layer data
I0726 14:01:11.305667 136306 net.cpp:106] Creating Layer data
I0726 14:01:11.305682 136306 net.cpp:411] data -> data
I0726 14:01:11.305696 136306 net.cpp:411] data -> label
I0726 14:01:14.218181 136306 net.cpp:150] Setting up data
I0726 14:01:14.218230 136306 net.cpp:157] Top shape: 128 3 64 64 (1572864)
I0726 14:01:14.218240 136306 net.cpp:157] Top shape: 128 1 1 1 (128)
I0726 14:01:14.218248 136306 net.cpp:165] Memory required for data: 6291968
I0726 14:01:14.218258 136306 layer_factory.hpp:76] Creating layer label_data_1_split
I0726 14:01:14.218283 136306 net.cpp:106] Creating Layer label_data_1_split
I0726 14:01:14.218292 136306 net.cpp:454] label_data_1_split <- label
I0726 14:01:14.218303 136306 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0726 14:01:14.218317 136306 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0726 14:01:14.218360 136306 net.cpp:150] Setting up label_data_1_split
I0726 14:01:14.218374 136306 net.cpp:157] Top shape: 128 1 1 1 (128)
I0726 14:01:14.218384 136306 net.cpp:157] Top shape: 128 1 1 1 (128)
I0726 14:01:14.218389 136306 net.cpp:165] Memory required for data: 6292992
I0726 14:01:14.218402 136306 layer_factory.hpp:76] Creating layer conv11
I0726 14:01:14.218433 136306 net.cpp:106] Creating Layer conv11
I0726 14:01:14.218441 136306 net.cpp:454] conv11 <- data
I0726 14:01:14.218452 136306 net.cpp:411] conv11 -> conv11
I0726 14:01:14.219833 136306 net.cpp:150] Setting up conv11
I0726 14:01:14.219887 136306 net.cpp:157] Top shape: 128 32 62 62 (15745024)
I0726 14:01:14.219895 136306 net.cpp:165] Memory required for data: 69273088
I0726 14:01:14.219910 136306 layer_factory.hpp:76] Creating layer relu11
I0726 14:01:14.219923 136306 net.cpp:106] Creating Layer relu11
I0726 14:01:14.219930 136306 net.cpp:454] relu11 <- conv11
I0726 14:01:14.219938 136306 net.cpp:397] relu11 -> conv11 (in-place)
I0726 14:01:14.220319 136306 net.cpp:150] Setting up relu11
I0726 14:01:14.220342 136306 net.cpp:157] Top shape: 128 32 62 62 (15745024)
I0726 14:01:14.220350 136306 net.cpp:165] Memory required for data: 132253184
I0726 14:01:14.220357 136306 layer_factory.hpp:76] Creating layer conv12
I0726 14:01:14.220371 136306 net.cpp:106] Creating Layer conv12
I0726 14:01:14.220379 136306 net.cpp:454] conv12 <- conv11
I0726 14:01:14.220389 136306 net.cpp:411] conv12 -> conv12
I0726 14:01:14.221320 136306 net.cpp:150] Setting up conv12
I0726 14:01:14.221340 136306 net.cpp:157] Top shape: 128 32 60 60 (14745600)
I0726 14:01:14.221349 136306 net.cpp:165] Memory required for data: 191235584
I0726 14:01:14.221359 136306 layer_factory.hpp:76] Creating layer relu12
I0726 14:01:14.221370 136306 net.cpp:106] Creating Layer relu12
I0726 14:01:14.221379 136306 net.cpp:454] relu12 <- conv12
I0726 14:01:14.221386 136306 net.cpp:397] relu12 -> conv12 (in-place)
I0726 14:01:14.221738 136306 net.cpp:150] Setting up relu12
I0726 14:01:14.221756 136306 net.cpp:157] Top shape: 128 32 60 60 (14745600)
I0726 14:01:14.221765 136306 net.cpp:165] Memory required for data: 250217984
I0726 14:01:14.221771 136306 layer_factory.hpp:76] Creating layer pool1
I0726 14:01:14.221782 136306 net.cpp:106] Creating Layer pool1
I0726 14:01:14.221791 136306 net.cpp:454] pool1 <- conv12
I0726 14:01:14.221799 136306 net.cpp:411] pool1 -> pool1
I0726 14:01:14.221961 136306 net.cpp:150] Setting up pool1
I0726 14:01:14.221976 136306 net.cpp:157] Top shape: 128 32 30 30 (3686400)
I0726 14:01:14.221984 136306 net.cpp:165] Memory required for data: 264963584
I0726 14:01:14.221992 136306 layer_factory.hpp:76] Creating layer conv21
I0726 14:01:14.222002 136306 net.cpp:106] Creating Layer conv21
I0726 14:01:14.222010 136306 net.cpp:454] conv21 <- pool1
I0726 14:01:14.222020 136306 net.cpp:411] conv21 -> conv21
I0726 14:01:14.223464 136306 net.cpp:150] Setting up conv21
I0726 14:01:14.223487 136306 net.cpp:157] Top shape: 128 64 28 28 (6422528)
I0726 14:01:14.223495 136306 net.cpp:165] Memory required for data: 290653696
I0726 14:01:14.223507 136306 layer_factory.hpp:76] Creating layer relu21
I0726 14:01:14.223520 136306 net.cpp:106] Creating Layer relu21
I0726 14:01:14.223527 136306 net.cpp:454] relu21 <- conv21
I0726 14:01:14.223536 136306 net.cpp:397] relu21 -> conv21 (in-place)
I0726 14:01:14.223960 136306 net.cpp:150] Setting up relu21
I0726 14:01:14.223979 136306 net.cpp:157] Top shape: 128 64 28 28 (6422528)
I0726 14:01:14.223986 136306 net.cpp:165] Memory required for data: 316343808
I0726 14:01:14.223994 136306 layer_factory.hpp:76] Creating layer conv22
I0726 14:01:14.224006 136306 net.cpp:106] Creating Layer conv22
I0726 14:01:14.224014 136306 net.cpp:454] conv22 <- conv21
I0726 14:01:14.224025 136306 net.cpp:411] conv22 -> conv22
I0726 14:01:14.225263 136306 net.cpp:150] Setting up conv22
I0726 14:01:14.225283 136306 net.cpp:157] Top shape: 128 64 26 26 (5537792)
I0726 14:01:14.225291 136306 net.cpp:165] Memory required for data: 338494976
I0726 14:01:14.225301 136306 layer_factory.hpp:76] Creating layer relu22
I0726 14:01:14.225314 136306 net.cpp:106] Creating Layer relu22
I0726 14:01:14.225322 136306 net.cpp:454] relu22 <- conv22
I0726 14:01:14.225330 136306 net.cpp:397] relu22 -> conv22 (in-place)
I0726 14:01:14.225467 136306 net.cpp:150] Setting up relu22
I0726 14:01:14.225481 136306 net.cpp:157] Top shape: 128 64 26 26 (5537792)
I0726 14:01:14.225488 136306 net.cpp:165] Memory required for data: 360646144
I0726 14:01:14.225495 136306 layer_factory.hpp:76] Creating layer pool2
I0726 14:01:14.225505 136306 net.cpp:106] Creating Layer pool2
I0726 14:01:14.225527 136306 net.cpp:454] pool2 <- conv22
I0726 14:01:14.225536 136306 net.cpp:411] pool2 -> pool2
I0726 14:01:14.225914 136306 net.cpp:150] Setting up pool2
I0726 14:01:14.225932 136306 net.cpp:157] Top shape: 128 64 13 13 (1384448)
I0726 14:01:14.225940 136306 net.cpp:165] Memory required for data: 366183936
I0726 14:01:14.225947 136306 layer_factory.hpp:76] Creating layer conv31
I0726 14:01:14.225960 136306 net.cpp:106] Creating Layer conv31
I0726 14:01:14.225967 136306 net.cpp:454] conv31 <- pool2
I0726 14:01:14.225977 136306 net.cpp:411] conv31 -> conv31
I0726 14:01:14.227803 136306 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 6912
I0726 14:01:14.227833 136306 net.cpp:150] Setting up conv31
I0726 14:01:14.227844 136306 net.cpp:157] Top shape: 128 128 11 11 (1982464)
I0726 14:01:14.227851 136306 net.cpp:165] Memory required for data: 374113792
I0726 14:01:14.227865 136306 layer_factory.hpp:76] Creating layer relu31
I0726 14:01:14.227875 136306 net.cpp:106] Creating Layer relu31
I0726 14:01:14.227883 136306 net.cpp:454] relu31 <- conv31
I0726 14:01:14.227892 136306 net.cpp:397] relu31 -> conv31 (in-place)
I0726 14:01:14.228255 136306 net.cpp:150] Setting up relu31
I0726 14:01:14.228272 136306 net.cpp:157] Top shape: 128 128 11 11 (1982464)
I0726 14:01:14.228279 136306 net.cpp:165] Memory required for data: 382043648
I0726 14:01:14.228287 136306 layer_factory.hpp:76] Creating layer conv32
I0726 14:01:14.228298 136306 net.cpp:106] Creating Layer conv32
I0726 14:01:14.228307 136306 net.cpp:454] conv32 <- conv31
I0726 14:01:14.228317 136306 net.cpp:411] conv32 -> conv32
I0726 14:01:14.230317 136306 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 13824
I0726 14:01:14.230348 136306 net.cpp:150] Setting up conv32
I0726 14:01:14.230360 136306 net.cpp:157] Top shape: 128 128 9 9 (1327104)
I0726 14:01:14.230368 136306 net.cpp:165] Memory required for data: 387352064
I0726 14:01:14.230378 136306 layer_factory.hpp:76] Creating layer relu32
I0726 14:01:14.230387 136306 net.cpp:106] Creating Layer relu32
I0726 14:01:14.230396 136306 net.cpp:454] relu32 <- conv32
I0726 14:01:14.230404 136306 net.cpp:397] relu32 -> conv32 (in-place)
I0726 14:01:14.230562 136306 net.cpp:150] Setting up relu32
I0726 14:01:14.230578 136306 net.cpp:157] Top shape: 128 128 9 9 (1327104)
I0726 14:01:14.230586 136306 net.cpp:165] Memory required for data: 392660480
I0726 14:01:14.230593 136306 layer_factory.hpp:76] Creating layer pool3
I0726 14:01:14.230618 136306 net.cpp:106] Creating Layer pool3
I0726 14:01:14.230625 136306 net.cpp:454] pool3 <- conv32
I0726 14:01:14.230679 136306 net.cpp:411] pool3 -> pool3
I0726 14:01:14.239653 136306 net.cpp:150] Setting up pool3
I0726 14:01:14.239676 136306 net.cpp:157] Top shape: 128 128 3 3 (147456)
I0726 14:01:14.239684 136306 net.cpp:165] Memory required for data: 393250304
I0726 14:01:14.239692 136306 layer_factory.hpp:76] Creating layer conv4
I0726 14:01:14.239704 136306 net.cpp:106] Creating Layer conv4
I0726 14:01:14.239712 136306 net.cpp:454] conv4 <- pool3
I0726 14:01:14.239722 136306 net.cpp:411] conv4 -> conv4
I0726 14:01:14.268746 136306 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 1769472
I0726 14:01:14.269039 136306 net.cpp:150] Setting up conv4
I0726 14:01:14.269067 136306 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0726 14:01:14.269073 136306 net.cpp:165] Memory required for data: 393381376
I0726 14:01:14.269084 136306 layer_factory.hpp:76] Creating layer relu4
I0726 14:01:14.269095 136306 net.cpp:106] Creating Layer relu4
I0726 14:01:14.269104 136306 net.cpp:454] relu4 <- conv4
I0726 14:01:14.269112 136306 net.cpp:397] relu4 -> conv4 (in-place)
I0726 14:01:14.269264 136306 net.cpp:150] Setting up relu4
I0726 14:01:14.269279 136306 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0726 14:01:14.269286 136306 net.cpp:165] Memory required for data: 393512448
I0726 14:01:14.269294 136306 layer_factory.hpp:76] Creating layer drop
I0726 14:01:14.269304 136306 net.cpp:106] Creating Layer drop
I0726 14:01:14.269311 136306 net.cpp:454] drop <- conv4
I0726 14:01:14.269335 136306 net.cpp:397] drop -> conv4 (in-place)
I0726 14:01:14.269368 136306 net.cpp:150] Setting up drop
I0726 14:01:14.269383 136306 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0726 14:01:14.269398 136306 net.cpp:165] Memory required for data: 393643520
I0726 14:01:14.269405 136306 layer_factory.hpp:76] Creating layer conv_c
I0726 14:01:14.269423 136306 net.cpp:106] Creating Layer conv_c
I0726 14:01:14.269429 136306 net.cpp:454] conv_c <- conv4
I0726 14:01:14.269439 136306 net.cpp:411] conv_c -> conv_c
I0726 14:01:14.271417 136306 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 3072
I0726 14:01:14.271450 136306 net.cpp:150] Setting up conv_c
I0726 14:01:14.271462 136306 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0726 14:01:14.271472 136306 net.cpp:165] Memory required for data: 393774592
I0726 14:01:14.271483 136306 layer_factory.hpp:76] Creating layer conv_c_conv_c_0_split
I0726 14:01:14.271494 136306 net.cpp:106] Creating Layer conv_c_conv_c_0_split
I0726 14:01:14.271503 136306 net.cpp:454] conv_c_conv_c_0_split <- conv_c
I0726 14:01:14.271513 136306 net.cpp:411] conv_c_conv_c_0_split -> conv_c_conv_c_0_split_0
I0726 14:01:14.271525 136306 net.cpp:411] conv_c_conv_c_0_split -> conv_c_conv_c_0_split_1
I0726 14:01:14.271567 136306 net.cpp:150] Setting up conv_c_conv_c_0_split
I0726 14:01:14.271589 136306 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0726 14:01:14.271600 136306 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0726 14:01:14.271610 136306 net.cpp:165] Memory required for data: 394036736
I0726 14:01:14.271617 136306 layer_factory.hpp:76] Creating layer accuracy
I0726 14:01:14.271634 136306 net.cpp:106] Creating Layer accuracy
I0726 14:01:14.271643 136306 net.cpp:454] accuracy <- conv_c_conv_c_0_split_0
I0726 14:01:14.271656 136306 net.cpp:454] accuracy <- label_data_1_split_0
I0726 14:01:14.271666 136306 net.cpp:411] accuracy -> accuracy
I0726 14:01:14.271687 136306 net.cpp:150] Setting up accuracy
I0726 14:01:14.271697 136306 net.cpp:157] Top shape: (1)
I0726 14:01:14.271704 136306 net.cpp:165] Memory required for data: 394036740
I0726 14:01:14.271714 136306 layer_factory.hpp:76] Creating layer loss
I0726 14:01:14.271724 136306 net.cpp:106] Creating Layer loss
I0726 14:01:14.271733 136306 net.cpp:454] loss <- conv_c_conv_c_0_split_1
I0726 14:01:14.271744 136306 net.cpp:454] loss <- label_data_1_split_1
I0726 14:01:14.271752 136306 net.cpp:411] loss -> loss
I0726 14:01:14.271764 136306 layer_factory.hpp:76] Creating layer loss
I0726 14:01:14.272141 136306 net.cpp:150] Setting up loss
I0726 14:01:14.272162 136306 net.cpp:157] Top shape: (1)
I0726 14:01:14.272171 136306 net.cpp:160]     with loss weight 1
I0726 14:01:14.272183 136306 net.cpp:165] Memory required for data: 394036744
I0726 14:01:14.272192 136306 net.cpp:226] loss needs backward computation.
I0726 14:01:14.272200 136306 net.cpp:228] accuracy does not need backward computation.
I0726 14:01:14.272209 136306 net.cpp:226] conv_c_conv_c_0_split needs backward computation.
I0726 14:01:14.272217 136306 net.cpp:226] conv_c needs backward computation.
I0726 14:01:14.272223 136306 net.cpp:226] drop needs backward computation.
I0726 14:01:14.272230 136306 net.cpp:226] relu4 needs backward computation.
I0726 14:01:14.272238 136306 net.cpp:226] conv4 needs backward computation.
I0726 14:01:14.272245 136306 net.cpp:226] pool3 needs backward computation.
I0726 14:01:14.272253 136306 net.cpp:226] relu32 needs backward computation.
I0726 14:01:14.272259 136306 net.cpp:226] conv32 needs backward computation.
I0726 14:01:14.272266 136306 net.cpp:226] relu31 needs backward computation.
I0726 14:01:14.272274 136306 net.cpp:226] conv31 needs backward computation.
I0726 14:01:14.272280 136306 net.cpp:226] pool2 needs backward computation.
I0726 14:01:14.272289 136306 net.cpp:226] relu22 needs backward computation.
I0726 14:01:14.272295 136306 net.cpp:226] conv22 needs backward computation.
I0726 14:01:14.272302 136306 net.cpp:226] relu21 needs backward computation.
I0726 14:01:14.272308 136306 net.cpp:226] conv21 needs backward computation.
I0726 14:01:14.272331 136306 net.cpp:226] pool1 needs backward computation.
I0726 14:01:14.272344 136306 net.cpp:226] relu12 needs backward computation.
I0726 14:01:14.272351 136306 net.cpp:226] conv12 needs backward computation.
I0726 14:01:14.272358 136306 net.cpp:226] relu11 needs backward computation.
I0726 14:01:14.272366 136306 net.cpp:226] conv11 needs backward computation.
I0726 14:01:14.272377 136306 net.cpp:228] label_data_1_split does not need backward computation.
I0726 14:01:14.272384 136306 net.cpp:228] data does not need backward computation.
I0726 14:01:14.272392 136306 net.cpp:270] This network produces output accuracy
I0726 14:01:14.272400 136306 net.cpp:270] This network produces output loss
I0726 14:01:14.272418 136306 net.cpp:283] Network initialization done.
I0726 14:01:14.272510 136306 solver.cpp:59] Solver scaffolding done.
I0726 14:01:14.273013 136306 caffe.cpp:128] Finetuning from models.final/gnet_iter_45000.caffemodel
I0726 14:01:14.280865 136306 caffe.cpp:212] Starting Optimization
I0726 14:01:14.280899 136306 solver.cpp:287] Solving Net
I0726 14:01:14.280906 136306 solver.cpp:288] Learning Rate Policy: step
I0726 14:01:14.281764 136306 solver.cpp:340] Iteration 0, Testing net (#0)
I0726 14:01:35.698925 136306 solver.cpp:408]     Test net output #0: accuracy = 0.821094
I0726 14:01:35.699053 136306 solver.cpp:408]     Test net output #1: loss = 0.669236 (* 1 = 0.669236 loss)
I0726 14:01:38.600129 136306 solver.cpp:236] Iteration 0, loss = 0.542373
I0726 14:01:38.600185 136306 solver.cpp:252]     Train net output #0: loss = 0.542373 (* 1 = 0.542373 loss)
I0726 14:01:38.600209 136306 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0726 14:01:58.025938 136306 solver.cpp:236] Iteration 10, loss = 0.241921
I0726 14:01:58.026010 136306 solver.cpp:252]     Train net output #0: loss = 0.241921 (* 1 = 0.241921 loss)
I0726 14:01:58.026023 136306 sgd_solver.cpp:106] Iteration 10, lr = 0.01
I0726 14:02:17.802750 136306 solver.cpp:236] Iteration 20, loss = 0.774507
I0726 14:02:17.802934 136306 solver.cpp:252]     Train net output #0: loss = 0.774507 (* 1 = 0.774507 loss)
I0726 14:02:17.802954 136306 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I0726 14:02:38.097053 136306 solver.cpp:236] Iteration 30, loss = 3.06036
I0726 14:02:38.097152 136306 solver.cpp:252]     Train net output #0: loss = 3.06036 (* 1 = 3.06036 loss)
I0726 14:02:38.097167 136306 sgd_solver.cpp:106] Iteration 30, lr = 0.01
I0726 14:03:01.376595 136306 solver.cpp:236] Iteration 40, loss = 87.3365
I0726 14:03:01.376786 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:03:01.376816 136306 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I0726 14:03:23.548077 136306 solver.cpp:236] Iteration 50, loss = 87.3365
I0726 14:03:23.548166 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:03:23.548179 136306 sgd_solver.cpp:106] Iteration 50, lr = 0.01
I0726 14:03:41.614665 136306 solver.cpp:236] Iteration 60, loss = 87.3365
I0726 14:03:41.614908 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:03:41.614924 136306 sgd_solver.cpp:106] Iteration 60, lr = 0.01
I0726 14:04:02.457954 136306 solver.cpp:236] Iteration 70, loss = 87.3365
I0726 14:04:02.458019 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:04:02.458034 136306 sgd_solver.cpp:106] Iteration 70, lr = 0.01
I0726 14:04:22.940521 136306 solver.cpp:236] Iteration 80, loss = 87.3365
I0726 14:04:22.940690 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:04:22.940721 136306 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I0726 14:04:41.478111 136306 solver.cpp:236] Iteration 90, loss = 87.3365
I0726 14:04:41.478174 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:04:41.478188 136306 sgd_solver.cpp:106] Iteration 90, lr = 0.01
I0726 14:05:01.669940 136306 solver.cpp:236] Iteration 100, loss = 87.3365
I0726 14:05:01.670266 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:05:01.670284 136306 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0726 14:05:17.851543 136306 solver.cpp:236] Iteration 110, loss = 87.3365
I0726 14:05:17.851598 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:05:17.851609 136306 sgd_solver.cpp:106] Iteration 110, lr = 0.01
I0726 14:05:35.263864 136306 solver.cpp:236] Iteration 120, loss = 87.3365
I0726 14:05:35.264103 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:05:35.264132 136306 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I0726 14:05:53.083403 136306 solver.cpp:236] Iteration 130, loss = 87.3365
I0726 14:05:53.083467 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:05:53.083482 136306 sgd_solver.cpp:106] Iteration 130, lr = 0.01
I0726 14:06:11.698066 136306 solver.cpp:236] Iteration 140, loss = 87.3365
I0726 14:06:11.698228 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:06:11.698251 136306 sgd_solver.cpp:106] Iteration 140, lr = 0.01
I0726 14:06:33.529613 136306 solver.cpp:236] Iteration 150, loss = 87.3365
I0726 14:06:33.529676 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:06:33.529690 136306 sgd_solver.cpp:106] Iteration 150, lr = 0.01
I0726 14:06:54.896760 136306 solver.cpp:236] Iteration 160, loss = 87.3365
I0726 14:06:54.896921 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:06:54.896950 136306 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I0726 14:07:15.009678 136306 solver.cpp:236] Iteration 170, loss = 87.3365
I0726 14:07:15.009730 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:07:15.009743 136306 sgd_solver.cpp:106] Iteration 170, lr = 0.01
I0726 14:07:32.302495 136306 solver.cpp:236] Iteration 180, loss = 87.3365
I0726 14:07:32.302680 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:07:32.302713 136306 sgd_solver.cpp:106] Iteration 180, lr = 0.01
I0726 14:07:52.203750 136306 solver.cpp:236] Iteration 190, loss = 87.3365
I0726 14:07:52.203815 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:07:52.203829 136306 sgd_solver.cpp:106] Iteration 190, lr = 0.01
I0726 14:08:12.149129 136306 solver.cpp:236] Iteration 200, loss = 87.3365
I0726 14:08:12.149266 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:08:12.149299 136306 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I0726 14:08:30.853353 136306 solver.cpp:236] Iteration 210, loss = 87.3365
I0726 14:08:30.853430 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:08:30.853443 136306 sgd_solver.cpp:106] Iteration 210, lr = 0.01
I0726 14:08:50.823539 136306 solver.cpp:236] Iteration 220, loss = 87.3365
I0726 14:08:50.823709 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:08:50.823726 136306 sgd_solver.cpp:106] Iteration 220, lr = 0.01
I0726 14:09:12.504353 136306 solver.cpp:236] Iteration 230, loss = 87.3365
I0726 14:09:12.504426 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:09:12.504439 136306 sgd_solver.cpp:106] Iteration 230, lr = 0.01
I0726 14:09:31.158382 136306 solver.cpp:236] Iteration 240, loss = 87.3365
I0726 14:09:31.158581 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:09:31.158624 136306 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I0726 14:09:51.155704 136306 solver.cpp:236] Iteration 250, loss = 87.3365
I0726 14:09:51.155766 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:09:51.155781 136306 sgd_solver.cpp:106] Iteration 250, lr = 0.01
I0726 14:10:06.490614 136306 solver.cpp:236] Iteration 260, loss = 87.3365
I0726 14:10:06.490875 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:10:06.490890 136306 sgd_solver.cpp:106] Iteration 260, lr = 0.01
I0726 14:10:25.089651 136306 solver.cpp:236] Iteration 270, loss = 87.3365
I0726 14:10:25.089715 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:10:25.089730 136306 sgd_solver.cpp:106] Iteration 270, lr = 0.01
I0726 14:10:43.165776 136306 solver.cpp:236] Iteration 280, loss = 87.3365
I0726 14:10:43.165969 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:10:43.166000 136306 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I0726 14:11:09.162314 136306 solver.cpp:236] Iteration 290, loss = 87.3365
I0726 14:11:09.162382 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:11:09.162398 136306 sgd_solver.cpp:106] Iteration 290, lr = 0.01
I0726 14:11:25.876902 136306 solver.cpp:236] Iteration 300, loss = 87.3365
I0726 14:11:25.877110 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:11:25.877138 136306 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I0726 14:11:43.016409 136306 solver.cpp:236] Iteration 310, loss = 87.3365
I0726 14:11:43.016479 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:11:43.016495 136306 sgd_solver.cpp:106] Iteration 310, lr = 0.01
I0726 14:12:02.110496 136306 solver.cpp:236] Iteration 320, loss = 87.3365
I0726 14:12:02.110743 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:12:02.110775 136306 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I0726 14:12:22.515985 136306 solver.cpp:236] Iteration 330, loss = 87.3365
I0726 14:12:22.516057 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:12:22.516070 136306 sgd_solver.cpp:106] Iteration 330, lr = 0.01
I0726 14:12:42.400881 136306 solver.cpp:236] Iteration 340, loss = 87.3365
I0726 14:12:42.401051 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:12:42.401090 136306 sgd_solver.cpp:106] Iteration 340, lr = 0.01
I0726 14:13:02.053345 136306 solver.cpp:236] Iteration 350, loss = 87.3365
I0726 14:13:02.053411 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:13:02.053424 136306 sgd_solver.cpp:106] Iteration 350, lr = 0.01
I0726 14:13:24.170212 136306 solver.cpp:236] Iteration 360, loss = 87.3365
I0726 14:13:24.170394 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:13:24.170444 136306 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I0726 14:13:43.626305 136306 solver.cpp:236] Iteration 370, loss = 87.3365
I0726 14:13:43.626368 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:13:43.626384 136306 sgd_solver.cpp:106] Iteration 370, lr = 0.01
I0726 14:14:02.437144 136306 solver.cpp:236] Iteration 380, loss = 87.3365
I0726 14:14:02.437386 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:14:02.437400 136306 sgd_solver.cpp:106] Iteration 380, lr = 0.01
I0726 14:14:19.217033 136306 solver.cpp:236] Iteration 390, loss = 87.3365
I0726 14:14:19.217099 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:14:19.217115 136306 sgd_solver.cpp:106] Iteration 390, lr = 0.01
I0726 14:14:38.269919 136306 solver.cpp:236] Iteration 400, loss = 87.3365
I0726 14:14:38.270169 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:14:38.270185 136306 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I0726 14:14:55.464282 136306 solver.cpp:236] Iteration 410, loss = 87.3365
I0726 14:14:55.464344 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:14:55.464360 136306 sgd_solver.cpp:106] Iteration 410, lr = 0.01
I0726 14:15:18.049844 136306 solver.cpp:236] Iteration 420, loss = 87.3365
I0726 14:15:18.050084 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:15:18.050101 136306 sgd_solver.cpp:106] Iteration 420, lr = 0.01
I0726 14:15:34.367312 136306 solver.cpp:236] Iteration 430, loss = 87.3365
I0726 14:15:34.367374 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:15:34.367388 136306 sgd_solver.cpp:106] Iteration 430, lr = 0.01
I0726 14:15:50.607574 136306 solver.cpp:236] Iteration 440, loss = 87.3365
I0726 14:15:50.607842 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:15:50.607861 136306 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I0726 14:16:12.266985 136306 solver.cpp:236] Iteration 450, loss = 87.3365
I0726 14:16:12.267047 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:16:12.267065 136306 sgd_solver.cpp:106] Iteration 450, lr = 0.01
I0726 14:16:33.094064 136306 solver.cpp:236] Iteration 460, loss = 87.3365
I0726 14:16:33.094271 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:16:33.094292 136306 sgd_solver.cpp:106] Iteration 460, lr = 0.01
I0726 14:16:54.397945 136306 solver.cpp:236] Iteration 470, loss = 87.3365
I0726 14:16:54.398006 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:16:54.398020 136306 sgd_solver.cpp:106] Iteration 470, lr = 0.01
I0726 14:17:17.604866 136306 solver.cpp:236] Iteration 480, loss = 87.3365
I0726 14:17:17.605063 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:17:17.605079 136306 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I0726 14:17:40.706795 136306 solver.cpp:236] Iteration 490, loss = 87.3365
I0726 14:17:40.706859 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:17:40.706873 136306 sgd_solver.cpp:106] Iteration 490, lr = 0.01
I0726 14:18:09.197032 136306 solver.cpp:340] Iteration 500, Testing net (#0)
I0726 14:18:36.715206 136306 solver.cpp:408]     Test net output #0: accuracy = 0
I0726 14:18:36.715279 136306 solver.cpp:408]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:18:40.603145 136306 solver.cpp:236] Iteration 500, loss = 87.3365
I0726 14:18:40.603519 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:18:40.603555 136306 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I0726 14:19:13.847378 136306 solver.cpp:236] Iteration 510, loss = 87.3365
I0726 14:19:13.847618 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:19:13.847650 136306 sgd_solver.cpp:106] Iteration 510, lr = 0.01
I0726 14:19:50.104632 136306 solver.cpp:236] Iteration 520, loss = 87.3365
I0726 14:19:50.104755 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:19:50.104770 136306 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I0726 14:20:32.994844 136306 solver.cpp:236] Iteration 530, loss = 87.3365
I0726 14:20:32.995034 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:20:32.995077 136306 sgd_solver.cpp:106] Iteration 530, lr = 0.01
I0726 14:21:15.311424 136306 solver.cpp:236] Iteration 540, loss = 87.3365
I0726 14:21:15.311615 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:21:15.311633 136306 sgd_solver.cpp:106] Iteration 540, lr = 0.01
I0726 14:22:03.049772 136306 solver.cpp:236] Iteration 550, loss = 87.3365
I0726 14:22:03.049990 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:22:03.050034 136306 sgd_solver.cpp:106] Iteration 550, lr = 0.01
I0726 14:22:46.953794 136306 solver.cpp:236] Iteration 560, loss = 87.3365
I0726 14:22:46.953994 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:22:46.954022 136306 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I0726 14:23:36.407176 136306 solver.cpp:236] Iteration 570, loss = 87.3365
I0726 14:23:36.407444 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:23:36.407461 136306 sgd_solver.cpp:106] Iteration 570, lr = 0.01
I0726 14:24:05.199349 136306 solver.cpp:236] Iteration 580, loss = 87.3365
I0726 14:24:05.199422 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:24:05.199437 136306 sgd_solver.cpp:106] Iteration 580, lr = 0.01
I0726 14:24:44.038151 136306 solver.cpp:236] Iteration 590, loss = 87.3365
I0726 14:24:44.038370 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:24:44.038394 136306 sgd_solver.cpp:106] Iteration 590, lr = 0.01
I0726 14:25:27.699879 136306 solver.cpp:236] Iteration 600, loss = 87.3365
I0726 14:25:27.700073 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:25:27.700117 136306 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I0726 14:26:14.323261 136306 solver.cpp:236] Iteration 610, loss = 87.3365
I0726 14:26:14.323496 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:26:14.323524 136306 sgd_solver.cpp:106] Iteration 610, lr = 0.01
I0726 14:26:55.042870 136306 solver.cpp:236] Iteration 620, loss = 87.3365
I0726 14:26:55.043056 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:26:55.043100 136306 sgd_solver.cpp:106] Iteration 620, lr = 0.01
I0726 14:27:55.634835 136306 solver.cpp:236] Iteration 630, loss = 87.3365
I0726 14:27:55.635102 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:27:55.635118 136306 sgd_solver.cpp:106] Iteration 630, lr = 0.01
I0726 14:28:38.505087 136306 solver.cpp:236] Iteration 640, loss = 87.3365
I0726 14:28:38.505306 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:28:38.505338 136306 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I0726 14:28:56.668123 136306 solver.cpp:236] Iteration 650, loss = 87.3365
I0726 14:28:56.668197 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:28:56.668212 136306 sgd_solver.cpp:106] Iteration 650, lr = 0.01
I0726 14:29:14.512827 136306 solver.cpp:236] Iteration 660, loss = 87.3365
I0726 14:29:14.513049 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:29:14.513067 136306 sgd_solver.cpp:106] Iteration 660, lr = 0.01
I0726 14:29:27.510093 136306 solver.cpp:236] Iteration 670, loss = 87.3365
I0726 14:29:27.510164 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:29:27.510179 136306 sgd_solver.cpp:106] Iteration 670, lr = 0.01
I0726 14:29:46.807749 136306 solver.cpp:236] Iteration 680, loss = 87.3365
I0726 14:29:46.807979 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:29:46.808017 136306 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I0726 14:30:05.244874 136306 solver.cpp:236] Iteration 690, loss = 87.3365
I0726 14:30:05.244942 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:30:05.244964 136306 sgd_solver.cpp:106] Iteration 690, lr = 0.01
I0726 14:30:22.863767 136306 solver.cpp:236] Iteration 700, loss = 87.3365
I0726 14:30:22.864013 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:30:22.864027 136306 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I0726 14:30:42.637804 136306 solver.cpp:236] Iteration 710, loss = 87.3365
I0726 14:30:42.637881 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:30:42.637897 136306 sgd_solver.cpp:106] Iteration 710, lr = 0.01
I0726 14:31:00.441944 136306 solver.cpp:236] Iteration 720, loss = 87.3365
I0726 14:31:00.442216 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:31:00.442232 136306 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I0726 14:31:17.518458 136306 solver.cpp:236] Iteration 730, loss = 87.3365
I0726 14:31:17.518535 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:31:17.518549 136306 sgd_solver.cpp:106] Iteration 730, lr = 0.01
I0726 14:31:38.518718 136306 solver.cpp:236] Iteration 740, loss = 87.3365
I0726 14:31:38.519001 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:31:38.519022 136306 sgd_solver.cpp:106] Iteration 740, lr = 0.01
I0726 14:31:54.481925 136306 solver.cpp:236] Iteration 750, loss = 87.3365
I0726 14:31:54.481998 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:31:54.482012 136306 sgd_solver.cpp:106] Iteration 750, lr = 0.01
I0726 14:32:18.133723 136306 solver.cpp:236] Iteration 760, loss = 87.3365
I0726 14:32:18.133874 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:32:18.133893 136306 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I0726 14:33:44.449025 136306 solver.cpp:236] Iteration 770, loss = 87.3365
I0726 14:33:44.449323 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:33:44.449342 136306 sgd_solver.cpp:106] Iteration 770, lr = 0.01
I0726 14:34:38.704200 136306 solver.cpp:236] Iteration 780, loss = 87.3365
I0726 14:34:38.704372 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:34:38.704402 136306 sgd_solver.cpp:106] Iteration 780, lr = 0.01
I0726 14:36:05.942734 136306 solver.cpp:236] Iteration 790, loss = 87.3365
I0726 14:36:05.942965 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:36:05.942987 136306 sgd_solver.cpp:106] Iteration 790, lr = 0.01
I0726 14:37:44.151609 136306 solver.cpp:236] Iteration 800, loss = 87.3365
I0726 14:37:44.151787 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:37:44.151814 136306 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I0726 14:39:32.177341 136306 solver.cpp:236] Iteration 810, loss = 87.3365
I0726 14:39:32.177582 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:39:32.177603 136306 sgd_solver.cpp:106] Iteration 810, lr = 0.01
I0726 14:40:51.417089 136306 solver.cpp:236] Iteration 820, loss = 87.3365
I0726 14:40:51.417258 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:40:51.417281 136306 sgd_solver.cpp:106] Iteration 820, lr = 0.01
I0726 14:42:25.631625 136306 solver.cpp:236] Iteration 830, loss = 87.3365
I0726 14:42:25.631804 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:42:25.631832 136306 sgd_solver.cpp:106] Iteration 830, lr = 0.01
I0726 14:43:48.606436 136306 solver.cpp:236] Iteration 840, loss = 87.3365
I0726 14:43:48.606637 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:43:48.606662 136306 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I0726 14:45:25.211470 136306 solver.cpp:236] Iteration 850, loss = 87.3365
I0726 14:45:25.211699 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:45:25.211736 136306 sgd_solver.cpp:106] Iteration 850, lr = 0.01
I0726 14:47:15.934604 136306 solver.cpp:236] Iteration 860, loss = 87.3365
I0726 14:47:15.934830 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:47:15.934861 136306 sgd_solver.cpp:106] Iteration 860, lr = 0.01
I0726 14:48:17.432277 136306 solver.cpp:236] Iteration 870, loss = 87.3365
I0726 14:48:17.432452 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:48:17.432471 136306 sgd_solver.cpp:106] Iteration 870, lr = 0.01
I0726 14:49:02.364500 136306 solver.cpp:236] Iteration 880, loss = 87.3365
I0726 14:49:02.364722 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:49:02.364749 136306 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I0726 14:49:27.820569 136306 solver.cpp:236] Iteration 890, loss = 87.3365
I0726 14:49:27.820634 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:49:27.820650 136306 sgd_solver.cpp:106] Iteration 890, lr = 0.01
I0726 14:49:50.874285 136306 solver.cpp:236] Iteration 900, loss = 87.3365
I0726 14:49:50.893935 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:49:50.893967 136306 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I0726 14:50:15.994163 136306 solver.cpp:236] Iteration 910, loss = 87.3365
I0726 14:50:15.994240 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:50:15.994254 136306 sgd_solver.cpp:106] Iteration 910, lr = 0.01
I0726 14:50:44.000607 136306 solver.cpp:236] Iteration 920, loss = 87.3365
I0726 14:50:44.000864 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:50:44.000881 136306 sgd_solver.cpp:106] Iteration 920, lr = 0.01
I0726 14:51:11.086688 136306 solver.cpp:236] Iteration 930, loss = 87.3365
I0726 14:51:11.086769 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:51:11.086786 136306 sgd_solver.cpp:106] Iteration 930, lr = 0.01
I0726 14:51:38.633352 136306 solver.cpp:236] Iteration 940, loss = 87.3365
I0726 14:51:38.633584 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:51:38.633599 136306 sgd_solver.cpp:106] Iteration 940, lr = 0.01
I0726 14:52:05.296025 136306 solver.cpp:236] Iteration 950, loss = 87.3365
I0726 14:52:05.296103 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:52:05.296120 136306 sgd_solver.cpp:106] Iteration 950, lr = 0.01
I0726 14:52:30.371403 136306 solver.cpp:236] Iteration 960, loss = 87.3365
I0726 14:52:30.371593 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:52:30.371630 136306 sgd_solver.cpp:106] Iteration 960, lr = 0.01
I0726 14:52:50.834012 136306 solver.cpp:236] Iteration 970, loss = 87.3365
I0726 14:52:50.834080 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:52:50.834095 136306 sgd_solver.cpp:106] Iteration 970, lr = 0.01
I0726 14:53:16.369679 136306 solver.cpp:236] Iteration 980, loss = 87.3365
I0726 14:53:16.369879 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:53:16.369907 136306 sgd_solver.cpp:106] Iteration 980, lr = 0.01
I0726 14:53:40.098919 136306 solver.cpp:236] Iteration 990, loss = 87.3365
I0726 14:53:40.098994 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:53:40.099012 136306 sgd_solver.cpp:106] Iteration 990, lr = 0.01
I0726 14:54:00.192512 136306 solver.cpp:461] Snapshotting to binary proto file models/gnet_iter_1000.caffemodel
I0726 14:54:00.313949 136306 sgd_solver.cpp:269] Snapshotting solver state to binary proto file models/gnet_iter_1000.solverstate
I0726 14:54:00.317875 136306 solver.cpp:340] Iteration 1000, Testing net (#0)
I0726 14:54:27.127238 136306 solver.cpp:408]     Test net output #0: accuracy = 0
I0726 14:54:27.127315 136306 solver.cpp:408]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:54:28.285101 136306 solver.cpp:236] Iteration 1000, loss = 87.3365
I0726 14:54:28.285169 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:54:28.285186 136306 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I0726 14:54:52.044396 136306 solver.cpp:236] Iteration 1010, loss = 87.3365
I0726 14:54:52.044654 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:54:52.044670 136306 sgd_solver.cpp:106] Iteration 1010, lr = 0.01
I0726 14:55:10.444349 136306 solver.cpp:236] Iteration 1020, loss = 87.3365
I0726 14:55:10.444427 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:55:10.444443 136306 sgd_solver.cpp:106] Iteration 1020, lr = 0.01
I0726 14:55:28.577219 136306 solver.cpp:236] Iteration 1030, loss = 87.3365
I0726 14:55:28.577491 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:55:28.577512 136306 sgd_solver.cpp:106] Iteration 1030, lr = 0.01
I0726 14:55:49.028472 136306 solver.cpp:236] Iteration 1040, loss = 87.3365
I0726 14:55:49.028565 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:55:49.028580 136306 sgd_solver.cpp:106] Iteration 1040, lr = 0.01
I0726 14:56:10.378201 136306 solver.cpp:236] Iteration 1050, loss = 87.3365
I0726 14:56:10.378363 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:56:10.378391 136306 sgd_solver.cpp:106] Iteration 1050, lr = 0.01
I0726 14:56:29.891857 136306 solver.cpp:236] Iteration 1060, loss = 87.3365
I0726 14:56:29.891934 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:56:29.891950 136306 sgd_solver.cpp:106] Iteration 1060, lr = 0.01
I0726 14:56:55.935138 136306 solver.cpp:236] Iteration 1070, loss = 87.3365
I0726 14:56:55.935353 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:56:55.935391 136306 sgd_solver.cpp:106] Iteration 1070, lr = 0.01
I0726 14:57:17.071188 136306 solver.cpp:236] Iteration 1080, loss = 87.3365
I0726 14:57:17.071265 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:57:17.071280 136306 sgd_solver.cpp:106] Iteration 1080, lr = 0.01
I0726 14:57:49.440868 136306 solver.cpp:236] Iteration 1090, loss = 87.3365
I0726 14:57:49.441087 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:57:49.441135 136306 sgd_solver.cpp:106] Iteration 1090, lr = 0.01
I0726 14:58:18.726127 136306 solver.cpp:236] Iteration 1100, loss = 87.3365
I0726 14:58:18.726199 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:58:18.726213 136306 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I0726 14:58:42.418903 136306 solver.cpp:236] Iteration 1110, loss = 87.3365
I0726 14:58:42.419158 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:58:42.419173 136306 sgd_solver.cpp:106] Iteration 1110, lr = 0.01
I0726 14:59:15.156559 136306 solver.cpp:236] Iteration 1120, loss = 87.3365
I0726 14:59:15.156800 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:59:15.156831 136306 sgd_solver.cpp:106] Iteration 1120, lr = 0.01
I0726 14:59:47.517235 136306 solver.cpp:236] Iteration 1130, loss = 87.3365
I0726 14:59:47.517470 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 14:59:47.517494 136306 sgd_solver.cpp:106] Iteration 1130, lr = 0.01
I0726 15:00:27.991853 136306 solver.cpp:236] Iteration 1140, loss = 87.3365
I0726 15:00:27.992064 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:00:27.992110 136306 sgd_solver.cpp:106] Iteration 1140, lr = 0.01
I0726 15:01:06.694176 136306 solver.cpp:236] Iteration 1150, loss = 87.3365
I0726 15:01:06.694401 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:01:06.694432 136306 sgd_solver.cpp:106] Iteration 1150, lr = 0.01
I0726 15:01:51.327879 136306 solver.cpp:236] Iteration 1160, loss = 87.3365
I0726 15:01:51.328069 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:01:51.328114 136306 sgd_solver.cpp:106] Iteration 1160, lr = 0.01
I0726 15:02:17.192945 136306 solver.cpp:236] Iteration 1170, loss = 87.3365
I0726 15:02:17.193025 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:02:17.193042 136306 sgd_solver.cpp:106] Iteration 1170, lr = 0.01
I0726 15:02:50.592234 136306 solver.cpp:236] Iteration 1180, loss = 87.3365
I0726 15:02:50.592422 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:02:50.592439 136306 sgd_solver.cpp:106] Iteration 1180, lr = 0.01
I0726 15:03:28.531898 136306 solver.cpp:236] Iteration 1190, loss = 87.3365
I0726 15:03:28.532176 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:03:28.532196 136306 sgd_solver.cpp:106] Iteration 1190, lr = 0.01
I0726 15:04:06.319677 136306 solver.cpp:236] Iteration 1200, loss = 87.3365
I0726 15:04:06.319845 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:04:06.319866 136306 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I0726 15:04:43.143857 136306 solver.cpp:236] Iteration 1210, loss = 87.3365
I0726 15:04:43.144044 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:04:43.144088 136306 sgd_solver.cpp:106] Iteration 1210, lr = 0.01
I0726 15:05:17.248639 136306 solver.cpp:236] Iteration 1220, loss = 87.3365
I0726 15:05:17.248920 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:05:17.248940 136306 sgd_solver.cpp:106] Iteration 1220, lr = 0.01
I0726 15:05:48.290859 136306 solver.cpp:236] Iteration 1230, loss = 87.3365
I0726 15:05:48.291081 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:05:48.291110 136306 sgd_solver.cpp:106] Iteration 1230, lr = 0.01
I0726 15:06:25.344089 136306 solver.cpp:236] Iteration 1240, loss = 87.3365
I0726 15:06:25.344352 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:06:25.344367 136306 sgd_solver.cpp:106] Iteration 1240, lr = 0.01
I0726 15:07:08.934825 136306 solver.cpp:236] Iteration 1250, loss = 87.3365
I0726 15:07:08.935014 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:07:08.935051 136306 sgd_solver.cpp:106] Iteration 1250, lr = 0.01
I0726 15:07:56.126024 136306 solver.cpp:236] Iteration 1260, loss = 87.3365
I0726 15:07:56.126227 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:07:56.126258 136306 sgd_solver.cpp:106] Iteration 1260, lr = 0.01
I0726 15:08:29.169013 136306 solver.cpp:236] Iteration 1270, loss = 87.3365
I0726 15:08:29.169235 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:08:29.169253 136306 sgd_solver.cpp:106] Iteration 1270, lr = 0.01
I0726 15:09:17.975195 136306 solver.cpp:236] Iteration 1280, loss = 87.3365
I0726 15:09:17.975409 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:09:17.975437 136306 sgd_solver.cpp:106] Iteration 1280, lr = 0.01
I0726 15:10:01.554561 136306 solver.cpp:236] Iteration 1290, loss = 87.3365
I0726 15:10:01.554785 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:10:01.554817 136306 sgd_solver.cpp:106] Iteration 1290, lr = 0.01
I0726 15:10:38.760355 136306 solver.cpp:236] Iteration 1300, loss = 87.3365
I0726 15:10:38.760599 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:10:38.760618 136306 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I0726 15:11:25.524727 136306 solver.cpp:236] Iteration 1310, loss = 87.3365
I0726 15:11:25.524933 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:11:25.524966 136306 sgd_solver.cpp:106] Iteration 1310, lr = 0.01
I0726 15:12:11.972940 136306 solver.cpp:236] Iteration 1320, loss = 87.3365
I0726 15:12:11.973193 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:12:11.973230 136306 sgd_solver.cpp:106] Iteration 1320, lr = 0.01
I0726 15:12:48.872138 136306 solver.cpp:236] Iteration 1330, loss = 87.3365
I0726 15:12:48.872367 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:12:48.872395 136306 sgd_solver.cpp:106] Iteration 1330, lr = 0.01
I0726 15:13:20.039031 136306 solver.cpp:236] Iteration 1340, loss = 87.3365
I0726 15:13:20.042721 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:13:20.042749 136306 sgd_solver.cpp:106] Iteration 1340, lr = 0.01
I0726 15:14:02.293858 136306 solver.cpp:236] Iteration 1350, loss = 87.3365
I0726 15:14:02.294287 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:14:02.294303 136306 sgd_solver.cpp:106] Iteration 1350, lr = 0.01
I0726 15:14:33.879043 136306 solver.cpp:236] Iteration 1360, loss = 87.3365
I0726 15:14:33.879232 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:14:33.879276 136306 sgd_solver.cpp:106] Iteration 1360, lr = 0.01
I0726 15:14:53.163733 136306 solver.cpp:236] Iteration 1370, loss = 87.3365
I0726 15:14:53.163791 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:14:53.163805 136306 sgd_solver.cpp:106] Iteration 1370, lr = 0.01
I0726 15:15:19.524546 136306 solver.cpp:236] Iteration 1380, loss = 87.3365
I0726 15:15:19.524845 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:15:19.524866 136306 sgd_solver.cpp:106] Iteration 1380, lr = 0.01
I0726 15:15:45.882776 136306 solver.cpp:236] Iteration 1390, loss = 87.3365
I0726 15:15:45.882839 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:15:45.882856 136306 sgd_solver.cpp:106] Iteration 1390, lr = 0.01
I0726 15:16:07.407757 136306 solver.cpp:236] Iteration 1400, loss = 87.3365
I0726 15:16:07.408057 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:16:07.408084 136306 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I0726 15:16:27.937676 136306 solver.cpp:236] Iteration 1410, loss = 87.3365
I0726 15:16:27.937758 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:16:27.937775 136306 sgd_solver.cpp:106] Iteration 1410, lr = 0.01
I0726 15:16:54.023588 136306 solver.cpp:236] Iteration 1420, loss = 87.3365
I0726 15:16:54.023810 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:16:54.023831 136306 sgd_solver.cpp:106] Iteration 1420, lr = 0.01
I0726 15:17:19.036389 136306 solver.cpp:236] Iteration 1430, loss = 87.3365
I0726 15:17:19.036460 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:17:19.036478 136306 sgd_solver.cpp:106] Iteration 1430, lr = 0.01
I0726 15:17:44.145654 136306 solver.cpp:236] Iteration 1440, loss = 87.3365
I0726 15:17:44.145859 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:17:44.145889 136306 sgd_solver.cpp:106] Iteration 1440, lr = 0.01
I0726 15:18:04.146874 136306 solver.cpp:236] Iteration 1450, loss = 87.3365
I0726 15:18:04.146958 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:18:04.146975 136306 sgd_solver.cpp:106] Iteration 1450, lr = 0.01
I0726 15:18:26.454164 136306 solver.cpp:236] Iteration 1460, loss = 87.3365
I0726 15:18:26.454385 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:18:26.454416 136306 sgd_solver.cpp:106] Iteration 1460, lr = 0.01
I0726 15:18:51.578542 136306 solver.cpp:236] Iteration 1470, loss = 87.3365
I0726 15:18:51.578611 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:18:51.578626 136306 sgd_solver.cpp:106] Iteration 1470, lr = 0.01
I0726 15:19:18.999027 136306 solver.cpp:236] Iteration 1480, loss = 87.3365
I0726 15:19:18.999318 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:19:18.999338 136306 sgd_solver.cpp:106] Iteration 1480, lr = 0.01
I0726 15:19:40.465114 136306 solver.cpp:236] Iteration 1490, loss = 87.3365
I0726 15:19:40.465204 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:19:40.465231 136306 sgd_solver.cpp:106] Iteration 1490, lr = 0.01
I0726 15:20:03.556851 136306 solver.cpp:340] Iteration 1500, Testing net (#0)
I0726 15:20:26.887542 136306 solver.cpp:408]     Test net output #0: accuracy = 0
I0726 15:20:26.887624 136306 solver.cpp:408]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:20:29.514783 136306 solver.cpp:236] Iteration 1500, loss = 87.3365
I0726 15:20:29.514843 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:20:29.514860 136306 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I0726 15:20:59.102731 136306 solver.cpp:236] Iteration 1510, loss = 87.3365
I0726 15:20:59.103088 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:20:59.103107 136306 sgd_solver.cpp:106] Iteration 1510, lr = 0.01
I0726 15:21:28.389966 136306 solver.cpp:236] Iteration 1520, loss = 87.3365
I0726 15:21:28.390036 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:21:28.390050 136306 sgd_solver.cpp:106] Iteration 1520, lr = 0.01
I0726 15:22:03.915349 136306 solver.cpp:236] Iteration 1530, loss = 87.3365
I0726 15:22:03.915599 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:22:03.915617 136306 sgd_solver.cpp:106] Iteration 1530, lr = 0.01
I0726 15:22:38.451206 136306 solver.cpp:236] Iteration 1540, loss = 87.3365
I0726 15:22:38.451419 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:22:38.451449 136306 sgd_solver.cpp:106] Iteration 1540, lr = 0.01
I0726 15:23:12.926254 136306 solver.cpp:236] Iteration 1550, loss = 87.3365
I0726 15:23:12.926503 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:23:12.926520 136306 sgd_solver.cpp:106] Iteration 1550, lr = 0.01
I0726 15:23:40.979007 136306 solver.cpp:236] Iteration 1560, loss = 87.3365
I0726 15:23:40.979087 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:23:40.979102 136306 sgd_solver.cpp:106] Iteration 1560, lr = 0.01
I0726 15:24:07.880094 136306 solver.cpp:236] Iteration 1570, loss = 87.3365
I0726 15:24:07.880348 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:24:07.880374 136306 sgd_solver.cpp:106] Iteration 1570, lr = 0.01
I0726 15:24:44.585234 136306 solver.cpp:236] Iteration 1580, loss = 87.3365
I0726 15:24:44.585427 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:24:44.585450 136306 sgd_solver.cpp:106] Iteration 1580, lr = 0.01
I0726 15:25:13.277318 136306 solver.cpp:236] Iteration 1590, loss = 87.3365
I0726 15:25:13.277400 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:25:13.277413 136306 sgd_solver.cpp:106] Iteration 1590, lr = 0.01
I0726 15:25:44.418972 136306 solver.cpp:236] Iteration 1600, loss = 87.3365
I0726 15:25:44.419186 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:25:44.419215 136306 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I0726 15:26:15.948153 136306 solver.cpp:236] Iteration 1610, loss = 87.3365
I0726 15:26:15.948379 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:26:15.948413 136306 sgd_solver.cpp:106] Iteration 1610, lr = 0.01
I0726 15:26:46.394695 136306 solver.cpp:236] Iteration 1620, loss = 87.3365
I0726 15:26:46.394955 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:26:46.394975 136306 sgd_solver.cpp:106] Iteration 1620, lr = 0.01
I0726 15:27:15.054541 136306 solver.cpp:236] Iteration 1630, loss = 87.3365
I0726 15:27:15.054625 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:27:15.054656 136306 sgd_solver.cpp:106] Iteration 1630, lr = 0.01
I0726 15:27:42.057705 136306 solver.cpp:236] Iteration 1640, loss = 87.3365
I0726 15:27:42.057914 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:27:42.057945 136306 sgd_solver.cpp:106] Iteration 1640, lr = 0.01
I0726 15:28:14.611907 136306 solver.cpp:236] Iteration 1650, loss = 87.3365
I0726 15:28:14.612102 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:28:14.612139 136306 sgd_solver.cpp:106] Iteration 1650, lr = 0.01
I0726 15:28:44.966526 136306 solver.cpp:236] Iteration 1660, loss = 87.3365
I0726 15:28:44.966835 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:28:44.966856 136306 sgd_solver.cpp:106] Iteration 1660, lr = 0.01
I0726 15:29:15.339898 136306 solver.cpp:236] Iteration 1670, loss = 87.3365
I0726 15:29:15.340128 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:29:15.340155 136306 sgd_solver.cpp:106] Iteration 1670, lr = 0.01
I0726 15:29:44.686929 136306 solver.cpp:236] Iteration 1680, loss = 87.3365
I0726 15:29:44.687001 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:29:44.687018 136306 sgd_solver.cpp:106] Iteration 1680, lr = 0.01
I0726 15:30:11.424654 136306 solver.cpp:236] Iteration 1690, loss = 87.3365
I0726 15:30:11.424871 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:30:11.424898 136306 sgd_solver.cpp:106] Iteration 1690, lr = 0.01
I0726 15:30:40.275444 136306 solver.cpp:236] Iteration 1700, loss = 87.3365
I0726 15:30:40.275513 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:30:40.275529 136306 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I0726 15:31:12.743438 136306 solver.cpp:236] Iteration 1710, loss = 87.3365
I0726 15:31:12.743649 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:31:12.743680 136306 sgd_solver.cpp:106] Iteration 1710, lr = 0.01
I0726 15:31:40.552131 136306 solver.cpp:236] Iteration 1720, loss = 87.3365
I0726 15:31:40.552211 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:31:40.552224 136306 sgd_solver.cpp:106] Iteration 1720, lr = 0.01
I0726 15:32:11.939851 136306 solver.cpp:236] Iteration 1730, loss = 87.3365
I0726 15:32:11.940106 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:32:11.940126 136306 sgd_solver.cpp:106] Iteration 1730, lr = 0.01
I0726 15:32:44.888933 136306 solver.cpp:236] Iteration 1740, loss = 87.3365
I0726 15:32:44.889163 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:32:44.889204 136306 sgd_solver.cpp:106] Iteration 1740, lr = 0.01
I0726 15:33:19.019986 136306 solver.cpp:236] Iteration 1750, loss = 87.3365
I0726 15:33:19.020190 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:33:19.020233 136306 sgd_solver.cpp:106] Iteration 1750, lr = 0.01
I0726 15:33:55.637127 136306 solver.cpp:236] Iteration 1760, loss = 87.3365
I0726 15:33:55.637339 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:33:55.637369 136306 sgd_solver.cpp:106] Iteration 1760, lr = 0.01
I0726 15:34:25.315868 136306 solver.cpp:236] Iteration 1770, loss = 87.3365
I0726 15:34:25.315948 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:34:25.315961 136306 sgd_solver.cpp:106] Iteration 1770, lr = 0.01
I0726 15:34:54.366699 136306 solver.cpp:236] Iteration 1780, loss = 87.3365
I0726 15:34:54.366901 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:34:54.366919 136306 sgd_solver.cpp:106] Iteration 1780, lr = 0.01
I0726 15:35:32.394207 136306 solver.cpp:236] Iteration 1790, loss = 87.3365
I0726 15:35:32.394474 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:35:32.394495 136306 sgd_solver.cpp:106] Iteration 1790, lr = 0.01
I0726 15:36:01.122530 136306 solver.cpp:236] Iteration 1800, loss = 87.3365
I0726 15:36:01.122606 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:36:01.122622 136306 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I0726 15:36:23.553234 136306 solver.cpp:236] Iteration 1810, loss = 87.3365
I0726 15:36:23.553493 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:36:23.553508 136306 sgd_solver.cpp:106] Iteration 1810, lr = 0.01
I0726 15:37:16.458341 136306 solver.cpp:236] Iteration 1820, loss = 87.3365
I0726 15:37:16.459719 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:37:16.459739 136306 sgd_solver.cpp:106] Iteration 1820, lr = 0.01
I0726 15:38:07.721598 136306 solver.cpp:236] Iteration 1830, loss = 87.3365
I0726 15:38:07.723544 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:38:07.723565 136306 sgd_solver.cpp:106] Iteration 1830, lr = 0.01
I0726 15:38:58.197449 136306 solver.cpp:236] Iteration 1840, loss = 87.3365
I0726 15:38:58.197674 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:38:58.197693 136306 sgd_solver.cpp:106] Iteration 1840, lr = 0.01
I0726 15:40:02.102403 136306 solver.cpp:236] Iteration 1850, loss = 87.3365
I0726 15:40:02.102602 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:40:02.102653 136306 sgd_solver.cpp:106] Iteration 1850, lr = 0.01
I0726 15:41:10.280635 136306 solver.cpp:236] Iteration 1860, loss = 87.3365
I0726 15:41:10.280860 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:41:10.280885 136306 sgd_solver.cpp:106] Iteration 1860, lr = 0.01
I0726 15:42:10.949596 136306 solver.cpp:236] Iteration 1870, loss = 87.3365
I0726 15:42:10.949823 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:42:10.949853 136306 sgd_solver.cpp:106] Iteration 1870, lr = 0.01
I0726 15:42:50.197180 136306 solver.cpp:236] Iteration 1880, loss = 87.3365
I0726 15:42:50.197374 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:42:50.197419 136306 sgd_solver.cpp:106] Iteration 1880, lr = 0.01
I0726 15:43:52.349043 136306 solver.cpp:236] Iteration 1890, loss = 87.3365
I0726 15:43:52.349220 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:43:52.349264 136306 sgd_solver.cpp:106] Iteration 1890, lr = 0.01
I0726 15:45:01.002980 136306 solver.cpp:236] Iteration 1900, loss = 87.3365
I0726 15:45:01.003176 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:45:01.003204 136306 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I0726 15:45:55.135794 136306 solver.cpp:236] Iteration 1910, loss = 87.3365
I0726 15:45:55.136000 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:45:55.136028 136306 sgd_solver.cpp:106] Iteration 1910, lr = 0.01
I0726 15:46:43.396033 136306 solver.cpp:236] Iteration 1920, loss = 87.3365
I0726 15:46:43.398836 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:46:43.398857 136306 sgd_solver.cpp:106] Iteration 1920, lr = 0.01
I0726 15:47:24.158646 136306 solver.cpp:236] Iteration 1930, loss = 87.3365
I0726 15:47:24.159294 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:47:24.159314 136306 sgd_solver.cpp:106] Iteration 1930, lr = 0.01
I0726 15:47:52.220793 136306 solver.cpp:236] Iteration 1940, loss = 87.3365
I0726 15:47:52.220856 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:47:52.220870 136306 sgd_solver.cpp:106] Iteration 1940, lr = 0.01
I0726 15:48:16.519098 136306 solver.cpp:236] Iteration 1950, loss = 87.3365
I0726 15:48:16.519368 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:48:16.519387 136306 sgd_solver.cpp:106] Iteration 1950, lr = 0.01
I0726 15:48:39.931746 136306 solver.cpp:236] Iteration 1960, loss = 87.3365
I0726 15:48:39.931816 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:48:39.931830 136306 sgd_solver.cpp:106] Iteration 1960, lr = 0.01
I0726 15:49:08.579676 136306 solver.cpp:236] Iteration 1970, loss = 87.3365
I0726 15:49:08.579921 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:49:08.579951 136306 sgd_solver.cpp:106] Iteration 1970, lr = 0.01
I0726 15:49:31.220047 136306 solver.cpp:236] Iteration 1980, loss = 87.3365
I0726 15:49:31.220116 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:49:31.220129 136306 sgd_solver.cpp:106] Iteration 1980, lr = 0.01
I0726 15:50:00.519042 136306 solver.cpp:236] Iteration 1990, loss = 87.3365
I0726 15:50:00.519381 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:50:00.519410 136306 sgd_solver.cpp:106] Iteration 1990, lr = 0.01
I0726 15:50:22.032853 136306 solver.cpp:461] Snapshotting to binary proto file models/gnet_iter_2000.caffemodel
I0726 15:50:22.125634 136306 sgd_solver.cpp:269] Snapshotting solver state to binary proto file models/gnet_iter_2000.solverstate
I0726 15:50:22.129387 136306 solver.cpp:340] Iteration 2000, Testing net (#0)
I0726 15:50:49.338204 136306 solver.cpp:408]     Test net output #0: accuracy = 0
I0726 15:50:49.338449 136306 solver.cpp:408]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:50:53.337004 136306 solver.cpp:236] Iteration 2000, loss = 87.3365
I0726 15:50:53.337079 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:50:53.337096 136306 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I0726 15:51:21.745779 136306 solver.cpp:236] Iteration 2010, loss = 87.3365
I0726 15:51:21.745980 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:51:21.746011 136306 sgd_solver.cpp:106] Iteration 2010, lr = 0.01
I0726 15:51:46.668606 136306 solver.cpp:236] Iteration 2020, loss = 87.3365
I0726 15:51:46.668679 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:51:46.668696 136306 sgd_solver.cpp:106] Iteration 2020, lr = 0.01
I0726 15:52:11.219350 136306 solver.cpp:236] Iteration 2030, loss = 87.3365
I0726 15:52:11.219521 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:52:11.219563 136306 sgd_solver.cpp:106] Iteration 2030, lr = 0.01
I0726 15:52:37.200271 136306 solver.cpp:236] Iteration 2040, loss = 87.3365
I0726 15:52:37.200352 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:52:37.200367 136306 sgd_solver.cpp:106] Iteration 2040, lr = 0.01
I0726 15:52:59.727880 136306 solver.cpp:236] Iteration 2050, loss = 87.3365
I0726 15:52:59.728152 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:52:59.728168 136306 sgd_solver.cpp:106] Iteration 2050, lr = 0.01
I0726 15:53:27.123276 136306 solver.cpp:236] Iteration 2060, loss = 87.3365
I0726 15:53:27.123374 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:53:27.123396 136306 sgd_solver.cpp:106] Iteration 2060, lr = 0.01
I0726 15:53:52.588966 136306 solver.cpp:236] Iteration 2070, loss = 87.3365
I0726 15:53:52.589150 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:53:52.589196 136306 sgd_solver.cpp:106] Iteration 2070, lr = 0.01
I0726 15:54:15.130373 136306 solver.cpp:236] Iteration 2080, loss = 87.3365
I0726 15:54:15.130440 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:54:15.130455 136306 sgd_solver.cpp:106] Iteration 2080, lr = 0.01
I0726 15:54:40.832720 136306 solver.cpp:236] Iteration 2090, loss = 87.3365
I0726 15:54:40.832965 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:54:40.832983 136306 sgd_solver.cpp:106] Iteration 2090, lr = 0.01
I0726 15:55:05.121089 136306 solver.cpp:236] Iteration 2100, loss = 87.3365
I0726 15:55:05.121163 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:55:05.121178 136306 sgd_solver.cpp:106] Iteration 2100, lr = 0.01
I0726 15:55:32.153110 136306 solver.cpp:236] Iteration 2110, loss = 87.3365
I0726 15:55:32.153304 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:55:32.153342 136306 sgd_solver.cpp:106] Iteration 2110, lr = 0.01
I0726 15:55:57.421634 136306 solver.cpp:236] Iteration 2120, loss = 87.3365
I0726 15:55:57.421705 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:55:57.421720 136306 sgd_solver.cpp:106] Iteration 2120, lr = 0.01
I0726 15:56:27.788110 136306 solver.cpp:236] Iteration 2130, loss = 87.3365
I0726 15:56:27.788368 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:56:27.788384 136306 sgd_solver.cpp:106] Iteration 2130, lr = 0.01
I0726 15:56:57.843830 136306 solver.cpp:236] Iteration 2140, loss = 87.3365
I0726 15:56:57.844072 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:56:57.844094 136306 sgd_solver.cpp:106] Iteration 2140, lr = 0.01
I0726 15:57:29.350811 136306 solver.cpp:236] Iteration 2150, loss = 87.3365
I0726 15:57:29.351047 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:57:29.351063 136306 sgd_solver.cpp:106] Iteration 2150, lr = 0.01
I0726 15:58:02.169066 136306 solver.cpp:236] Iteration 2160, loss = 87.3365
I0726 15:58:02.169275 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:58:02.169303 136306 sgd_solver.cpp:106] Iteration 2160, lr = 0.01
I0726 15:58:33.863641 136306 solver.cpp:236] Iteration 2170, loss = 87.3365
I0726 15:58:33.863800 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:58:33.863827 136306 sgd_solver.cpp:106] Iteration 2170, lr = 0.01
I0726 15:59:04.702987 136306 solver.cpp:236] Iteration 2180, loss = 87.3365
I0726 15:59:04.703186 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:59:04.703203 136306 sgd_solver.cpp:106] Iteration 2180, lr = 0.01
I0726 15:59:37.817811 136306 solver.cpp:236] Iteration 2190, loss = 87.3365
I0726 15:59:37.818070 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 15:59:37.818086 136306 sgd_solver.cpp:106] Iteration 2190, lr = 0.01
I0726 16:00:01.055965 136306 solver.cpp:236] Iteration 2200, loss = 87.3365
I0726 16:00:01.056032 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:00:01.056049 136306 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I0726 16:00:29.577591 136306 solver.cpp:236] Iteration 2210, loss = 87.3365
I0726 16:00:29.577781 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:00:29.577811 136306 sgd_solver.cpp:106] Iteration 2210, lr = 0.01
I0726 16:01:03.753371 136306 solver.cpp:236] Iteration 2220, loss = 87.3365
I0726 16:01:03.753569 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:01:03.753602 136306 sgd_solver.cpp:106] Iteration 2220, lr = 0.01
I0726 16:01:34.668206 136306 solver.cpp:236] Iteration 2230, loss = 87.3365
I0726 16:01:34.668432 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:01:34.668457 136306 sgd_solver.cpp:106] Iteration 2230, lr = 0.01
I0726 16:02:10.033871 136306 solver.cpp:236] Iteration 2240, loss = 87.3365
I0726 16:02:10.034059 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:02:10.034087 136306 sgd_solver.cpp:106] Iteration 2240, lr = 0.01
I0726 16:02:52.144333 136306 solver.cpp:236] Iteration 2250, loss = 87.3365
I0726 16:02:52.144510 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:02:52.144552 136306 sgd_solver.cpp:106] Iteration 2250, lr = 0.01
I0726 16:03:21.690661 136306 solver.cpp:236] Iteration 2260, loss = 87.3365
I0726 16:03:21.690742 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:03:21.690757 136306 sgd_solver.cpp:106] Iteration 2260, lr = 0.01
I0726 16:04:08.873625 136306 solver.cpp:236] Iteration 2270, loss = 87.3365
I0726 16:04:08.873832 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:04:08.873865 136306 sgd_solver.cpp:106] Iteration 2270, lr = 0.01
I0726 16:04:39.284260 136306 solver.cpp:236] Iteration 2280, loss = 87.3365
I0726 16:04:39.284545 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:04:39.284560 136306 sgd_solver.cpp:106] Iteration 2280, lr = 0.01
I0726 16:05:13.339164 136306 solver.cpp:236] Iteration 2290, loss = 87.3365
I0726 16:05:13.339395 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:05:13.339424 136306 sgd_solver.cpp:106] Iteration 2290, lr = 0.01
I0726 16:05:51.637892 136306 solver.cpp:236] Iteration 2300, loss = 87.3365
I0726 16:05:51.638089 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:05:51.638120 136306 sgd_solver.cpp:106] Iteration 2300, lr = 0.01
I0726 16:06:35.428170 136306 solver.cpp:236] Iteration 2310, loss = 87.3365
I0726 16:06:35.428372 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:06:35.428416 136306 sgd_solver.cpp:106] Iteration 2310, lr = 0.01
I0726 16:07:15.434170 136306 solver.cpp:236] Iteration 2320, loss = 87.3365
I0726 16:07:15.434406 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:07:15.434442 136306 sgd_solver.cpp:106] Iteration 2320, lr = 0.01
I0726 16:07:55.982607 136306 solver.cpp:236] Iteration 2330, loss = 87.3365
I0726 16:07:55.983259 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:07:55.983283 136306 sgd_solver.cpp:106] Iteration 2330, lr = 0.01
I0726 16:08:37.296177 136306 solver.cpp:236] Iteration 2340, loss = 87.3365
I0726 16:08:37.296432 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:08:37.296450 136306 sgd_solver.cpp:106] Iteration 2340, lr = 0.01
I0726 16:09:07.408329 136306 solver.cpp:236] Iteration 2350, loss = 87.3365
I0726 16:09:07.408530 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:09:07.408561 136306 sgd_solver.cpp:106] Iteration 2350, lr = 0.01
I0726 16:09:41.679474 136306 solver.cpp:236] Iteration 2360, loss = 87.3365
I0726 16:09:41.679754 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:09:41.679775 136306 sgd_solver.cpp:106] Iteration 2360, lr = 0.01
I0726 16:10:29.932639 136306 solver.cpp:236] Iteration 2370, loss = 87.3365
I0726 16:10:29.932826 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:10:29.932847 136306 sgd_solver.cpp:106] Iteration 2370, lr = 0.01
I0726 16:11:14.658040 136306 solver.cpp:236] Iteration 2380, loss = 87.3365
I0726 16:11:14.658284 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:11:14.658316 136306 sgd_solver.cpp:106] Iteration 2380, lr = 0.01
I0726 16:11:49.465505 136306 solver.cpp:236] Iteration 2390, loss = 87.3365
I0726 16:11:49.465692 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:11:49.465718 136306 sgd_solver.cpp:106] Iteration 2390, lr = 0.01
I0726 16:12:34.549300 136306 solver.cpp:236] Iteration 2400, loss = 87.3365
I0726 16:12:34.549504 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:12:34.549522 136306 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I0726 16:13:12.157937 136306 solver.cpp:236] Iteration 2410, loss = 87.3365
I0726 16:13:12.158164 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:13:12.158181 136306 sgd_solver.cpp:106] Iteration 2410, lr = 0.01
I0726 16:13:44.952538 136306 solver.cpp:236] Iteration 2420, loss = 87.3365
I0726 16:13:44.952718 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:13:44.952739 136306 sgd_solver.cpp:106] Iteration 2420, lr = 0.01
I0726 16:14:32.059597 136306 solver.cpp:236] Iteration 2430, loss = 87.3365
I0726 16:14:32.059793 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:14:32.059811 136306 sgd_solver.cpp:106] Iteration 2430, lr = 0.01
I0726 16:15:07.953804 136306 solver.cpp:236] Iteration 2440, loss = 87.3365
I0726 16:15:07.955281 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:15:07.955303 136306 sgd_solver.cpp:106] Iteration 2440, lr = 0.01
I0726 16:15:53.973721 136306 solver.cpp:236] Iteration 2450, loss = 87.3365
I0726 16:15:53.981930 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:15:53.981948 136306 sgd_solver.cpp:106] Iteration 2450, lr = 0.01
I0726 16:16:32.128839 136306 solver.cpp:236] Iteration 2460, loss = 87.3365
I0726 16:16:32.129081 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:16:32.129118 136306 sgd_solver.cpp:106] Iteration 2460, lr = 0.01
I0726 16:17:07.493180 136306 solver.cpp:236] Iteration 2470, loss = 87.3365
I0726 16:17:07.493396 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:17:07.493429 136306 sgd_solver.cpp:106] Iteration 2470, lr = 0.01
I0726 16:17:32.453464 136306 solver.cpp:236] Iteration 2480, loss = 87.3365
I0726 16:17:32.453539 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:17:32.453557 136306 sgd_solver.cpp:106] Iteration 2480, lr = 0.01
I0726 16:18:05.754766 136306 solver.cpp:236] Iteration 2490, loss = 87.3365
I0726 16:18:05.754914 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:18:05.754942 136306 sgd_solver.cpp:106] Iteration 2490, lr = 0.01
I0726 16:18:32.847285 136306 solver.cpp:340] Iteration 2500, Testing net (#0)
I0726 16:19:08.567451 136306 solver.cpp:408]     Test net output #0: accuracy = 0
I0726 16:19:08.567654 136306 solver.cpp:408]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:19:12.846408 136306 solver.cpp:236] Iteration 2500, loss = 87.3365
I0726 16:19:12.846473 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:19:12.846493 136306 sgd_solver.cpp:106] Iteration 2500, lr = 0.01
I0726 16:19:54.400723 136306 solver.cpp:236] Iteration 2510, loss = 87.3365
I0726 16:19:54.400923 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:19:54.400940 136306 sgd_solver.cpp:106] Iteration 2510, lr = 0.01
I0726 16:20:33.682770 136306 solver.cpp:236] Iteration 2520, loss = 87.3365
I0726 16:20:33.682920 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:20:33.682950 136306 sgd_solver.cpp:106] Iteration 2520, lr = 0.01
I0726 16:21:01.065066 136306 solver.cpp:236] Iteration 2530, loss = 87.3365
I0726 16:21:01.065142 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:21:01.065160 136306 sgd_solver.cpp:106] Iteration 2530, lr = 0.01
I0726 16:21:40.725786 136306 solver.cpp:236] Iteration 2540, loss = 87.3365
I0726 16:21:40.726068 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:21:40.726095 136306 sgd_solver.cpp:106] Iteration 2540, lr = 0.01
I0726 16:22:15.591377 136306 solver.cpp:236] Iteration 2550, loss = 87.3365
I0726 16:22:15.591547 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:22:15.591588 136306 sgd_solver.cpp:106] Iteration 2550, lr = 0.01
I0726 16:22:54.904129 136306 solver.cpp:236] Iteration 2560, loss = 87.3365
I0726 16:22:54.904325 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:22:54.904348 136306 sgd_solver.cpp:106] Iteration 2560, lr = 0.01
I0726 16:23:52.611585 136306 solver.cpp:236] Iteration 2570, loss = 87.3365
I0726 16:23:52.611745 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:23:52.611788 136306 sgd_solver.cpp:106] Iteration 2570, lr = 0.01
I0726 16:24:28.641463 136306 solver.cpp:236] Iteration 2580, loss = 87.3365
I0726 16:24:28.641669 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:24:28.641688 136306 sgd_solver.cpp:106] Iteration 2580, lr = 0.01
I0726 16:24:55.572518 136306 solver.cpp:236] Iteration 2590, loss = 87.3365
I0726 16:24:55.572589 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:24:55.572604 136306 sgd_solver.cpp:106] Iteration 2590, lr = 0.01
I0726 16:25:31.022958 136306 solver.cpp:236] Iteration 2600, loss = 87.3365
I0726 16:25:31.023229 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:25:31.023258 136306 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I0726 16:26:06.794685 136306 solver.cpp:236] Iteration 2610, loss = 87.3365
I0726 16:26:06.794926 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:26:06.794965 136306 sgd_solver.cpp:106] Iteration 2610, lr = 0.01
I0726 16:26:45.050767 136306 solver.cpp:236] Iteration 2620, loss = 87.3365
I0726 16:26:45.050938 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:26:45.050966 136306 sgd_solver.cpp:106] Iteration 2620, lr = 0.01
I0726 16:27:25.649415 136306 solver.cpp:236] Iteration 2630, loss = 87.3365
I0726 16:27:25.649590 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:27:25.649636 136306 sgd_solver.cpp:106] Iteration 2630, lr = 0.01
I0726 16:28:04.296692 136306 solver.cpp:236] Iteration 2640, loss = 87.3365
I0726 16:28:04.296870 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:28:04.296911 136306 sgd_solver.cpp:106] Iteration 2640, lr = 0.01
I0726 16:28:38.440980 136306 solver.cpp:236] Iteration 2650, loss = 87.3365
I0726 16:28:38.441190 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:28:38.441220 136306 sgd_solver.cpp:106] Iteration 2650, lr = 0.01
I0726 16:29:14.686617 136306 solver.cpp:236] Iteration 2660, loss = 87.3365
I0726 16:29:14.686864 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:29:14.686902 136306 sgd_solver.cpp:106] Iteration 2660, lr = 0.01
I0726 16:29:53.648047 136306 solver.cpp:236] Iteration 2670, loss = 87.3365
I0726 16:29:53.648264 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:29:53.648303 136306 sgd_solver.cpp:106] Iteration 2670, lr = 0.01
I0726 16:30:36.442587 136306 solver.cpp:236] Iteration 2680, loss = 87.3365
I0726 16:30:36.442813 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:30:36.442833 136306 sgd_solver.cpp:106] Iteration 2680, lr = 0.01
I0726 16:31:16.516314 136306 solver.cpp:236] Iteration 2690, loss = 87.3365
I0726 16:31:16.516541 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:31:16.516561 136306 sgd_solver.cpp:106] Iteration 2690, lr = 0.01
I0726 16:31:56.620204 136306 solver.cpp:236] Iteration 2700, loss = 87.3365
I0726 16:31:56.620448 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:31:56.620481 136306 sgd_solver.cpp:106] Iteration 2700, lr = 0.01
I0726 16:32:38.389498 136306 solver.cpp:236] Iteration 2710, loss = 87.3365
I0726 16:32:38.389732 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:32:38.389752 136306 sgd_solver.cpp:106] Iteration 2710, lr = 0.01
I0726 16:33:16.273396 136306 solver.cpp:236] Iteration 2720, loss = 87.3365
I0726 16:33:16.273603 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:33:16.273635 136306 sgd_solver.cpp:106] Iteration 2720, lr = 0.01
I0726 16:34:00.203763 136306 solver.cpp:236] Iteration 2730, loss = 87.3365
I0726 16:34:00.203989 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:34:00.204032 136306 sgd_solver.cpp:106] Iteration 2730, lr = 0.01
I0726 16:34:35.392814 136306 solver.cpp:236] Iteration 2740, loss = 87.3365
I0726 16:34:35.393033 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:34:35.393066 136306 sgd_solver.cpp:106] Iteration 2740, lr = 0.01
I0726 16:35:17.345651 136306 solver.cpp:236] Iteration 2750, loss = 87.3365
I0726 16:35:17.345960 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:35:17.345983 136306 sgd_solver.cpp:106] Iteration 2750, lr = 0.01
I0726 16:35:51.785817 136306 solver.cpp:236] Iteration 2760, loss = 87.3365
I0726 16:35:51.786046 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:35:51.786069 136306 sgd_solver.cpp:106] Iteration 2760, lr = 0.01
I0726 16:36:30.924744 136306 solver.cpp:236] Iteration 2770, loss = 87.3365
I0726 16:36:30.924965 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:36:30.924984 136306 sgd_solver.cpp:106] Iteration 2770, lr = 0.01
I0726 16:37:06.982501 136306 solver.cpp:236] Iteration 2780, loss = 87.3365
I0726 16:37:06.982722 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:37:06.982760 136306 sgd_solver.cpp:106] Iteration 2780, lr = 0.01
I0726 16:37:51.607192 136306 solver.cpp:236] Iteration 2790, loss = 87.3365
I0726 16:37:51.607429 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:37:51.607468 136306 sgd_solver.cpp:106] Iteration 2790, lr = 0.01
I0726 16:38:28.987381 136306 solver.cpp:236] Iteration 2800, loss = 87.3365
I0726 16:38:28.987576 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:38:28.987604 136306 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I0726 16:38:54.392482 136306 solver.cpp:236] Iteration 2810, loss = 87.3365
I0726 16:38:54.392576 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:38:54.392592 136306 sgd_solver.cpp:106] Iteration 2810, lr = 0.01
I0726 16:39:19.026523 136306 solver.cpp:236] Iteration 2820, loss = 87.3365
I0726 16:39:19.026728 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:39:19.026757 136306 sgd_solver.cpp:106] Iteration 2820, lr = 0.01
I0726 16:40:12.756816 136306 solver.cpp:236] Iteration 2830, loss = 87.3365
I0726 16:40:12.757046 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:40:12.757064 136306 sgd_solver.cpp:106] Iteration 2830, lr = 0.01
I0726 16:40:43.737422 136306 solver.cpp:236] Iteration 2840, loss = 87.3365
I0726 16:40:43.737609 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:40:43.737637 136306 sgd_solver.cpp:106] Iteration 2840, lr = 0.01
I0726 16:41:26.602493 136306 solver.cpp:236] Iteration 2850, loss = 87.3365
I0726 16:41:26.602725 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:41:26.602757 136306 sgd_solver.cpp:106] Iteration 2850, lr = 0.01
I0726 16:42:09.584785 136306 solver.cpp:236] Iteration 2860, loss = 87.3365
I0726 16:42:09.585011 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:42:09.585029 136306 sgd_solver.cpp:106] Iteration 2860, lr = 0.01
I0726 16:42:47.496644 136306 solver.cpp:236] Iteration 2870, loss = 87.3365
I0726 16:42:47.496805 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:42:47.496831 136306 sgd_solver.cpp:106] Iteration 2870, lr = 0.01
I0726 16:43:27.465276 136306 solver.cpp:236] Iteration 2880, loss = 87.3365
I0726 16:43:27.465451 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:43:27.465469 136306 sgd_solver.cpp:106] Iteration 2880, lr = 0.01
I0726 16:44:14.574952 136306 solver.cpp:236] Iteration 2890, loss = 87.3365
I0726 16:44:14.575124 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:44:14.575145 136306 sgd_solver.cpp:106] Iteration 2890, lr = 0.01
I0726 16:45:08.682061 136306 solver.cpp:236] Iteration 2900, loss = 87.3365
I0726 16:45:08.682286 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:45:08.682315 136306 sgd_solver.cpp:106] Iteration 2900, lr = 0.01
I0726 16:45:55.410717 136306 solver.cpp:236] Iteration 2910, loss = 87.3365
I0726 16:45:55.410917 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:45:55.410934 136306 sgd_solver.cpp:106] Iteration 2910, lr = 0.01
I0726 16:46:45.113272 136306 solver.cpp:236] Iteration 2920, loss = 87.3365
I0726 16:46:45.113488 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:46:45.113507 136306 sgd_solver.cpp:106] Iteration 2920, lr = 0.01
I0726 16:47:53.662282 136306 solver.cpp:236] Iteration 2930, loss = 87.3365
I0726 16:47:53.662461 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:47:53.662479 136306 sgd_solver.cpp:106] Iteration 2930, lr = 0.01
I0726 16:48:51.856717 136306 solver.cpp:236] Iteration 2940, loss = 87.3365
I0726 16:48:51.856909 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:48:51.856938 136306 sgd_solver.cpp:106] Iteration 2940, lr = 0.01
I0726 16:49:48.989292 136306 solver.cpp:236] Iteration 2950, loss = 87.3365
I0726 16:49:48.989480 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:49:48.989508 136306 sgd_solver.cpp:106] Iteration 2950, lr = 0.01
I0726 16:50:43.994688 136306 solver.cpp:236] Iteration 2960, loss = 87.3365
I0726 16:50:43.994900 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:50:43.994918 136306 sgd_solver.cpp:106] Iteration 2960, lr = 0.01
I0726 16:51:55.700382 136306 solver.cpp:236] Iteration 2970, loss = 87.3365
I0726 16:51:55.700688 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:51:55.700719 136306 sgd_solver.cpp:106] Iteration 2970, lr = 0.01
I0726 16:53:04.569604 136306 solver.cpp:236] Iteration 2980, loss = 87.3365
I0726 16:53:04.569772 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:53:04.569799 136306 sgd_solver.cpp:106] Iteration 2980, lr = 0.01
I0726 16:53:38.955128 136306 solver.cpp:236] Iteration 2990, loss = 87.3365
I0726 16:53:38.955387 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:53:38.955407 136306 sgd_solver.cpp:106] Iteration 2990, lr = 0.01
I0726 16:54:12.393229 136306 solver.cpp:461] Snapshotting to binary proto file models/gnet_iter_3000.caffemodel
I0726 16:54:12.498802 136306 sgd_solver.cpp:269] Snapshotting solver state to binary proto file models/gnet_iter_3000.solverstate
I0726 16:54:12.502679 136306 solver.cpp:340] Iteration 3000, Testing net (#0)
I0726 16:54:32.522400 136306 solver.cpp:408]     Test net output #0: accuracy = 0
I0726 16:54:32.522471 136306 solver.cpp:408]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:54:34.242638 136306 solver.cpp:236] Iteration 3000, loss = 87.3365
I0726 16:54:34.242712 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:54:34.242727 136306 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I0726 16:54:53.665853 136306 solver.cpp:236] Iteration 3010, loss = 87.3365
I0726 16:54:53.666056 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:54:53.666072 136306 sgd_solver.cpp:106] Iteration 3010, lr = 0.01
I0726 16:55:09.165170 136306 solver.cpp:236] Iteration 3020, loss = 87.3365
I0726 16:55:09.165241 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:55:09.165254 136306 sgd_solver.cpp:106] Iteration 3020, lr = 0.01
I0726 16:55:30.902315 136306 solver.cpp:236] Iteration 3030, loss = 87.3365
I0726 16:55:30.902575 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:55:30.902591 136306 sgd_solver.cpp:106] Iteration 3030, lr = 0.01
I0726 16:55:46.404395 136306 solver.cpp:236] Iteration 3040, loss = 87.3365
I0726 16:55:46.404458 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:55:46.404471 136306 sgd_solver.cpp:106] Iteration 3040, lr = 0.01
I0726 16:56:04.987452 136306 solver.cpp:236] Iteration 3050, loss = 87.3365
I0726 16:56:04.987695 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:56:04.987710 136306 sgd_solver.cpp:106] Iteration 3050, lr = 0.01
I0726 16:56:22.154916 136306 solver.cpp:236] Iteration 3060, loss = 87.3365
I0726 16:56:22.154980 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:56:22.154994 136306 sgd_solver.cpp:106] Iteration 3060, lr = 0.01
I0726 16:56:42.823032 136306 solver.cpp:236] Iteration 3070, loss = 87.3365
I0726 16:56:42.823310 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:56:42.823343 136306 sgd_solver.cpp:106] Iteration 3070, lr = 0.01
I0726 16:56:59.778820 136306 solver.cpp:236] Iteration 3080, loss = 87.3365
I0726 16:56:59.778887 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:56:59.778900 136306 sgd_solver.cpp:106] Iteration 3080, lr = 0.01
I0726 16:57:20.291613 136306 solver.cpp:236] Iteration 3090, loss = 87.3365
I0726 16:57:20.291815 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:57:20.291829 136306 sgd_solver.cpp:106] Iteration 3090, lr = 0.01
I0726 16:57:40.239425 136306 solver.cpp:236] Iteration 3100, loss = 87.3365
I0726 16:57:40.239487 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:57:40.239501 136306 sgd_solver.cpp:106] Iteration 3100, lr = 0.01
I0726 16:57:57.867832 136306 solver.cpp:236] Iteration 3110, loss = 87.3365
I0726 16:57:57.868034 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:57:57.868052 136306 sgd_solver.cpp:106] Iteration 3110, lr = 0.01
I0726 16:58:19.701782 136306 solver.cpp:236] Iteration 3120, loss = 87.3365
I0726 16:58:19.701834 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:58:19.701846 136306 sgd_solver.cpp:106] Iteration 3120, lr = 0.01
I0726 16:58:37.832509 136306 solver.cpp:236] Iteration 3130, loss = 87.3365
I0726 16:58:37.832682 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:58:37.832710 136306 sgd_solver.cpp:106] Iteration 3130, lr = 0.01
I0726 16:58:55.340636 136306 solver.cpp:236] Iteration 3140, loss = 87.3365
I0726 16:58:55.340693 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:58:55.340708 136306 sgd_solver.cpp:106] Iteration 3140, lr = 0.01
I0726 16:59:12.703495 136306 solver.cpp:236] Iteration 3150, loss = 87.3365
I0726 16:59:12.703692 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:59:12.703707 136306 sgd_solver.cpp:106] Iteration 3150, lr = 0.01
I0726 16:59:30.856631 136306 solver.cpp:236] Iteration 3160, loss = 87.3365
I0726 16:59:30.856696 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:59:30.856710 136306 sgd_solver.cpp:106] Iteration 3160, lr = 0.01
I0726 16:59:52.273512 136306 solver.cpp:236] Iteration 3170, loss = 87.3365
I0726 16:59:52.273694 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 16:59:52.273722 136306 sgd_solver.cpp:106] Iteration 3170, lr = 0.01
I0726 17:00:11.776521 136306 solver.cpp:236] Iteration 3180, loss = 87.3365
I0726 17:00:11.776587 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:00:11.776602 136306 sgd_solver.cpp:106] Iteration 3180, lr = 0.01
I0726 17:00:27.419786 136306 solver.cpp:236] Iteration 3190, loss = 87.3365
I0726 17:00:27.419948 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:00:27.419992 136306 sgd_solver.cpp:106] Iteration 3190, lr = 0.01
I0726 17:00:44.856328 136306 solver.cpp:236] Iteration 3200, loss = 87.3365
I0726 17:00:44.856390 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:00:44.856405 136306 sgd_solver.cpp:106] Iteration 3200, lr = 0.01
I0726 17:01:03.133961 136306 solver.cpp:236] Iteration 3210, loss = 87.3365
I0726 17:01:03.134227 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:01:03.134243 136306 sgd_solver.cpp:106] Iteration 3210, lr = 0.01
I0726 17:01:19.429366 136306 solver.cpp:236] Iteration 3220, loss = 87.3365
I0726 17:01:19.429435 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:01:19.429448 136306 sgd_solver.cpp:106] Iteration 3220, lr = 0.01
I0726 17:01:39.198915 136306 solver.cpp:236] Iteration 3230, loss = 87.3365
I0726 17:01:39.199213 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:01:39.199230 136306 sgd_solver.cpp:106] Iteration 3230, lr = 0.01
I0726 17:02:06.004094 136306 solver.cpp:236] Iteration 3240, loss = 87.3365
I0726 17:02:06.004164 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:02:06.004179 136306 sgd_solver.cpp:106] Iteration 3240, lr = 0.01
I0726 17:02:23.631669 136306 solver.cpp:236] Iteration 3250, loss = 87.3365
I0726 17:02:23.631793 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:02:23.631819 136306 sgd_solver.cpp:106] Iteration 3250, lr = 0.01
I0726 17:02:44.403482 136306 solver.cpp:236] Iteration 3260, loss = 87.3365
I0726 17:02:44.403553 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:02:44.403568 136306 sgd_solver.cpp:106] Iteration 3260, lr = 0.01
I0726 17:03:03.890256 136306 solver.cpp:236] Iteration 3270, loss = 87.3365
I0726 17:03:03.890436 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:03:03.890451 136306 sgd_solver.cpp:106] Iteration 3270, lr = 0.01
I0726 17:03:22.767087 136306 solver.cpp:236] Iteration 3280, loss = 87.3365
I0726 17:03:22.767158 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:03:22.767171 136306 sgd_solver.cpp:106] Iteration 3280, lr = 0.01
I0726 17:03:42.356350 136306 solver.cpp:236] Iteration 3290, loss = 87.3365
I0726 17:03:42.356554 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:03:42.356582 136306 sgd_solver.cpp:106] Iteration 3290, lr = 0.01
I0726 17:04:02.891144 136306 solver.cpp:236] Iteration 3300, loss = 87.3365
I0726 17:04:02.891216 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:04:02.891229 136306 sgd_solver.cpp:106] Iteration 3300, lr = 0.01
I0726 17:04:22.176506 136306 solver.cpp:236] Iteration 3310, loss = 87.3365
I0726 17:04:22.176748 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:04:22.176764 136306 sgd_solver.cpp:106] Iteration 3310, lr = 0.01
I0726 17:04:40.311225 136306 solver.cpp:236] Iteration 3320, loss = 87.3365
I0726 17:04:40.311277 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:04:40.311290 136306 sgd_solver.cpp:106] Iteration 3320, lr = 0.01
I0726 17:04:59.927359 136306 solver.cpp:236] Iteration 3330, loss = 87.3365
I0726 17:04:59.927614 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:04:59.927634 136306 sgd_solver.cpp:106] Iteration 3330, lr = 0.01
I0726 17:05:19.886548 136306 solver.cpp:236] Iteration 3340, loss = 87.3365
I0726 17:05:19.886617 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:05:19.886637 136306 sgd_solver.cpp:106] Iteration 3340, lr = 0.01
I0726 17:05:37.234016 136306 solver.cpp:236] Iteration 3350, loss = 87.3365
I0726 17:05:37.234385 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:05:37.234402 136306 sgd_solver.cpp:106] Iteration 3350, lr = 0.01
I0726 17:05:53.011782 136306 solver.cpp:236] Iteration 3360, loss = 87.3365
I0726 17:05:53.011855 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:05:53.011869 136306 sgd_solver.cpp:106] Iteration 3360, lr = 0.01
I0726 17:06:11.571230 136306 solver.cpp:236] Iteration 3370, loss = 87.3365
I0726 17:06:11.571467 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:06:11.571483 136306 sgd_solver.cpp:106] Iteration 3370, lr = 0.01
I0726 17:06:29.255107 136306 solver.cpp:236] Iteration 3380, loss = 87.3365
I0726 17:06:29.255178 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:06:29.255192 136306 sgd_solver.cpp:106] Iteration 3380, lr = 0.01
I0726 17:06:50.686197 136306 solver.cpp:236] Iteration 3390, loss = 87.3365
I0726 17:06:50.686416 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:06:50.686439 136306 sgd_solver.cpp:106] Iteration 3390, lr = 0.01
I0726 17:07:09.421963 136306 solver.cpp:236] Iteration 3400, loss = 87.3365
I0726 17:07:09.422034 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:07:09.422049 136306 sgd_solver.cpp:106] Iteration 3400, lr = 0.01
I0726 17:07:26.174037 136306 solver.cpp:236] Iteration 3410, loss = 87.3365
I0726 17:07:26.174213 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:07:26.174253 136306 sgd_solver.cpp:106] Iteration 3410, lr = 0.01
I0726 17:07:45.322867 136306 solver.cpp:236] Iteration 3420, loss = 87.3365
I0726 17:07:45.322942 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:07:45.322957 136306 sgd_solver.cpp:106] Iteration 3420, lr = 0.01
I0726 17:08:04.956743 136306 solver.cpp:236] Iteration 3430, loss = 87.3365
I0726 17:08:04.956920 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:08:04.956960 136306 sgd_solver.cpp:106] Iteration 3430, lr = 0.01
I0726 17:08:21.586019 136306 solver.cpp:236] Iteration 3440, loss = 87.3365
I0726 17:08:21.586091 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:08:21.586104 136306 sgd_solver.cpp:106] Iteration 3440, lr = 0.01
I0726 17:08:39.034409 136306 solver.cpp:236] Iteration 3450, loss = 87.3365
I0726 17:08:39.034591 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:08:39.034651 136306 sgd_solver.cpp:106] Iteration 3450, lr = 0.01
I0726 17:08:57.057709 136306 solver.cpp:236] Iteration 3460, loss = 87.3365
I0726 17:08:57.057775 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:08:57.057790 136306 sgd_solver.cpp:106] Iteration 3460, lr = 0.01
I0726 17:09:15.077332 136306 solver.cpp:236] Iteration 3470, loss = 87.3365
I0726 17:09:15.077510 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:09:15.077524 136306 sgd_solver.cpp:106] Iteration 3470, lr = 0.01
I0726 17:09:35.586287 136306 solver.cpp:236] Iteration 3480, loss = 87.3365
I0726 17:09:35.586350 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:09:35.586364 136306 sgd_solver.cpp:106] Iteration 3480, lr = 0.01
I0726 17:09:52.365985 136306 solver.cpp:236] Iteration 3490, loss = 87.3365
I0726 17:09:52.366180 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:09:52.366195 136306 sgd_solver.cpp:106] Iteration 3490, lr = 0.01
I0726 17:10:04.978160 136306 solver.cpp:340] Iteration 3500, Testing net (#0)
I0726 17:10:22.934502 136306 solver.cpp:408]     Test net output #0: accuracy = 0
I0726 17:10:22.934689 136306 solver.cpp:408]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:10:25.362231 136306 solver.cpp:236] Iteration 3500, loss = 87.3365
I0726 17:10:25.362298 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:10:25.362311 136306 sgd_solver.cpp:106] Iteration 3500, lr = 0.01
I0726 17:10:44.126895 136306 solver.cpp:236] Iteration 3510, loss = 87.3365
I0726 17:10:44.126945 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:10:44.126960 136306 sgd_solver.cpp:106] Iteration 3510, lr = 0.01
I0726 17:11:00.218641 136306 solver.cpp:236] Iteration 3520, loss = 87.3365
I0726 17:11:00.218921 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:11:00.218941 136306 sgd_solver.cpp:106] Iteration 3520, lr = 0.01
I0726 17:11:19.781743 136306 solver.cpp:236] Iteration 3530, loss = 87.3365
I0726 17:11:19.781808 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:11:19.781822 136306 sgd_solver.cpp:106] Iteration 3530, lr = 0.01
I0726 17:11:39.552793 136306 solver.cpp:236] Iteration 3540, loss = 87.3365
I0726 17:11:39.552999 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:11:39.553014 136306 sgd_solver.cpp:106] Iteration 3540, lr = 0.01
I0726 17:11:56.710176 136306 solver.cpp:236] Iteration 3550, loss = 87.3365
I0726 17:11:56.710243 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:11:56.710258 136306 sgd_solver.cpp:106] Iteration 3550, lr = 0.01
I0726 17:12:15.855139 136306 solver.cpp:236] Iteration 3560, loss = 87.3365
I0726 17:12:15.855334 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:12:15.855348 136306 sgd_solver.cpp:106] Iteration 3560, lr = 0.01
I0726 17:12:34.536442 136306 solver.cpp:236] Iteration 3570, loss = 87.3365
I0726 17:12:34.536509 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:12:34.536525 136306 sgd_solver.cpp:106] Iteration 3570, lr = 0.01
I0726 17:12:53.523172 136306 solver.cpp:236] Iteration 3580, loss = 87.3365
I0726 17:12:53.523445 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:12:53.523476 136306 sgd_solver.cpp:106] Iteration 3580, lr = 0.01
I0726 17:13:10.589058 136306 solver.cpp:236] Iteration 3590, loss = 87.3365
I0726 17:13:10.589128 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:13:10.589141 136306 sgd_solver.cpp:106] Iteration 3590, lr = 0.01
I0726 17:13:27.578755 136306 solver.cpp:236] Iteration 3600, loss = 87.3365
I0726 17:13:27.578994 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:13:27.579012 136306 sgd_solver.cpp:106] Iteration 3600, lr = 0.01
I0726 17:13:48.140553 136306 solver.cpp:236] Iteration 3610, loss = 87.3365
I0726 17:13:48.140619 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:13:48.140633 136306 sgd_solver.cpp:106] Iteration 3610, lr = 0.01
I0726 17:14:03.688962 136306 solver.cpp:236] Iteration 3620, loss = 87.3365
I0726 17:14:03.689189 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:14:03.689204 136306 sgd_solver.cpp:106] Iteration 3620, lr = 0.01
I0726 17:14:25.939561 136306 solver.cpp:236] Iteration 3630, loss = 87.3365
I0726 17:14:25.939635 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:14:25.939649 136306 sgd_solver.cpp:106] Iteration 3630, lr = 0.01
I0726 17:14:46.772481 136306 solver.cpp:236] Iteration 3640, loss = 87.3365
I0726 17:14:46.772658 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:14:46.772686 136306 sgd_solver.cpp:106] Iteration 3640, lr = 0.01
I0726 17:15:19.271464 136306 solver.cpp:236] Iteration 3650, loss = 87.3365
I0726 17:15:19.271608 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:15:19.271626 136306 sgd_solver.cpp:106] Iteration 3650, lr = 0.01
I0726 17:16:04.766178 136306 solver.cpp:236] Iteration 3660, loss = 87.3365
I0726 17:16:04.766401 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:16:04.766417 136306 sgd_solver.cpp:106] Iteration 3660, lr = 0.01
I0726 17:16:29.357172 136306 solver.cpp:236] Iteration 3670, loss = 87.3365
I0726 17:16:29.357249 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:16:29.357264 136306 sgd_solver.cpp:106] Iteration 3670, lr = 0.01
I0726 17:16:55.343924 136306 solver.cpp:236] Iteration 3680, loss = 87.3365
I0726 17:16:55.344130 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:16:55.344154 136306 sgd_solver.cpp:106] Iteration 3680, lr = 0.01
I0726 17:17:48.676045 136306 solver.cpp:236] Iteration 3690, loss = 87.3365
I0726 17:17:48.676281 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:17:48.676300 136306 sgd_solver.cpp:106] Iteration 3690, lr = 0.01
I0726 17:18:17.201284 136306 solver.cpp:236] Iteration 3700, loss = 87.3365
I0726 17:18:17.201359 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:18:17.201375 136306 sgd_solver.cpp:106] Iteration 3700, lr = 0.01
I0726 17:18:35.253836 136306 solver.cpp:236] Iteration 3710, loss = 87.3365
I0726 17:18:35.254035 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:18:35.254051 136306 sgd_solver.cpp:106] Iteration 3710, lr = 0.01
I0726 17:19:01.520519 136306 solver.cpp:236] Iteration 3720, loss = 87.3365
I0726 17:19:01.520596 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:19:01.520612 136306 sgd_solver.cpp:106] Iteration 3720, lr = 0.01
I0726 17:19:31.557461 136306 solver.cpp:236] Iteration 3730, loss = 87.3365
I0726 17:19:31.557694 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:19:31.557711 136306 sgd_solver.cpp:106] Iteration 3730, lr = 0.01
I0726 17:19:50.729894 136306 solver.cpp:236] Iteration 3740, loss = 87.3365
I0726 17:19:50.729969 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:19:50.729982 136306 sgd_solver.cpp:106] Iteration 3740, lr = 0.01
I0726 17:20:13.143519 136306 solver.cpp:236] Iteration 3750, loss = 87.3365
I0726 17:20:13.143753 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:20:13.143771 136306 sgd_solver.cpp:106] Iteration 3750, lr = 0.01
I0726 17:20:39.840335 136306 solver.cpp:236] Iteration 3760, loss = 87.3365
I0726 17:20:39.840412 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:20:39.840428 136306 sgd_solver.cpp:106] Iteration 3760, lr = 0.01
I0726 17:20:57.913513 136306 solver.cpp:236] Iteration 3770, loss = 87.3365
I0726 17:20:57.913664 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:20:57.913696 136306 sgd_solver.cpp:106] Iteration 3770, lr = 0.01
I0726 17:21:15.990335 136306 solver.cpp:236] Iteration 3780, loss = 87.3365
I0726 17:21:15.990407 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:21:15.990422 136306 sgd_solver.cpp:106] Iteration 3780, lr = 0.01
I0726 17:21:50.522353 136306 solver.cpp:236] Iteration 3790, loss = 87.3365
I0726 17:21:50.522562 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:21:50.522580 136306 sgd_solver.cpp:106] Iteration 3790, lr = 0.01
I0726 17:22:47.811725 136306 solver.cpp:236] Iteration 3800, loss = 87.3365
I0726 17:22:47.811899 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:22:47.811933 136306 sgd_solver.cpp:106] Iteration 3800, lr = 0.01
I0726 17:23:48.228309 136306 solver.cpp:236] Iteration 3810, loss = 87.3365
I0726 17:23:48.228499 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:23:48.228526 136306 sgd_solver.cpp:106] Iteration 3810, lr = 0.01
I0726 17:24:49.554316 136306 solver.cpp:236] Iteration 3820, loss = 87.3365
I0726 17:24:49.554505 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:24:49.554522 136306 sgd_solver.cpp:106] Iteration 3820, lr = 0.01
I0726 17:25:52.446559 136306 solver.cpp:236] Iteration 3830, loss = 87.3365
I0726 17:25:52.446982 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:25:52.447005 136306 sgd_solver.cpp:106] Iteration 3830, lr = 0.01
I0726 17:26:36.350177 136306 solver.cpp:236] Iteration 3840, loss = 87.3365
I0726 17:26:36.350519 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:26:36.350545 136306 sgd_solver.cpp:106] Iteration 3840, lr = 0.01
I0726 17:27:33.878022 136306 solver.cpp:236] Iteration 3850, loss = 87.3365
I0726 17:27:33.878192 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:27:33.878211 136306 sgd_solver.cpp:106] Iteration 3850, lr = 0.01
I0726 17:28:17.257583 136306 solver.cpp:236] Iteration 3860, loss = 87.3365
I0726 17:28:17.257812 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:28:17.257828 136306 sgd_solver.cpp:106] Iteration 3860, lr = 0.01
I0726 17:28:50.598877 136306 solver.cpp:236] Iteration 3870, loss = 87.3365
I0726 17:28:50.599120 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:28:50.599140 136306 sgd_solver.cpp:106] Iteration 3870, lr = 0.01
I0726 17:29:24.672538 136306 solver.cpp:236] Iteration 3880, loss = 87.3365
I0726 17:29:24.672868 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:29:24.672890 136306 sgd_solver.cpp:106] Iteration 3880, lr = 0.01
I0726 17:29:59.941170 136306 solver.cpp:236] Iteration 3890, loss = 87.3365
I0726 17:29:59.941339 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:29:59.941368 136306 sgd_solver.cpp:106] Iteration 3890, lr = 0.01
I0726 17:30:43.865020 136306 solver.cpp:236] Iteration 3900, loss = 87.3365
I0726 17:30:43.865241 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:30:43.865275 136306 sgd_solver.cpp:106] Iteration 3900, lr = 0.01
I0726 17:31:21.913472 136306 solver.cpp:236] Iteration 3910, loss = 87.3365
I0726 17:31:21.922694 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:31:21.922719 136306 sgd_solver.cpp:106] Iteration 3910, lr = 0.01
I0726 17:31:46.744074 136306 solver.cpp:236] Iteration 3920, loss = 87.3365
I0726 17:31:46.744159 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:31:46.744179 136306 sgd_solver.cpp:106] Iteration 3920, lr = 0.01
I0726 17:32:08.899065 136306 solver.cpp:236] Iteration 3930, loss = 87.3365
I0726 17:32:08.899260 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:32:08.899289 136306 sgd_solver.cpp:106] Iteration 3930, lr = 0.01
I0726 17:32:32.755408 136306 solver.cpp:236] Iteration 3940, loss = 87.3365
I0726 17:32:32.755481 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:32:32.755496 136306 sgd_solver.cpp:106] Iteration 3940, lr = 0.01
I0726 17:32:51.288527 136306 solver.cpp:236] Iteration 3950, loss = 87.3365
I0726 17:32:51.288878 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:32:51.288894 136306 sgd_solver.cpp:106] Iteration 3950, lr = 0.01
I0726 17:33:09.929304 136306 solver.cpp:236] Iteration 3960, loss = 87.3365
I0726 17:33:09.929369 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:33:09.929383 136306 sgd_solver.cpp:106] Iteration 3960, lr = 0.01
I0726 17:33:28.491920 136306 solver.cpp:236] Iteration 3970, loss = 87.3365
I0726 17:33:28.492182 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:33:28.492202 136306 sgd_solver.cpp:106] Iteration 3970, lr = 0.01
I0726 17:33:47.406260 136306 solver.cpp:236] Iteration 3980, loss = 87.3365
I0726 17:33:47.406332 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:33:47.406347 136306 sgd_solver.cpp:106] Iteration 3980, lr = 0.01
I0726 17:34:10.431443 136306 solver.cpp:236] Iteration 3990, loss = 87.3365
I0726 17:34:10.431669 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:34:10.431686 136306 sgd_solver.cpp:106] Iteration 3990, lr = 0.01
I0726 17:34:30.665848 136306 solver.cpp:461] Snapshotting to binary proto file models/gnet_iter_4000.caffemodel
I0726 17:34:30.791491 136306 sgd_solver.cpp:269] Snapshotting solver state to binary proto file models/gnet_iter_4000.solverstate
I0726 17:34:30.795337 136306 solver.cpp:340] Iteration 4000, Testing net (#0)
I0726 17:34:52.136241 136306 solver.cpp:408]     Test net output #0: accuracy = 0
I0726 17:34:52.136430 136306 solver.cpp:408]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:34:55.411056 136306 solver.cpp:236] Iteration 4000, loss = 87.3365
I0726 17:34:55.411126 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:34:55.411144 136306 sgd_solver.cpp:106] Iteration 4000, lr = 0.01
I0726 17:35:16.872745 136306 solver.cpp:236] Iteration 4010, loss = 87.3365
I0726 17:35:16.872807 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:35:16.872822 136306 sgd_solver.cpp:106] Iteration 4010, lr = 0.01
I0726 17:35:42.591316 136306 solver.cpp:236] Iteration 4020, loss = 87.3365
I0726 17:35:42.591528 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:35:42.591548 136306 sgd_solver.cpp:106] Iteration 4020, lr = 0.01
I0726 17:36:10.091646 136306 solver.cpp:236] Iteration 4030, loss = 87.3365
I0726 17:36:10.091728 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:36:10.091743 136306 sgd_solver.cpp:106] Iteration 4030, lr = 0.01
I0726 17:36:34.452672 136306 solver.cpp:236] Iteration 4040, loss = 87.3365
I0726 17:36:34.452852 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:36:34.452872 136306 sgd_solver.cpp:106] Iteration 4040, lr = 0.01
I0726 17:36:58.251646 136306 solver.cpp:236] Iteration 4050, loss = 87.3365
I0726 17:36:58.251709 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:36:58.251724 136306 sgd_solver.cpp:106] Iteration 4050, lr = 0.01
I0726 17:37:24.593128 136306 solver.cpp:236] Iteration 4060, loss = 87.3365
I0726 17:37:24.593499 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:37:24.593518 136306 sgd_solver.cpp:106] Iteration 4060, lr = 0.01
I0726 17:37:46.905297 136306 solver.cpp:236] Iteration 4070, loss = 87.3365
I0726 17:37:46.905401 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:37:46.905418 136306 sgd_solver.cpp:106] Iteration 4070, lr = 0.01
I0726 17:38:08.509899 136306 solver.cpp:236] Iteration 4080, loss = 87.3365
I0726 17:38:08.510159 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:38:08.510176 136306 sgd_solver.cpp:106] Iteration 4080, lr = 0.01
I0726 17:38:34.076225 136306 solver.cpp:236] Iteration 4090, loss = 87.3365
I0726 17:38:34.076313 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:38:34.076330 136306 sgd_solver.cpp:106] Iteration 4090, lr = 0.01
I0726 17:38:59.412021 136306 solver.cpp:236] Iteration 4100, loss = 87.3365
I0726 17:38:59.412178 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:38:59.412205 136306 sgd_solver.cpp:106] Iteration 4100, lr = 0.01
I0726 17:39:22.310820 136306 solver.cpp:236] Iteration 4110, loss = 87.3365
I0726 17:39:22.310883 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:39:22.310897 136306 sgd_solver.cpp:106] Iteration 4110, lr = 0.01
I0726 17:39:41.880580 136306 solver.cpp:236] Iteration 4120, loss = 87.3365
I0726 17:39:41.880785 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:39:41.880807 136306 sgd_solver.cpp:106] Iteration 4120, lr = 0.01
I0726 17:40:03.687901 136306 solver.cpp:236] Iteration 4130, loss = 87.3365
I0726 17:40:03.687980 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:40:03.687997 136306 sgd_solver.cpp:106] Iteration 4130, lr = 0.01
I0726 17:40:28.462736 136306 solver.cpp:236] Iteration 4140, loss = 87.3365
I0726 17:40:28.462971 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:40:28.462988 136306 sgd_solver.cpp:106] Iteration 4140, lr = 0.01
I0726 17:40:50.279654 136306 solver.cpp:236] Iteration 4150, loss = 87.3365
I0726 17:40:50.279724 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:40:50.279739 136306 sgd_solver.cpp:106] Iteration 4150, lr = 0.01
I0726 17:41:18.608530 136306 solver.cpp:236] Iteration 4160, loss = 87.3365
I0726 17:41:18.608743 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:41:18.608765 136306 sgd_solver.cpp:106] Iteration 4160, lr = 0.01
I0726 17:41:45.009373 136306 solver.cpp:236] Iteration 4170, loss = 87.3365
I0726 17:41:45.009433 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:41:45.009448 136306 sgd_solver.cpp:106] Iteration 4170, lr = 0.01
I0726 17:42:05.918089 136306 solver.cpp:236] Iteration 4180, loss = 87.3365
I0726 17:42:05.918326 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:42:05.918360 136306 sgd_solver.cpp:106] Iteration 4180, lr = 0.01
I0726 17:42:32.626845 136306 solver.cpp:236] Iteration 4190, loss = 87.3365
I0726 17:42:32.626935 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:42:32.626960 136306 sgd_solver.cpp:106] Iteration 4190, lr = 0.01
I0726 17:42:52.082386 136306 solver.cpp:236] Iteration 4200, loss = 87.3365
I0726 17:42:52.082582 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:42:52.082624 136306 sgd_solver.cpp:106] Iteration 4200, lr = 0.01
I0726 17:43:11.825899 136306 solver.cpp:236] Iteration 4210, loss = 87.3365
I0726 17:43:11.825959 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:43:11.825976 136306 sgd_solver.cpp:106] Iteration 4210, lr = 0.01
I0726 17:43:34.191222 136306 solver.cpp:236] Iteration 4220, loss = 87.3365
I0726 17:43:34.191493 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:43:34.191530 136306 sgd_solver.cpp:106] Iteration 4220, lr = 0.01
I0726 17:44:07.299631 136306 solver.cpp:236] Iteration 4230, loss = 87.3365
I0726 17:44:07.299803 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:44:07.299837 136306 sgd_solver.cpp:106] Iteration 4230, lr = 0.01
I0726 17:44:26.547777 136306 solver.cpp:236] Iteration 4240, loss = 87.3365
I0726 17:44:26.547838 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:44:26.547853 136306 sgd_solver.cpp:106] Iteration 4240, lr = 0.01
I0726 17:44:44.678081 136306 solver.cpp:236] Iteration 4250, loss = 87.3365
I0726 17:44:44.678289 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:44:44.678308 136306 sgd_solver.cpp:106] Iteration 4250, lr = 0.01
I0726 17:45:06.385915 136306 solver.cpp:236] Iteration 4260, loss = 87.3365
I0726 17:45:06.385978 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:45:06.385993 136306 sgd_solver.cpp:106] Iteration 4260, lr = 0.01
I0726 17:45:29.008532 136306 solver.cpp:236] Iteration 4270, loss = 87.3365
I0726 17:45:29.008775 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:45:29.008790 136306 sgd_solver.cpp:106] Iteration 4270, lr = 0.01
I0726 17:45:53.321866 136306 solver.cpp:236] Iteration 4280, loss = 87.3365
I0726 17:45:53.321933 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:45:53.321948 136306 sgd_solver.cpp:106] Iteration 4280, lr = 0.01
I0726 17:46:14.817045 136306 solver.cpp:236] Iteration 4290, loss = 87.3365
I0726 17:46:14.817298 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:46:14.817318 136306 sgd_solver.cpp:106] Iteration 4290, lr = 0.01
I0726 17:46:33.492120 136306 solver.cpp:236] Iteration 4300, loss = 87.3365
I0726 17:46:33.492177 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:46:33.492190 136306 sgd_solver.cpp:106] Iteration 4300, lr = 0.01
I0726 17:46:59.482050 136306 solver.cpp:236] Iteration 4310, loss = 87.3365
I0726 17:46:59.482292 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:46:59.482311 136306 sgd_solver.cpp:106] Iteration 4310, lr = 0.01
I0726 17:47:20.540462 136306 solver.cpp:236] Iteration 4320, loss = 87.3365
I0726 17:47:20.540520 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:47:20.540535 136306 sgd_solver.cpp:106] Iteration 4320, lr = 0.01
I0726 17:47:40.019740 136306 solver.cpp:236] Iteration 4330, loss = 87.3365
I0726 17:47:40.019917 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:47:40.019959 136306 sgd_solver.cpp:106] Iteration 4330, lr = 0.01
I0726 17:48:00.594913 136306 solver.cpp:236] Iteration 4340, loss = 87.3365
I0726 17:48:00.594972 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:48:00.594986 136306 sgd_solver.cpp:106] Iteration 4340, lr = 0.01
I0726 17:48:19.617074 136306 solver.cpp:236] Iteration 4350, loss = 87.3365
I0726 17:48:19.617203 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:48:19.617224 136306 sgd_solver.cpp:106] Iteration 4350, lr = 0.01
I0726 17:48:36.702805 136306 solver.cpp:236] Iteration 4360, loss = 87.3365
I0726 17:48:36.702860 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:48:36.702874 136306 sgd_solver.cpp:106] Iteration 4360, lr = 0.01
I0726 17:48:59.872354 136306 solver.cpp:236] Iteration 4370, loss = 87.3365
I0726 17:48:59.872544 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:48:59.872584 136306 sgd_solver.cpp:106] Iteration 4370, lr = 0.01
I0726 17:49:20.034435 136306 solver.cpp:236] Iteration 4380, loss = 87.3365
I0726 17:49:20.034498 136306 solver.cpp:252]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0726 17:49:20.034512 136306 sgd_solver.cpp:106] Iteration 4380, lr = 0.01
I0726 17:49:42.768492 136306 solver.cpp:236] Iteration 4390, loss = 87.3365
