Log file created at: 2016/07/28 10:31:47
Running on machine: deepath-01
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0728 10:31:47.430017 65336 caffe.cpp:184] Using GPUs 3
I0728 10:31:47.985151 65336 solver.cpp:47] Initializing solver from parameters: 
test_iter: 10
test_interval: 500
base_lr: 0.001
display: 10
max_iter: 60000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 20000
snapshot: 1000
snapshot_prefix: "models/gnet"
solver_mode: GPU
device_id: 3
net: "train_val.prototxt"
test_initialization: true
I0728 10:31:47.985433 65336 solver.cpp:90] Creating training net from net file: train_val.prototxt
I0728 10:31:47.986062 65336 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0728 10:31:47.986102 65336 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0728 10:31:47.986249 65336 net.cpp:49] Initializing net from parameters: 
name: "Net"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  python_param {
    module: "mask_image_layer"
    layer: "random_patches_from_images_withlabel"
    param_str: "{\'root_folder\': \'/home/dywang/Proliferation/data/mitoses\', \'image_list\': \'/home/dywang/00exp_wdy/stage04_mitosisDetection/step02_train_mitosis_detecotor/heatmap_mc13_tr/all_image_withlabel_mc13_tr.lst\', \'seed\': 8899, \'mean\': (128, 128, 128), \'size\': 64, \'batch\': 128, \'scale\':0.1, \'colorn\':20, \'classes\':\'1:0 2:1 3:0\', \'DEBUG\': True}"
  }
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "data"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv12"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv21"
  type: "Convolution"
  bottom: "pool1"
  top: "conv21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu21"
  type: "ReLU"
  bottom: "conv21"
  top: "conv21"
}
layer {
  name: "conv22"
  type: "Convolution"
  bottom: "conv21"
  top: "conv22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu22"
  type: "ReLU"
  bottom: "conv22"
  top: "conv22"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv22"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv31"
  type: "Convolution"
  bottom: "pool2"
  top: "conv31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu31"
  type: "ReLU"
  bottom: "conv31"
  top: "conv31"
}
layer {
  name: "conv32"
  type: "Convolution"
  bottom: "conv31"
  top: "conv32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu32"
  type: "ReLU"
  bottom: "conv32"
  top: "conv32"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv32"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "drop"
  type: "Dropout"
  bottom: "conv4"
  top: "conv4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv_c"
  type: "Convolution"
  bottom: "conv4"
  top: "conv_c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 1
    kernel_w: 1
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv_c"
  bottom: "label"
  top: "loss"
}
I0728 10:31:47.987407 65336 layer_factory.hpp:76] Creating layer data
I0728 10:31:50.633921 65336 net.cpp:106] Creating Layer data
I0728 10:31:50.633994 65336 net.cpp:411] data -> data
I0728 10:31:50.634027 65336 net.cpp:411] data -> label
I0728 10:32:01.578439 65336 net.cpp:150] Setting up data
I0728 10:32:01.578537 65336 net.cpp:157] Top shape: 128 3 64 64 (1572864)
I0728 10:32:01.578559 65336 net.cpp:157] Top shape: 128 1 1 1 (128)
I0728 10:32:01.578573 65336 net.cpp:165] Memory required for data: 6291968
I0728 10:32:01.578603 65336 layer_factory.hpp:76] Creating layer conv11
I0728 10:32:01.578662 65336 net.cpp:106] Creating Layer conv11
I0728 10:32:01.578677 65336 net.cpp:454] conv11 <- data
I0728 10:32:01.578706 65336 net.cpp:411] conv11 -> conv11
I0728 10:32:01.863445 65336 net.cpp:150] Setting up conv11
I0728 10:32:01.863503 65336 net.cpp:157] Top shape: 128 32 62 62 (15745024)
I0728 10:32:01.863514 65336 net.cpp:165] Memory required for data: 69272064
I0728 10:32:01.863557 65336 layer_factory.hpp:76] Creating layer relu11
I0728 10:32:01.863582 65336 net.cpp:106] Creating Layer relu11
I0728 10:32:01.863593 65336 net.cpp:454] relu11 <- conv11
I0728 10:32:01.863605 65336 net.cpp:397] relu11 -> conv11 (in-place)
I0728 10:32:01.864009 65336 net.cpp:150] Setting up relu11
I0728 10:32:01.864028 65336 net.cpp:157] Top shape: 128 32 62 62 (15745024)
I0728 10:32:01.864037 65336 net.cpp:165] Memory required for data: 132252160
I0728 10:32:01.864048 65336 layer_factory.hpp:76] Creating layer conv12
I0728 10:32:01.864064 65336 net.cpp:106] Creating Layer conv12
I0728 10:32:01.864073 65336 net.cpp:454] conv12 <- conv11
I0728 10:32:01.864086 65336 net.cpp:411] conv12 -> conv12
I0728 10:32:01.871001 65336 net.cpp:150] Setting up conv12
I0728 10:32:01.871042 65336 net.cpp:157] Top shape: 128 32 60 60 (14745600)
I0728 10:32:01.871052 65336 net.cpp:165] Memory required for data: 191234560
I0728 10:32:01.871073 65336 layer_factory.hpp:76] Creating layer relu12
I0728 10:32:01.871090 65336 net.cpp:106] Creating Layer relu12
I0728 10:32:01.871100 65336 net.cpp:454] relu12 <- conv12
I0728 10:32:01.871111 65336 net.cpp:397] relu12 -> conv12 (in-place)
I0728 10:32:01.873191 65336 net.cpp:150] Setting up relu12
I0728 10:32:01.873216 65336 net.cpp:157] Top shape: 128 32 60 60 (14745600)
I0728 10:32:01.873229 65336 net.cpp:165] Memory required for data: 250216960
I0728 10:32:01.873239 65336 layer_factory.hpp:76] Creating layer pool1
I0728 10:32:01.873267 65336 net.cpp:106] Creating Layer pool1
I0728 10:32:01.873278 65336 net.cpp:454] pool1 <- conv12
I0728 10:32:01.873291 65336 net.cpp:411] pool1 -> pool1
I0728 10:32:01.875577 65336 net.cpp:150] Setting up pool1
I0728 10:32:01.875613 65336 net.cpp:157] Top shape: 128 32 30 30 (3686400)
I0728 10:32:01.875623 65336 net.cpp:165] Memory required for data: 264962560
I0728 10:32:01.875633 65336 layer_factory.hpp:76] Creating layer conv21
I0728 10:32:01.875653 65336 net.cpp:106] Creating Layer conv21
I0728 10:32:01.875663 65336 net.cpp:454] conv21 <- pool1
I0728 10:32:01.875674 65336 net.cpp:411] conv21 -> conv21
I0728 10:32:01.881250 65336 net.cpp:150] Setting up conv21
I0728 10:32:01.881306 65336 net.cpp:157] Top shape: 128 64 28 28 (6422528)
I0728 10:32:01.881317 65336 net.cpp:165] Memory required for data: 290652672
I0728 10:32:01.881338 65336 layer_factory.hpp:76] Creating layer relu21
I0728 10:32:01.881371 65336 net.cpp:106] Creating Layer relu21
I0728 10:32:01.881383 65336 net.cpp:454] relu21 <- conv21
I0728 10:32:01.881395 65336 net.cpp:397] relu21 -> conv21 (in-place)
I0728 10:32:01.882534 65336 net.cpp:150] Setting up relu21
I0728 10:32:01.882561 65336 net.cpp:157] Top shape: 128 64 28 28 (6422528)
I0728 10:32:01.882571 65336 net.cpp:165] Memory required for data: 316342784
I0728 10:32:01.882581 65336 layer_factory.hpp:76] Creating layer conv22
I0728 10:32:01.882616 65336 net.cpp:106] Creating Layer conv22
I0728 10:32:01.882634 65336 net.cpp:454] conv22 <- conv21
I0728 10:32:01.882650 65336 net.cpp:411] conv22 -> conv22
I0728 10:32:01.889611 65336 net.cpp:150] Setting up conv22
I0728 10:32:01.889665 65336 net.cpp:157] Top shape: 128 64 26 26 (5537792)
I0728 10:32:01.889677 65336 net.cpp:165] Memory required for data: 338493952
I0728 10:32:01.889693 65336 layer_factory.hpp:76] Creating layer relu22
I0728 10:32:01.889713 65336 net.cpp:106] Creating Layer relu22
I0728 10:32:01.889724 65336 net.cpp:454] relu22 <- conv22
I0728 10:32:01.889736 65336 net.cpp:397] relu22 -> conv22 (in-place)
I0728 10:32:01.891264 65336 net.cpp:150] Setting up relu22
I0728 10:32:01.891299 65336 net.cpp:157] Top shape: 128 64 26 26 (5537792)
I0728 10:32:01.891309 65336 net.cpp:165] Memory required for data: 360645120
I0728 10:32:01.891320 65336 layer_factory.hpp:76] Creating layer pool2
I0728 10:32:01.891338 65336 net.cpp:106] Creating Layer pool2
I0728 10:32:01.891347 65336 net.cpp:454] pool2 <- conv22
I0728 10:32:01.891360 65336 net.cpp:411] pool2 -> pool2
I0728 10:32:01.892781 65336 net.cpp:150] Setting up pool2
I0728 10:32:01.892810 65336 net.cpp:157] Top shape: 128 64 13 13 (1384448)
I0728 10:32:01.892819 65336 net.cpp:165] Memory required for data: 366182912
I0728 10:32:01.892829 65336 layer_factory.hpp:76] Creating layer conv31
I0728 10:32:01.892869 65336 net.cpp:106] Creating Layer conv31
I0728 10:32:01.892881 65336 net.cpp:454] conv31 <- pool2
I0728 10:32:01.892896 65336 net.cpp:411] conv31 -> conv31
I0728 10:32:01.900046 65336 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 6912
I0728 10:32:01.900446 65336 net.cpp:150] Setting up conv31
I0728 10:32:01.900481 65336 net.cpp:157] Top shape: 128 128 11 11 (1982464)
I0728 10:32:01.900499 65336 net.cpp:165] Memory required for data: 374112768
I0728 10:32:01.900532 65336 layer_factory.hpp:76] Creating layer relu31
I0728 10:32:01.900559 65336 net.cpp:106] Creating Layer relu31
I0728 10:32:01.900578 65336 net.cpp:454] relu31 <- conv31
I0728 10:32:01.900602 65336 net.cpp:397] relu31 -> conv31 (in-place)
I0728 10:32:01.902258 65336 net.cpp:150] Setting up relu31
I0728 10:32:01.902288 65336 net.cpp:157] Top shape: 128 128 11 11 (1982464)
I0728 10:32:01.902298 65336 net.cpp:165] Memory required for data: 382042624
I0728 10:32:01.902307 65336 layer_factory.hpp:76] Creating layer conv32
I0728 10:32:01.902326 65336 net.cpp:106] Creating Layer conv32
I0728 10:32:01.902336 65336 net.cpp:454] conv32 <- conv31
I0728 10:32:01.902349 65336 net.cpp:411] conv32 -> conv32
I0728 10:32:01.908726 65336 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 13824
I0728 10:32:01.908787 65336 net.cpp:150] Setting up conv32
I0728 10:32:01.908799 65336 net.cpp:157] Top shape: 128 128 9 9 (1327104)
I0728 10:32:01.908805 65336 net.cpp:165] Memory required for data: 387351040
I0728 10:32:01.908849 65336 layer_factory.hpp:76] Creating layer relu32
I0728 10:32:01.908875 65336 net.cpp:106] Creating Layer relu32
I0728 10:32:01.908885 65336 net.cpp:454] relu32 <- conv32
I0728 10:32:01.908893 65336 net.cpp:397] relu32 -> conv32 (in-place)
I0728 10:32:01.909832 65336 net.cpp:150] Setting up relu32
I0728 10:32:01.909849 65336 net.cpp:157] Top shape: 128 128 9 9 (1327104)
I0728 10:32:01.909855 65336 net.cpp:165] Memory required for data: 392659456
I0728 10:32:01.909862 65336 layer_factory.hpp:76] Creating layer pool3
I0728 10:32:01.909883 65336 net.cpp:106] Creating Layer pool3
I0728 10:32:01.909888 65336 net.cpp:454] pool3 <- conv32
I0728 10:32:01.909895 65336 net.cpp:411] pool3 -> pool3
I0728 10:32:01.912410 65336 net.cpp:150] Setting up pool3
I0728 10:32:01.912446 65336 net.cpp:157] Top shape: 128 128 3 3 (147456)
I0728 10:32:01.912451 65336 net.cpp:165] Memory required for data: 393249280
I0728 10:32:01.912459 65336 layer_factory.hpp:76] Creating layer conv4
I0728 10:32:01.912490 65336 net.cpp:106] Creating Layer conv4
I0728 10:32:01.912497 65336 net.cpp:454] conv4 <- pool3
I0728 10:32:01.912508 65336 net.cpp:411] conv4 -> conv4
I0728 10:32:01.921607 65336 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 1769472
I0728 10:32:01.921973 65336 net.cpp:150] Setting up conv4
I0728 10:32:01.922000 65336 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0728 10:32:01.922011 65336 net.cpp:165] Memory required for data: 393380352
I0728 10:32:01.922032 65336 layer_factory.hpp:76] Creating layer relu4
I0728 10:32:01.922050 65336 net.cpp:106] Creating Layer relu4
I0728 10:32:01.922061 65336 net.cpp:454] relu4 <- conv4
I0728 10:32:01.922080 65336 net.cpp:397] relu4 -> conv4 (in-place)
I0728 10:32:01.922574 65336 net.cpp:150] Setting up relu4
I0728 10:32:01.922596 65336 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0728 10:32:01.922608 65336 net.cpp:165] Memory required for data: 393511424
I0728 10:32:01.922617 65336 layer_factory.hpp:76] Creating layer drop
I0728 10:32:01.922662 65336 net.cpp:106] Creating Layer drop
I0728 10:32:01.922673 65336 net.cpp:454] drop <- conv4
I0728 10:32:01.922693 65336 net.cpp:397] drop -> conv4 (in-place)
I0728 10:32:01.922737 65336 net.cpp:150] Setting up drop
I0728 10:32:01.922760 65336 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0728 10:32:01.922775 65336 net.cpp:165] Memory required for data: 393642496
I0728 10:32:01.922817 65336 layer_factory.hpp:76] Creating layer conv_c
I0728 10:32:01.922848 65336 net.cpp:106] Creating Layer conv_c
I0728 10:32:01.922855 65336 net.cpp:454] conv_c <- conv4
I0728 10:32:01.922871 65336 net.cpp:411] conv_c -> conv_c
I0728 10:32:01.930378 65336 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 3072
I0728 10:32:01.930586 65336 net.cpp:150] Setting up conv_c
I0728 10:32:01.930603 65336 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0728 10:32:01.930608 65336 net.cpp:165] Memory required for data: 393773568
I0728 10:32:01.930619 65336 layer_factory.hpp:76] Creating layer loss
I0728 10:32:01.930644 65336 net.cpp:106] Creating Layer loss
I0728 10:32:01.930650 65336 net.cpp:454] loss <- conv_c
I0728 10:32:01.930658 65336 net.cpp:454] loss <- label
I0728 10:32:01.930671 65336 net.cpp:411] loss -> loss
I0728 10:32:01.930690 65336 layer_factory.hpp:76] Creating layer loss
I0728 10:32:01.933670 65336 net.cpp:150] Setting up loss
I0728 10:32:01.933681 65336 net.cpp:157] Top shape: (1)
I0728 10:32:01.933686 65336 net.cpp:160]     with loss weight 1
I0728 10:32:01.933717 65336 net.cpp:165] Memory required for data: 393773572
I0728 10:32:01.933722 65336 net.cpp:226] loss needs backward computation.
I0728 10:32:01.933727 65336 net.cpp:226] conv_c needs backward computation.
I0728 10:32:01.933732 65336 net.cpp:226] drop needs backward computation.
I0728 10:32:01.933737 65336 net.cpp:226] relu4 needs backward computation.
I0728 10:32:01.933742 65336 net.cpp:226] conv4 needs backward computation.
I0728 10:32:01.933748 65336 net.cpp:226] pool3 needs backward computation.
I0728 10:32:01.933753 65336 net.cpp:226] relu32 needs backward computation.
I0728 10:32:01.933794 65336 net.cpp:226] conv32 needs backward computation.
I0728 10:32:01.933799 65336 net.cpp:226] relu31 needs backward computation.
I0728 10:32:01.933804 65336 net.cpp:226] conv31 needs backward computation.
I0728 10:32:01.933809 65336 net.cpp:226] pool2 needs backward computation.
I0728 10:32:01.933815 65336 net.cpp:226] relu22 needs backward computation.
I0728 10:32:01.933820 65336 net.cpp:226] conv22 needs backward computation.
I0728 10:32:01.933825 65336 net.cpp:226] relu21 needs backward computation.
I0728 10:32:01.933830 65336 net.cpp:226] conv21 needs backward computation.
I0728 10:32:01.933835 65336 net.cpp:226] pool1 needs backward computation.
I0728 10:32:01.933840 65336 net.cpp:226] relu12 needs backward computation.
I0728 10:32:01.933845 65336 net.cpp:226] conv12 needs backward computation.
I0728 10:32:01.933850 65336 net.cpp:226] relu11 needs backward computation.
I0728 10:32:01.933854 65336 net.cpp:226] conv11 needs backward computation.
I0728 10:32:01.933859 65336 net.cpp:228] data does not need backward computation.
I0728 10:32:01.933863 65336 net.cpp:270] This network produces output loss
I0728 10:32:01.933881 65336 net.cpp:283] Network initialization done.
I0728 10:32:01.934494 65336 solver.cpp:180] Creating test net (#0) specified by net file: train_val.prototxt
I0728 10:32:01.934535 65336 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0728 10:32:01.934782 65336 net.cpp:49] Initializing net from parameters: 
name: "Net"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  python_param {
    module: "mask_image_layer"
    layer: "random_patches_from_images_withlabel"
    param_str: "{\'root_folder\': \'/home/dywang/Proliferation/data/mitoses\', \'image_list\': \'/home/dywang/00exp_wdy/stage04_mitosisDetection/step02_train_mitosis_detecotor/heatmap_mc13_tr/all_image_withlabel_mc13_tr.lst\', \'seed\': 8899, \'mean\': (128, 128, 128), \'size\': 64, \'batch\': 128, \'scale\':0.1, \'colorn\':20, \'classes\':\'1:0 2:1 3:0\', \'DEBUG\': True}"
  }
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "data"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv12"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv21"
  type: "Convolution"
  bottom: "pool1"
  top: "conv21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu21"
  type: "ReLU"
  bottom: "conv21"
  top: "conv21"
}
layer {
  name: "conv22"
  type: "Convolution"
  bottom: "conv21"
  top: "conv22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu22"
  type: "ReLU"
  bottom: "conv22"
  top: "conv22"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv22"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv31"
  type: "Convolution"
  bottom: "pool2"
  top: "conv31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu31"
  type: "ReLU"
  bottom: "conv31"
  top: "conv31"
}
layer {
  name: "conv32"
  type: "Convolution"
  bottom: "conv31"
  top: "conv32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu32"
  type: "ReLU"
  bottom: "conv32"
  top: "conv32"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv32"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 3
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "drop"
  type: "Dropout"
  bottom: "conv4"
  top: "conv4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv_c"
  type: "Convolution"
  bottom: "conv4"
  top: "conv_c"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 1
    kernel_w: 1
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "conv_c"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv_c"
  bottom: "label"
  top: "loss"
}
I0728 10:32:01.935108 65336 layer_factory.hpp:76] Creating layer data
I0728 10:32:01.935223 65336 net.cpp:106] Creating Layer data
I0728 10:32:01.935235 65336 net.cpp:411] data -> data
I0728 10:32:01.935248 65336 net.cpp:411] data -> label
I0728 10:32:09.103513 65336 net.cpp:150] Setting up data
I0728 10:32:09.103576 65336 net.cpp:157] Top shape: 128 3 64 64 (1572864)
I0728 10:32:09.103590 65336 net.cpp:157] Top shape: 128 1 1 1 (128)
I0728 10:32:09.103600 65336 net.cpp:165] Memory required for data: 6291968
I0728 10:32:09.103612 65336 layer_factory.hpp:76] Creating layer label_data_1_split
I0728 10:32:09.103647 65336 net.cpp:106] Creating Layer label_data_1_split
I0728 10:32:09.103657 65336 net.cpp:454] label_data_1_split <- label
I0728 10:32:09.103672 65336 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0728 10:32:09.103688 65336 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0728 10:32:09.103740 65336 net.cpp:150] Setting up label_data_1_split
I0728 10:32:09.103754 65336 net.cpp:157] Top shape: 128 1 1 1 (128)
I0728 10:32:09.103763 65336 net.cpp:157] Top shape: 128 1 1 1 (128)
I0728 10:32:09.103771 65336 net.cpp:165] Memory required for data: 6292992
I0728 10:32:09.103780 65336 layer_factory.hpp:76] Creating layer conv11
I0728 10:32:09.103803 65336 net.cpp:106] Creating Layer conv11
I0728 10:32:09.103812 65336 net.cpp:454] conv11 <- data
I0728 10:32:09.103824 65336 net.cpp:411] conv11 -> conv11
I0728 10:32:09.109561 65336 net.cpp:150] Setting up conv11
I0728 10:32:09.109586 65336 net.cpp:157] Top shape: 128 32 62 62 (15745024)
I0728 10:32:09.109596 65336 net.cpp:165] Memory required for data: 69273088
I0728 10:32:09.109612 65336 layer_factory.hpp:76] Creating layer relu11
I0728 10:32:09.109663 65336 net.cpp:106] Creating Layer relu11
I0728 10:32:09.109673 65336 net.cpp:454] relu11 <- conv11
I0728 10:32:09.109683 65336 net.cpp:397] relu11 -> conv11 (in-place)
I0728 10:32:09.112480 65336 net.cpp:150] Setting up relu11
I0728 10:32:09.112504 65336 net.cpp:157] Top shape: 128 32 62 62 (15745024)
I0728 10:32:09.112512 65336 net.cpp:165] Memory required for data: 132253184
I0728 10:32:09.112520 65336 layer_factory.hpp:76] Creating layer conv12
I0728 10:32:09.112540 65336 net.cpp:106] Creating Layer conv12
I0728 10:32:09.112548 65336 net.cpp:454] conv12 <- conv11
I0728 10:32:09.112561 65336 net.cpp:411] conv12 -> conv12
I0728 10:32:09.119390 65336 net.cpp:150] Setting up conv12
I0728 10:32:09.119413 65336 net.cpp:157] Top shape: 128 32 60 60 (14745600)
I0728 10:32:09.119422 65336 net.cpp:165] Memory required for data: 191235584
I0728 10:32:09.119436 65336 layer_factory.hpp:76] Creating layer relu12
I0728 10:32:09.119449 65336 net.cpp:106] Creating Layer relu12
I0728 10:32:09.119458 65336 net.cpp:454] relu12 <- conv12
I0728 10:32:09.119469 65336 net.cpp:397] relu12 -> conv12 (in-place)
I0728 10:32:09.121533 65336 net.cpp:150] Setting up relu12
I0728 10:32:09.121554 65336 net.cpp:157] Top shape: 128 32 60 60 (14745600)
I0728 10:32:09.121563 65336 net.cpp:165] Memory required for data: 250217984
I0728 10:32:09.121572 65336 layer_factory.hpp:76] Creating layer pool1
I0728 10:32:09.121587 65336 net.cpp:106] Creating Layer pool1
I0728 10:32:09.121595 65336 net.cpp:454] pool1 <- conv12
I0728 10:32:09.121605 65336 net.cpp:411] pool1 -> pool1
I0728 10:32:09.130204 65336 net.cpp:150] Setting up pool1
I0728 10:32:09.130223 65336 net.cpp:157] Top shape: 128 32 30 30 (3686400)
I0728 10:32:09.130231 65336 net.cpp:165] Memory required for data: 264963584
I0728 10:32:09.130241 65336 layer_factory.hpp:76] Creating layer conv21
I0728 10:32:09.130257 65336 net.cpp:106] Creating Layer conv21
I0728 10:32:09.130266 65336 net.cpp:454] conv21 <- pool1
I0728 10:32:09.130280 65336 net.cpp:411] conv21 -> conv21
I0728 10:32:09.136643 65336 net.cpp:150] Setting up conv21
I0728 10:32:09.136667 65336 net.cpp:157] Top shape: 128 64 28 28 (6422528)
I0728 10:32:09.136675 65336 net.cpp:165] Memory required for data: 290653696
I0728 10:32:09.136690 65336 layer_factory.hpp:76] Creating layer relu21
I0728 10:32:09.136704 65336 net.cpp:106] Creating Layer relu21
I0728 10:32:09.136713 65336 net.cpp:454] relu21 <- conv21
I0728 10:32:09.136725 65336 net.cpp:397] relu21 -> conv21 (in-place)
I0728 10:32:09.137048 65336 net.cpp:150] Setting up relu21
I0728 10:32:09.137068 65336 net.cpp:157] Top shape: 128 64 28 28 (6422528)
I0728 10:32:09.137078 65336 net.cpp:165] Memory required for data: 316343808
I0728 10:32:09.137086 65336 layer_factory.hpp:76] Creating layer conv22
I0728 10:32:09.137104 65336 net.cpp:106] Creating Layer conv22
I0728 10:32:09.137112 65336 net.cpp:454] conv22 <- conv21
I0728 10:32:09.137126 65336 net.cpp:411] conv22 -> conv22
I0728 10:32:09.138460 65336 net.cpp:150] Setting up conv22
I0728 10:32:09.138505 65336 net.cpp:157] Top shape: 128 64 26 26 (5537792)
I0728 10:32:09.138515 65336 net.cpp:165] Memory required for data: 338494976
I0728 10:32:09.138530 65336 layer_factory.hpp:76] Creating layer relu22
I0728 10:32:09.138545 65336 net.cpp:106] Creating Layer relu22
I0728 10:32:09.138555 65336 net.cpp:454] relu22 <- conv22
I0728 10:32:09.138567 65336 net.cpp:397] relu22 -> conv22 (in-place)
I0728 10:32:09.138756 65336 net.cpp:150] Setting up relu22
I0728 10:32:09.138774 65336 net.cpp:157] Top shape: 128 64 26 26 (5537792)
I0728 10:32:09.138783 65336 net.cpp:165] Memory required for data: 360646144
I0728 10:32:09.138792 65336 layer_factory.hpp:76] Creating layer pool2
I0728 10:32:09.138805 65336 net.cpp:106] Creating Layer pool2
I0728 10:32:09.138814 65336 net.cpp:454] pool2 <- conv22
I0728 10:32:09.138824 65336 net.cpp:411] pool2 -> pool2
I0728 10:32:09.139189 65336 net.cpp:150] Setting up pool2
I0728 10:32:09.139211 65336 net.cpp:157] Top shape: 128 64 13 13 (1384448)
I0728 10:32:09.139220 65336 net.cpp:165] Memory required for data: 366183936
I0728 10:32:09.139264 65336 layer_factory.hpp:76] Creating layer conv31
I0728 10:32:09.139284 65336 net.cpp:106] Creating Layer conv31
I0728 10:32:09.139295 65336 net.cpp:454] conv31 <- pool2
I0728 10:32:09.139308 65336 net.cpp:411] conv31 -> conv31
I0728 10:32:09.141449 65336 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 6912
I0728 10:32:09.141585 65336 net.cpp:150] Setting up conv31
I0728 10:32:09.141604 65336 net.cpp:157] Top shape: 128 128 11 11 (1982464)
I0728 10:32:09.141614 65336 net.cpp:165] Memory required for data: 374113792
I0728 10:32:09.141638 65336 layer_factory.hpp:76] Creating layer relu31
I0728 10:32:09.141664 65336 net.cpp:106] Creating Layer relu31
I0728 10:32:09.141679 65336 net.cpp:454] relu31 <- conv31
I0728 10:32:09.141691 65336 net.cpp:397] relu31 -> conv31 (in-place)
I0728 10:32:09.142026 65336 net.cpp:150] Setting up relu31
I0728 10:32:09.142048 65336 net.cpp:157] Top shape: 128 128 11 11 (1982464)
I0728 10:32:09.142057 65336 net.cpp:165] Memory required for data: 382043648
I0728 10:32:09.142066 65336 layer_factory.hpp:76] Creating layer conv32
I0728 10:32:09.142086 65336 net.cpp:106] Creating Layer conv32
I0728 10:32:09.142096 65336 net.cpp:454] conv32 <- conv31
I0728 10:32:09.142109 65336 net.cpp:411] conv32 -> conv32
I0728 10:32:09.144412 65336 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 13824
I0728 10:32:09.144484 65336 net.cpp:150] Setting up conv32
I0728 10:32:09.144502 65336 net.cpp:157] Top shape: 128 128 9 9 (1327104)
I0728 10:32:09.144512 65336 net.cpp:165] Memory required for data: 387352064
I0728 10:32:09.144527 65336 layer_factory.hpp:76] Creating layer relu32
I0728 10:32:09.144542 65336 net.cpp:106] Creating Layer relu32
I0728 10:32:09.144553 65336 net.cpp:454] relu32 <- conv32
I0728 10:32:09.144564 65336 net.cpp:397] relu32 -> conv32 (in-place)
I0728 10:32:09.144742 65336 net.cpp:150] Setting up relu32
I0728 10:32:09.144759 65336 net.cpp:157] Top shape: 128 128 9 9 (1327104)
I0728 10:32:09.144768 65336 net.cpp:165] Memory required for data: 392660480
I0728 10:32:09.144781 65336 layer_factory.hpp:76] Creating layer pool3
I0728 10:32:09.144798 65336 net.cpp:106] Creating Layer pool3
I0728 10:32:09.144809 65336 net.cpp:454] pool3 <- conv32
I0728 10:32:09.144819 65336 net.cpp:411] pool3 -> pool3
I0728 10:32:09.145225 65336 net.cpp:150] Setting up pool3
I0728 10:32:09.145246 65336 net.cpp:157] Top shape: 128 128 3 3 (147456)
I0728 10:32:09.145256 65336 net.cpp:165] Memory required for data: 393250304
I0728 10:32:09.145264 65336 layer_factory.hpp:76] Creating layer conv4
I0728 10:32:09.145280 65336 net.cpp:106] Creating Layer conv4
I0728 10:32:09.145290 65336 net.cpp:454] conv4 <- pool3
I0728 10:32:09.145303 65336 net.cpp:411] conv4 -> conv4
I0728 10:32:09.151160 65336 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 1769472
I0728 10:32:09.151374 65336 net.cpp:150] Setting up conv4
I0728 10:32:09.151396 65336 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0728 10:32:09.151404 65336 net.cpp:165] Memory required for data: 393381376
I0728 10:32:09.151417 65336 layer_factory.hpp:76] Creating layer relu4
I0728 10:32:09.151432 65336 net.cpp:106] Creating Layer relu4
I0728 10:32:09.151442 65336 net.cpp:454] relu4 <- conv4
I0728 10:32:09.151453 65336 net.cpp:397] relu4 -> conv4 (in-place)
I0728 10:32:09.153370 65336 net.cpp:150] Setting up relu4
I0728 10:32:09.153389 65336 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0728 10:32:09.153398 65336 net.cpp:165] Memory required for data: 393512448
I0728 10:32:09.153408 65336 layer_factory.hpp:76] Creating layer drop
I0728 10:32:09.153421 65336 net.cpp:106] Creating Layer drop
I0728 10:32:09.153430 65336 net.cpp:454] drop <- conv4
I0728 10:32:09.153442 65336 net.cpp:397] drop -> conv4 (in-place)
I0728 10:32:09.153477 65336 net.cpp:150] Setting up drop
I0728 10:32:09.153492 65336 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0728 10:32:09.153504 65336 net.cpp:165] Memory required for data: 393643520
I0728 10:32:09.153513 65336 layer_factory.hpp:76] Creating layer conv_c
I0728 10:32:09.153563 65336 net.cpp:106] Creating Layer conv_c
I0728 10:32:09.153575 65336 net.cpp:454] conv_c <- conv4
I0728 10:32:09.153589 65336 net.cpp:411] conv_c -> conv_c
I0728 10:32:09.160382 65336 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 3072
I0728 10:32:09.160419 65336 net.cpp:150] Setting up conv_c
I0728 10:32:09.160434 65336 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0728 10:32:09.160442 65336 net.cpp:165] Memory required for data: 393774592
I0728 10:32:09.160454 65336 layer_factory.hpp:76] Creating layer conv_c_conv_c_0_split
I0728 10:32:09.160470 65336 net.cpp:106] Creating Layer conv_c_conv_c_0_split
I0728 10:32:09.160482 65336 net.cpp:454] conv_c_conv_c_0_split <- conv_c
I0728 10:32:09.160493 65336 net.cpp:411] conv_c_conv_c_0_split -> conv_c_conv_c_0_split_0
I0728 10:32:09.160507 65336 net.cpp:411] conv_c_conv_c_0_split -> conv_c_conv_c_0_split_1
I0728 10:32:09.160560 65336 net.cpp:150] Setting up conv_c_conv_c_0_split
I0728 10:32:09.160575 65336 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0728 10:32:09.160591 65336 net.cpp:157] Top shape: 128 256 1 1 (32768)
I0728 10:32:09.160599 65336 net.cpp:165] Memory required for data: 394036736
I0728 10:32:09.160609 65336 layer_factory.hpp:76] Creating layer accuracy
I0728 10:32:09.160630 65336 net.cpp:106] Creating Layer accuracy
I0728 10:32:09.160641 65336 net.cpp:454] accuracy <- conv_c_conv_c_0_split_0
I0728 10:32:09.160651 65336 net.cpp:454] accuracy <- label_data_1_split_0
I0728 10:32:09.160662 65336 net.cpp:411] accuracy -> accuracy
I0728 10:32:09.160681 65336 net.cpp:150] Setting up accuracy
I0728 10:32:09.160691 65336 net.cpp:157] Top shape: (1)
I0728 10:32:09.160701 65336 net.cpp:165] Memory required for data: 394036740
I0728 10:32:09.160712 65336 layer_factory.hpp:76] Creating layer loss
I0728 10:32:09.160727 65336 net.cpp:106] Creating Layer loss
I0728 10:32:09.160737 65336 net.cpp:454] loss <- conv_c_conv_c_0_split_1
I0728 10:32:09.160747 65336 net.cpp:454] loss <- label_data_1_split_1
I0728 10:32:09.160758 65336 net.cpp:411] loss -> loss
I0728 10:32:09.160770 65336 layer_factory.hpp:76] Creating layer loss
I0728 10:32:09.162714 65336 net.cpp:150] Setting up loss
I0728 10:32:09.162736 65336 net.cpp:157] Top shape: (1)
I0728 10:32:09.162745 65336 net.cpp:160]     with loss weight 1
I0728 10:32:09.162762 65336 net.cpp:165] Memory required for data: 394036744
I0728 10:32:09.162775 65336 net.cpp:226] loss needs backward computation.
I0728 10:32:09.162783 65336 net.cpp:228] accuracy does not need backward computation.
I0728 10:32:09.162793 65336 net.cpp:226] conv_c_conv_c_0_split needs backward computation.
I0728 10:32:09.162802 65336 net.cpp:226] conv_c needs backward computation.
I0728 10:32:09.162811 65336 net.cpp:226] drop needs backward computation.
I0728 10:32:09.162818 65336 net.cpp:226] relu4 needs backward computation.
I0728 10:32:09.162825 65336 net.cpp:226] conv4 needs backward computation.
I0728 10:32:09.162833 65336 net.cpp:226] pool3 needs backward computation.
I0728 10:32:09.162842 65336 net.cpp:226] relu32 needs backward computation.
I0728 10:32:09.162850 65336 net.cpp:226] conv32 needs backward computation.
I0728 10:32:09.162858 65336 net.cpp:226] relu31 needs backward computation.
I0728 10:32:09.162866 65336 net.cpp:226] conv31 needs backward computation.
I0728 10:32:09.162875 65336 net.cpp:226] pool2 needs backward computation.
I0728 10:32:09.162884 65336 net.cpp:226] relu22 needs backward computation.
I0728 10:32:09.162891 65336 net.cpp:226] conv22 needs backward computation.
I0728 10:32:09.162899 65336 net.cpp:226] relu21 needs backward computation.
I0728 10:32:09.162907 65336 net.cpp:226] conv21 needs backward computation.
I0728 10:32:09.162915 65336 net.cpp:226] pool1 needs backward computation.
I0728 10:32:09.162926 65336 net.cpp:226] relu12 needs backward computation.
I0728 10:32:09.162935 65336 net.cpp:226] conv12 needs backward computation.
I0728 10:32:09.162942 65336 net.cpp:226] relu11 needs backward computation.
I0728 10:32:09.162950 65336 net.cpp:226] conv11 needs backward computation.
I0728 10:32:09.162978 65336 net.cpp:228] label_data_1_split does not need backward computation.
I0728 10:32:09.162988 65336 net.cpp:228] data does not need backward computation.
I0728 10:32:09.162995 65336 net.cpp:270] This network produces output accuracy
I0728 10:32:09.163007 65336 net.cpp:270] This network produces output loss
I0728 10:32:09.163028 65336 net.cpp:283] Network initialization done.
I0728 10:32:09.163203 65336 solver.cpp:59] Solver scaffolding done.
I0728 10:32:09.163771 65336 caffe.cpp:202] Resuming from models/gnet_iter_19000.solverstate
I0728 10:32:09.237856 65336 sgd_solver.cpp:314] SGDSolver: restoring history
I0728 10:32:09.240193 65336 caffe.cpp:212] Starting Optimization
I0728 10:32:09.240221 65336 solver.cpp:287] Solving Net
I0728 10:32:09.240231 65336 solver.cpp:288] Learning Rate Policy: step
I0728 10:32:09.241199 65336 solver.cpp:340] Iteration 19000, Testing net (#0)
I0728 10:33:05.635982 65336 solver.cpp:408]     Test net output #0: accuracy = 0.930469
I0728 10:33:05.636113 65336 solver.cpp:408]     Test net output #1: loss = 0.168951 (* 1 = 0.168951 loss)
I0728 10:33:12.507709 65336 solver.cpp:236] Iteration 19000, loss = 0.147679
I0728 10:33:12.507776 65336 solver.cpp:252]     Train net output #0: loss = 0.147679 (* 1 = 0.147679 loss)
I0728 10:33:12.507804 65336 sgd_solver.cpp:106] Iteration 19000, lr = 0.001
I0728 10:33:59.703676 65336 solver.cpp:236] Iteration 19010, loss = 0.114388
I0728 10:33:59.703860 65336 solver.cpp:252]     Train net output #0: loss = 0.114388 (* 1 = 0.114388 loss)
I0728 10:33:59.703881 65336 sgd_solver.cpp:106] Iteration 19010, lr = 0.001
I0728 10:34:55.110744 65336 solver.cpp:236] Iteration 19020, loss = 0.0904705
I0728 10:34:55.110949 65336 solver.cpp:252]     Train net output #0: loss = 0.0904705 (* 1 = 0.0904705 loss)
I0728 10:34:55.110983 65336 sgd_solver.cpp:106] Iteration 19020, lr = 0.001
I0728 10:35:56.866997 65336 solver.cpp:236] Iteration 19030, loss = 0.334927
I0728 10:35:56.867213 65336 solver.cpp:252]     Train net output #0: loss = 0.334927 (* 1 = 0.334927 loss)
I0728 10:35:56.867238 65336 sgd_solver.cpp:106] Iteration 19030, lr = 0.001
I0728 10:36:55.411751 65336 solver.cpp:236] Iteration 19040, loss = 0.133737
I0728 10:36:55.411948 65336 solver.cpp:252]     Train net output #0: loss = 0.133737 (* 1 = 0.133737 loss)
I0728 10:36:55.411977 65336 sgd_solver.cpp:106] Iteration 19040, lr = 0.001
I0728 10:37:31.791513 65336 solver.cpp:236] Iteration 19050, loss = 0.300125
I0728 10:37:31.791666 65336 solver.cpp:252]     Train net output #0: loss = 0.300125 (* 1 = 0.300125 loss)
I0728 10:37:31.791682 65336 sgd_solver.cpp:106] Iteration 19050, lr = 0.001
I0728 10:37:56.977367 65336 solver.cpp:236] Iteration 19060, loss = 0.303831
I0728 10:37:56.977464 65336 solver.cpp:252]     Train net output #0: loss = 0.303831 (* 1 = 0.303831 loss)
I0728 10:37:56.977489 65336 sgd_solver.cpp:106] Iteration 19060, lr = 0.001
I0728 10:38:30.578361 65336 solver.cpp:236] Iteration 19070, loss = 0.131431
I0728 10:38:30.578542 65336 solver.cpp:252]     Train net output #0: loss = 0.131431 (* 1 = 0.131431 loss)
I0728 10:38:30.578562 65336 sgd_solver.cpp:106] Iteration 19070, lr = 0.001
I0728 10:39:03.651682 65336 solver.cpp:236] Iteration 19080, loss = 0.171473
I0728 10:39:03.651880 65336 solver.cpp:252]     Train net output #0: loss = 0.171473 (* 1 = 0.171473 loss)
I0728 10:39:03.651902 65336 sgd_solver.cpp:106] Iteration 19080, lr = 0.001
I0728 10:39:34.453860 65336 solver.cpp:236] Iteration 19090, loss = 0.117087
I0728 10:39:34.454054 65336 solver.cpp:252]     Train net output #0: loss = 0.117087 (* 1 = 0.117087 loss)
I0728 10:39:34.454083 65336 sgd_solver.cpp:106] Iteration 19090, lr = 0.001
I0728 10:40:05.263691 65336 solver.cpp:236] Iteration 19100, loss = 0.0672304
I0728 10:40:05.263907 65336 solver.cpp:252]     Train net output #0: loss = 0.0672304 (* 1 = 0.0672304 loss)
I0728 10:40:05.263924 65336 sgd_solver.cpp:106] Iteration 19100, lr = 0.001
I0728 10:40:32.175637 65336 solver.cpp:236] Iteration 19110, loss = 0.213346
I0728 10:40:32.175721 65336 solver.cpp:252]     Train net output #0: loss = 0.213346 (* 1 = 0.213346 loss)
I0728 10:40:32.175743 65336 sgd_solver.cpp:106] Iteration 19110, lr = 0.001
I0728 10:40:59.249858 65336 solver.cpp:236] Iteration 19120, loss = 0.26103
I0728 10:40:59.250152 65336 solver.cpp:252]     Train net output #0: loss = 0.26103 (* 1 = 0.26103 loss)
I0728 10:40:59.250171 65336 sgd_solver.cpp:106] Iteration 19120, lr = 0.001
I0728 10:41:24.819161 65336 solver.cpp:236] Iteration 19130, loss = 0.111834
I0728 10:41:24.819233 65336 solver.cpp:252]     Train net output #0: loss = 0.111834 (* 1 = 0.111834 loss)
I0728 10:41:24.819248 65336 sgd_solver.cpp:106] Iteration 19130, lr = 0.001
I0728 10:41:53.525393 65336 solver.cpp:236] Iteration 19140, loss = 0.1749
I0728 10:41:53.525532 65336 solver.cpp:252]     Train net output #0: loss = 0.1749 (* 1 = 0.1749 loss)
I0728 10:41:53.525549 65336 sgd_solver.cpp:106] Iteration 19140, lr = 0.001
I0728 10:42:28.057773 65336 solver.cpp:236] Iteration 19150, loss = 0.143706
I0728 10:42:28.057950 65336 solver.cpp:252]     Train net output #0: loss = 0.143706 (* 1 = 0.143706 loss)
I0728 10:42:28.057978 65336 sgd_solver.cpp:106] Iteration 19150, lr = 0.001
I0728 10:42:58.000267 65336 solver.cpp:236] Iteration 19160, loss = 0.0190722
I0728 10:42:58.000329 65336 solver.cpp:252]     Train net output #0: loss = 0.0190722 (* 1 = 0.0190722 loss)
I0728 10:42:58.000345 65336 sgd_solver.cpp:106] Iteration 19160, lr = 0.001
I0728 10:43:28.662971 65336 solver.cpp:236] Iteration 19170, loss = 0.178363
I0728 10:43:28.663197 65336 solver.cpp:252]     Train net output #0: loss = 0.178363 (* 1 = 0.178363 loss)
I0728 10:43:28.663218 65336 sgd_solver.cpp:106] Iteration 19170, lr = 0.001
I0728 10:44:01.202626 65336 solver.cpp:236] Iteration 19180, loss = 0.2151
I0728 10:44:01.202867 65336 solver.cpp:252]     Train net output #0: loss = 0.2151 (* 1 = 0.2151 loss)
I0728 10:44:01.202898 65336 sgd_solver.cpp:106] Iteration 19180, lr = 0.001
I0728 10:44:30.326174 65336 solver.cpp:236] Iteration 19190, loss = 0.120974
I0728 10:44:30.326253 65336 solver.cpp:252]     Train net output #0: loss = 0.120974 (* 1 = 0.120974 loss)
I0728 10:44:30.326270 65336 sgd_solver.cpp:106] Iteration 19190, lr = 0.001
I0728 10:45:02.776777 65336 solver.cpp:236] Iteration 19200, loss = 0.245256
I0728 10:45:02.776932 65336 solver.cpp:252]     Train net output #0: loss = 0.245256 (* 1 = 0.245256 loss)
I0728 10:45:02.776962 65336 sgd_solver.cpp:106] Iteration 19200, lr = 0.001
I0728 10:45:32.910235 65336 solver.cpp:236] Iteration 19210, loss = 0.346762
I0728 10:45:32.914717 65336 solver.cpp:252]     Train net output #0: loss = 0.346762 (* 1 = 0.346762 loss)
I0728 10:45:32.914753 65336 sgd_solver.cpp:106] Iteration 19210, lr = 0.001
I0728 10:46:04.036533 65336 solver.cpp:236] Iteration 19220, loss = 0.194684
I0728 10:46:04.036702 65336 solver.cpp:252]     Train net output #0: loss = 0.194684 (* 1 = 0.194684 loss)
I0728 10:46:04.036725 65336 sgd_solver.cpp:106] Iteration 19220, lr = 0.001
I0728 10:46:35.792327 65336 solver.cpp:236] Iteration 19230, loss = 0.140527
I0728 10:46:35.794395 65336 solver.cpp:252]     Train net output #0: loss = 0.140527 (* 1 = 0.140527 loss)
I0728 10:46:35.794414 65336 sgd_solver.cpp:106] Iteration 19230, lr = 0.001
I0728 10:47:06.781534 65336 solver.cpp:236] Iteration 19240, loss = 0.112994
I0728 10:47:06.781723 65336 solver.cpp:252]     Train net output #0: loss = 0.112994 (* 1 = 0.112994 loss)
I0728 10:47:06.781746 65336 sgd_solver.cpp:106] Iteration 19240, lr = 0.001
I0728 10:47:40.183140 65336 solver.cpp:236] Iteration 19250, loss = 0.104005
I0728 10:47:40.183307 65336 solver.cpp:252]     Train net output #0: loss = 0.104005 (* 1 = 0.104005 loss)
I0728 10:47:40.183327 65336 sgd_solver.cpp:106] Iteration 19250, lr = 0.001
I0728 10:48:05.878481 65336 solver.cpp:236] Iteration 19260, loss = 0.0936805
I0728 10:48:05.878550 65336 solver.cpp:252]     Train net output #0: loss = 0.0936806 (* 1 = 0.0936806 loss)
I0728 10:48:05.878564 65336 sgd_solver.cpp:106] Iteration 19260, lr = 0.001
I0728 10:48:36.672328 65336 solver.cpp:236] Iteration 19270, loss = 0.174277
I0728 10:48:36.672524 65336 solver.cpp:252]     Train net output #0: loss = 0.174277 (* 1 = 0.174277 loss)
I0728 10:48:36.672541 65336 sgd_solver.cpp:106] Iteration 19270, lr = 0.001
I0728 10:49:05.001044 65336 solver.cpp:236] Iteration 19280, loss = 0.189366
I0728 10:49:05.001132 65336 solver.cpp:252]     Train net output #0: loss = 0.189366 (* 1 = 0.189366 loss)
I0728 10:49:05.001148 65336 sgd_solver.cpp:106] Iteration 19280, lr = 0.001
I0728 10:49:39.748628 65336 solver.cpp:236] Iteration 19290, loss = 0.182614
I0728 10:49:39.748816 65336 solver.cpp:252]     Train net output #0: loss = 0.182614 (* 1 = 0.182614 loss)
I0728 10:49:39.748836 65336 sgd_solver.cpp:106] Iteration 19290, lr = 0.001
I0728 10:50:06.623298 65336 solver.cpp:236] Iteration 19300, loss = 0.216591
I0728 10:50:06.623369 65336 solver.cpp:252]     Train net output #0: loss = 0.216591 (* 1 = 0.216591 loss)
I0728 10:50:06.623388 65336 sgd_solver.cpp:106] Iteration 19300, lr = 0.001
I0728 10:50:33.525003 65336 solver.cpp:236] Iteration 19310, loss = 0.155118
I0728 10:50:33.525178 65336 solver.cpp:252]     Train net output #0: loss = 0.155118 (* 1 = 0.155118 loss)
I0728 10:50:33.525215 65336 sgd_solver.cpp:106] Iteration 19310, lr = 0.001
I0728 10:51:06.277878 65336 solver.cpp:236] Iteration 19320, loss = 0.241643
I0728 10:51:06.278005 65336 solver.cpp:252]     Train net output #0: loss = 0.241643 (* 1 = 0.241643 loss)
I0728 10:51:06.278023 65336 sgd_solver.cpp:106] Iteration 19320, lr = 0.001
I0728 10:51:38.952013 65336 solver.cpp:236] Iteration 19330, loss = 0.220206
I0728 10:51:38.952173 65336 solver.cpp:252]     Train net output #0: loss = 0.220206 (* 1 = 0.220206 loss)
I0728 10:51:38.952193 65336 sgd_solver.cpp:106] Iteration 19330, lr = 0.001
I0728 10:52:11.538290 65336 solver.cpp:236] Iteration 19340, loss = 0.207063
I0728 10:52:11.538455 65336 solver.cpp:252]     Train net output #0: loss = 0.207063 (* 1 = 0.207063 loss)
I0728 10:52:11.538489 65336 sgd_solver.cpp:106] Iteration 19340, lr = 0.001
I0728 10:52:43.016476 65336 solver.cpp:236] Iteration 19350, loss = 0.0629913
I0728 10:52:43.016639 65336 solver.cpp:252]     Train net output #0: loss = 0.0629913 (* 1 = 0.0629913 loss)
I0728 10:52:43.016666 65336 sgd_solver.cpp:106] Iteration 19350, lr = 0.001
I0728 10:53:18.322515 65336 solver.cpp:236] Iteration 19360, loss = 0.0180865
I0728 10:53:18.324378 65336 solver.cpp:252]     Train net output #0: loss = 0.0180866 (* 1 = 0.0180866 loss)
I0728 10:53:18.324403 65336 sgd_solver.cpp:106] Iteration 19360, lr = 0.001
I0728 10:53:49.103055 65336 solver.cpp:236] Iteration 19370, loss = 0.0945114
I0728 10:53:49.103250 65336 solver.cpp:252]     Train net output #0: loss = 0.0945114 (* 1 = 0.0945114 loss)
I0728 10:53:49.103266 65336 sgd_solver.cpp:106] Iteration 19370, lr = 0.001
I0728 10:54:18.747977 65336 solver.cpp:236] Iteration 19380, loss = 0.257923
I0728 10:54:18.748039 65336 solver.cpp:252]     Train net output #0: loss = 0.257923 (* 1 = 0.257923 loss)
I0728 10:54:18.748054 65336 sgd_solver.cpp:106] Iteration 19380, lr = 0.001
I0728 10:54:47.070996 65336 solver.cpp:236] Iteration 19390, loss = 0.205021
I0728 10:54:47.071135 65336 solver.cpp:252]     Train net output #0: loss = 0.205021 (* 1 = 0.205021 loss)
I0728 10:54:47.071152 65336 sgd_solver.cpp:106] Iteration 19390, lr = 0.001
I0728 10:55:20.186686 65336 solver.cpp:236] Iteration 19400, loss = 0.164793
I0728 10:55:20.186933 65336 solver.cpp:252]     Train net output #0: loss = 0.164793 (* 1 = 0.164793 loss)
I0728 10:55:20.186951 65336 sgd_solver.cpp:106] Iteration 19400, lr = 0.001
I0728 10:55:48.269445 65336 solver.cpp:236] Iteration 19410, loss = 0.0259699
I0728 10:55:48.269508 65336 solver.cpp:252]     Train net output #0: loss = 0.02597 (* 1 = 0.02597 loss)
I0728 10:55:48.269523 65336 sgd_solver.cpp:106] Iteration 19410, lr = 0.001
I0728 10:56:26.297164 65336 solver.cpp:236] Iteration 19420, loss = 0.0088685
I0728 10:56:26.297351 65336 solver.cpp:252]     Train net output #0: loss = 0.00886858 (* 1 = 0.00886858 loss)
I0728 10:56:26.297374 65336 sgd_solver.cpp:106] Iteration 19420, lr = 0.001
I0728 10:56:57.123731 65336 solver.cpp:236] Iteration 19430, loss = 0.177393
I0728 10:56:57.123910 65336 solver.cpp:252]     Train net output #0: loss = 0.177393 (* 1 = 0.177393 loss)
I0728 10:56:57.123950 65336 sgd_solver.cpp:106] Iteration 19430, lr = 0.001
I0728 10:57:27.047890 65336 solver.cpp:236] Iteration 19440, loss = 0.21517
I0728 10:57:27.047971 65336 solver.cpp:252]     Train net output #0: loss = 0.215171 (* 1 = 0.215171 loss)
I0728 10:57:27.047988 65336 sgd_solver.cpp:106] Iteration 19440, lr = 0.001
I0728 10:58:03.453948 65336 solver.cpp:236] Iteration 19450, loss = 0.401851
I0728 10:58:03.454099 65336 solver.cpp:252]     Train net output #0: loss = 0.401851 (* 1 = 0.401851 loss)
I0728 10:58:03.454118 65336 sgd_solver.cpp:106] Iteration 19450, lr = 0.001
I0728 10:58:40.739981 65336 solver.cpp:236] Iteration 19460, loss = 0.138111
I0728 10:58:40.740140 65336 solver.cpp:252]     Train net output #0: loss = 0.138111 (* 1 = 0.138111 loss)
I0728 10:58:40.740180 65336 sgd_solver.cpp:106] Iteration 19460, lr = 0.001
I0728 10:59:17.562742 65336 solver.cpp:236] Iteration 19470, loss = 0.277559
I0728 10:59:17.562949 65336 solver.cpp:252]     Train net output #0: loss = 0.277559 (* 1 = 0.277559 loss)
I0728 10:59:17.562975 65336 sgd_solver.cpp:106] Iteration 19470, lr = 0.001
I0728 10:59:48.911219 65336 solver.cpp:236] Iteration 19480, loss = 0.0807082
I0728 10:59:48.911373 65336 solver.cpp:252]     Train net output #0: loss = 0.0807083 (* 1 = 0.0807083 loss)
I0728 10:59:48.911391 65336 sgd_solver.cpp:106] Iteration 19480, lr = 0.001
I0728 11:00:17.157491 65336 solver.cpp:236] Iteration 19490, loss = 0.106501
I0728 11:00:17.157557 65336 solver.cpp:252]     Train net output #0: loss = 0.106501 (* 1 = 0.106501 loss)
I0728 11:00:17.157574 65336 sgd_solver.cpp:106] Iteration 19490, lr = 0.001
I0728 11:00:50.411201 65336 solver.cpp:340] Iteration 19500, Testing net (#0)
I0728 11:01:17.930105 65336 solver.cpp:408]     Test net output #0: accuracy = 0.944531
I0728 11:01:17.930179 65336 solver.cpp:408]     Test net output #1: loss = 0.140345 (* 1 = 0.140345 loss)
I0728 11:01:22.186229 65336 solver.cpp:236] Iteration 19500, loss = 0.0971319
I0728 11:01:22.189507 65336 solver.cpp:252]     Train net output #0: loss = 0.097132 (* 1 = 0.097132 loss)
I0728 11:01:22.189527 65336 sgd_solver.cpp:106] Iteration 19500, lr = 0.001
I0728 11:02:01.710342 65336 solver.cpp:236] Iteration 19510, loss = 0.240429
I0728 11:02:01.710505 65336 solver.cpp:252]     Train net output #0: loss = 0.240429 (* 1 = 0.240429 loss)
I0728 11:02:01.710522 65336 sgd_solver.cpp:106] Iteration 19510, lr = 0.001
I0728 11:02:32.326321 65336 solver.cpp:236] Iteration 19520, loss = 0.153351
I0728 11:02:32.326462 65336 solver.cpp:252]     Train net output #0: loss = 0.153351 (* 1 = 0.153351 loss)
I0728 11:02:32.326480 65336 sgd_solver.cpp:106] Iteration 19520, lr = 0.001
I0728 11:03:09.217694 65336 solver.cpp:236] Iteration 19530, loss = 0.249256
I0728 11:03:09.220541 65336 solver.cpp:252]     Train net output #0: loss = 0.249256 (* 1 = 0.249256 loss)
I0728 11:03:09.220563 65336 sgd_solver.cpp:106] Iteration 19530, lr = 0.001
I0728 11:03:48.710454 65336 solver.cpp:236] Iteration 19540, loss = 0.0310413
I0728 11:03:48.710891 65336 solver.cpp:252]     Train net output #0: loss = 0.0310415 (* 1 = 0.0310415 loss)
I0728 11:03:48.710912 65336 sgd_solver.cpp:106] Iteration 19540, lr = 0.001
I0728 11:04:26.634636 65336 solver.cpp:236] Iteration 19550, loss = 0.202159
I0728 11:04:26.634845 65336 solver.cpp:252]     Train net output #0: loss = 0.202159 (* 1 = 0.202159 loss)
I0728 11:04:26.634862 65336 sgd_solver.cpp:106] Iteration 19550, lr = 0.001
I0728 11:05:03.557056 65336 solver.cpp:236] Iteration 19560, loss = 0.150386
I0728 11:05:03.557224 65336 solver.cpp:252]     Train net output #0: loss = 0.150386 (* 1 = 0.150386 loss)
I0728 11:05:03.557243 65336 sgd_solver.cpp:106] Iteration 19560, lr = 0.001
I0728 11:05:45.647416 65336 solver.cpp:236] Iteration 19570, loss = 0.123381
I0728 11:05:45.647624 65336 solver.cpp:252]     Train net output #0: loss = 0.123381 (* 1 = 0.123381 loss)
I0728 11:05:45.647642 65336 sgd_solver.cpp:106] Iteration 19570, lr = 0.001
I0728 11:06:19.575001 65336 solver.cpp:236] Iteration 19580, loss = 0.159939
I0728 11:06:19.575500 65336 solver.cpp:252]     Train net output #0: loss = 0.159939 (* 1 = 0.159939 loss)
I0728 11:06:19.575526 65336 sgd_solver.cpp:106] Iteration 19580, lr = 0.001
I0728 11:06:56.120584 65336 solver.cpp:236] Iteration 19590, loss = 0.298589
I0728 11:06:56.120764 65336 solver.cpp:252]     Train net output #0: loss = 0.29859 (* 1 = 0.29859 loss)
I0728 11:06:56.120780 65336 sgd_solver.cpp:106] Iteration 19590, lr = 0.001
I0728 11:07:29.535899 65336 solver.cpp:236] Iteration 19600, loss = 0.239444
I0728 11:07:29.536083 65336 solver.cpp:252]     Train net output #0: loss = 0.239444 (* 1 = 0.239444 loss)
I0728 11:07:29.536101 65336 sgd_solver.cpp:106] Iteration 19600, lr = 0.001
I0728 11:08:03.497282 65336 solver.cpp:236] Iteration 19610, loss = 0.112238
I0728 11:08:03.497419 65336 solver.cpp:252]     Train net output #0: loss = 0.112238 (* 1 = 0.112238 loss)
I0728 11:08:03.497438 65336 sgd_solver.cpp:106] Iteration 19610, lr = 0.001
I0728 11:08:38.967947 65336 solver.cpp:236] Iteration 19620, loss = 0.089871
I0728 11:08:38.968171 65336 solver.cpp:252]     Train net output #0: loss = 0.0898712 (* 1 = 0.0898712 loss)
I0728 11:08:38.968197 65336 sgd_solver.cpp:106] Iteration 19620, lr = 0.001
I0728 11:09:22.138289 65336 solver.cpp:236] Iteration 19630, loss = 0.22745
I0728 11:09:22.138489 65336 solver.cpp:252]     Train net output #0: loss = 0.22745 (* 1 = 0.22745 loss)
I0728 11:09:22.138509 65336 sgd_solver.cpp:106] Iteration 19630, lr = 0.001
I0728 11:09:56.534942 65336 solver.cpp:236] Iteration 19640, loss = 0.013714
I0728 11:09:56.535195 65336 solver.cpp:252]     Train net output #0: loss = 0.0137142 (* 1 = 0.0137142 loss)
I0728 11:09:56.535221 65336 sgd_solver.cpp:106] Iteration 19640, lr = 0.001
I0728 11:10:29.997277 65336 solver.cpp:236] Iteration 19650, loss = 0.0241093
I0728 11:10:29.997509 65336 solver.cpp:252]     Train net output #0: loss = 0.0241094 (* 1 = 0.0241094 loss)
I0728 11:10:29.997535 65336 sgd_solver.cpp:106] Iteration 19650, lr = 0.001
I0728 11:11:07.871913 65336 solver.cpp:236] Iteration 19660, loss = 0.154208
I0728 11:11:07.872072 65336 solver.cpp:252]     Train net output #0: loss = 0.154208 (* 1 = 0.154208 loss)
I0728 11:11:07.872090 65336 sgd_solver.cpp:106] Iteration 19660, lr = 0.001
I0728 11:11:37.551010 65336 solver.cpp:236] Iteration 19670, loss = 0.0167427
I0728 11:11:37.551103 65336 solver.cpp:252]     Train net output #0: loss = 0.0167429 (* 1 = 0.0167429 loss)
I0728 11:11:37.551120 65336 sgd_solver.cpp:106] Iteration 19670, lr = 0.001
I0728 11:12:20.498667 65336 solver.cpp:236] Iteration 19680, loss = 0.234243
I0728 11:12:20.498913 65336 solver.cpp:252]     Train net output #0: loss = 0.234243 (* 1 = 0.234243 loss)
I0728 11:12:20.498946 65336 sgd_solver.cpp:106] Iteration 19680, lr = 0.001
I0728 11:13:15.147966 65336 solver.cpp:236] Iteration 19690, loss = 0.0878884
I0728 11:13:15.149806 65336 solver.cpp:252]     Train net output #0: loss = 0.0878886 (* 1 = 0.0878886 loss)
I0728 11:13:15.149829 65336 sgd_solver.cpp:106] Iteration 19690, lr = 0.001
I0728 11:14:11.482578 65336 solver.cpp:236] Iteration 19700, loss = 0.142194
I0728 11:14:11.482772 65336 solver.cpp:252]     Train net output #0: loss = 0.142194 (* 1 = 0.142194 loss)
I0728 11:14:11.482802 65336 sgd_solver.cpp:106] Iteration 19700, lr = 0.001
I0728 11:15:06.856997 65336 solver.cpp:236] Iteration 19710, loss = 0.229892
I0728 11:15:06.857195 65336 solver.cpp:252]     Train net output #0: loss = 0.229893 (* 1 = 0.229893 loss)
I0728 11:15:06.857216 65336 sgd_solver.cpp:106] Iteration 19710, lr = 0.001
I0728 11:16:13.817730 65336 solver.cpp:236] Iteration 19720, loss = 0.118829
I0728 11:16:13.825013 65336 solver.cpp:252]     Train net output #0: loss = 0.118829 (* 1 = 0.118829 loss)
I0728 11:16:13.825042 65336 sgd_solver.cpp:106] Iteration 19720, lr = 0.001
I0728 11:17:08.624905 65336 solver.cpp:236] Iteration 19730, loss = 0.204598
I0728 11:17:08.638257 65336 solver.cpp:252]     Train net output #0: loss = 0.204598 (* 1 = 0.204598 loss)
I0728 11:17:08.638309 65336 sgd_solver.cpp:106] Iteration 19730, lr = 0.001
I0728 11:18:20.653254 65336 solver.cpp:236] Iteration 19740, loss = 0.160046
I0728 11:18:20.653466 65336 solver.cpp:252]     Train net output #0: loss = 0.160046 (* 1 = 0.160046 loss)
I0728 11:18:20.653486 65336 sgd_solver.cpp:106] Iteration 19740, lr = 0.001
I0728 11:19:30.438771 65336 solver.cpp:236] Iteration 19750, loss = 0.0526477
I0728 11:19:30.438923 65336 solver.cpp:252]     Train net output #0: loss = 0.0526478 (* 1 = 0.0526478 loss)
I0728 11:19:30.438946 65336 sgd_solver.cpp:106] Iteration 19750, lr = 0.001
I0728 11:20:17.829208 65336 solver.cpp:236] Iteration 19760, loss = 0.133835
I0728 11:20:17.829375 65336 solver.cpp:252]     Train net output #0: loss = 0.133835 (* 1 = 0.133835 loss)
I0728 11:20:17.829396 65336 sgd_solver.cpp:106] Iteration 19760, lr = 0.001
I0728 11:21:12.430250 65336 solver.cpp:236] Iteration 19770, loss = 0.221015
I0728 11:21:12.430404 65336 solver.cpp:252]     Train net output #0: loss = 0.221015 (* 1 = 0.221015 loss)
I0728 11:21:12.430425 65336 sgd_solver.cpp:106] Iteration 19770, lr = 0.001
I0728 11:22:14.581019 65336 solver.cpp:236] Iteration 19780, loss = 0.109249
I0728 11:22:14.581202 65336 solver.cpp:252]     Train net output #0: loss = 0.109249 (* 1 = 0.109249 loss)
I0728 11:22:14.581226 65336 sgd_solver.cpp:106] Iteration 19780, lr = 0.001
I0728 11:23:18.394116 65336 solver.cpp:236] Iteration 19790, loss = 0.0703617
I0728 11:23:18.394264 65336 solver.cpp:252]     Train net output #0: loss = 0.0703619 (* 1 = 0.0703619 loss)
I0728 11:23:18.394285 65336 sgd_solver.cpp:106] Iteration 19790, lr = 0.001
I0728 11:24:27.115142 65336 solver.cpp:236] Iteration 19800, loss = 0.391035
I0728 11:24:27.115296 65336 solver.cpp:252]     Train net output #0: loss = 0.391035 (* 1 = 0.391035 loss)
I0728 11:24:27.115321 65336 sgd_solver.cpp:106] Iteration 19800, lr = 0.001
I0728 11:25:27.024348 65336 solver.cpp:236] Iteration 19810, loss = 0.181663
I0728 11:25:27.024687 65336 solver.cpp:252]     Train net output #0: loss = 0.181663 (* 1 = 0.181663 loss)
I0728 11:25:27.024711 65336 sgd_solver.cpp:106] Iteration 19810, lr = 0.001
I0728 11:26:27.153534 65336 solver.cpp:236] Iteration 19820, loss = 0.0738353
I0728 11:26:27.153754 65336 solver.cpp:252]     Train net output #0: loss = 0.0738355 (* 1 = 0.0738355 loss)
I0728 11:26:27.153780 65336 sgd_solver.cpp:106] Iteration 19820, lr = 0.001
I0728 11:27:15.119690 65336 solver.cpp:236] Iteration 19830, loss = 0.276409
I0728 11:27:15.119844 65336 solver.cpp:252]     Train net output #0: loss = 0.276409 (* 1 = 0.276409 loss)
I0728 11:27:15.119874 65336 sgd_solver.cpp:106] Iteration 19830, lr = 0.001
I0728 11:28:11.360966 65336 solver.cpp:236] Iteration 19840, loss = 0.149379
I0728 11:28:11.361138 65336 solver.cpp:252]     Train net output #0: loss = 0.149379 (* 1 = 0.149379 loss)
I0728 11:28:11.361157 65336 sgd_solver.cpp:106] Iteration 19840, lr = 0.001
I0728 11:29:19.493564 65336 solver.cpp:236] Iteration 19850, loss = 0.178432
I0728 11:29:19.493767 65336 solver.cpp:252]     Train net output #0: loss = 0.178433 (* 1 = 0.178433 loss)
I0728 11:29:19.493808 65336 sgd_solver.cpp:106] Iteration 19850, lr = 0.001
I0728 11:30:07.561461 65336 solver.cpp:236] Iteration 19860, loss = 0.132549
I0728 11:30:07.561673 65336 solver.cpp:252]     Train net output #0: loss = 0.132549 (* 1 = 0.132549 loss)
I0728 11:30:07.561691 65336 sgd_solver.cpp:106] Iteration 19860, lr = 0.001
I0728 11:31:22.454421 65336 solver.cpp:236] Iteration 19870, loss = 0.246977
I0728 11:31:22.454646 65336 solver.cpp:252]     Train net output #0: loss = 0.246977 (* 1 = 0.246977 loss)
I0728 11:31:22.454673 65336 sgd_solver.cpp:106] Iteration 19870, lr = 0.001
I0728 11:32:38.584300 65336 solver.cpp:236] Iteration 19880, loss = 0.108465
I0728 11:32:38.584463 65336 solver.cpp:252]     Train net output #0: loss = 0.108465 (* 1 = 0.108465 loss)
I0728 11:32:38.584484 65336 sgd_solver.cpp:106] Iteration 19880, lr = 0.001
I0728 11:33:38.985980 65336 solver.cpp:236] Iteration 19890, loss = 0.127112
I0728 11:33:38.996492 65336 solver.cpp:252]     Train net output #0: loss = 0.127112 (* 1 = 0.127112 loss)
I0728 11:33:38.996520 65336 sgd_solver.cpp:106] Iteration 19890, lr = 0.001
I0728 11:34:43.751760 65336 solver.cpp:236] Iteration 19900, loss = 0.119124
I0728 11:34:43.751946 65336 solver.cpp:252]     Train net output #0: loss = 0.119124 (* 1 = 0.119124 loss)
I0728 11:34:43.751969 65336 sgd_solver.cpp:106] Iteration 19900, lr = 0.001
I0728 11:36:06.424619 65336 solver.cpp:236] Iteration 19910, loss = 0.188753
I0728 11:36:06.424809 65336 solver.cpp:252]     Train net output #0: loss = 0.188753 (* 1 = 0.188753 loss)
I0728 11:36:06.424830 65336 sgd_solver.cpp:106] Iteration 19910, lr = 0.001
I0728 11:37:15.807977 65336 solver.cpp:236] Iteration 19920, loss = 0.0104891
I0728 11:37:15.808184 65336 solver.cpp:252]     Train net output #0: loss = 0.0104892 (* 1 = 0.0104892 loss)
I0728 11:37:15.808219 65336 sgd_solver.cpp:106] Iteration 19920, lr = 0.001
I0728 11:38:22.555765 65336 solver.cpp:236] Iteration 19930, loss = 0.021661
I0728 11:38:22.556005 65336 solver.cpp:252]     Train net output #0: loss = 0.0216611 (* 1 = 0.0216611 loss)
I0728 11:38:22.556023 65336 sgd_solver.cpp:106] Iteration 19930, lr = 0.001
I0728 11:39:46.366966 65336 solver.cpp:236] Iteration 19940, loss = 0.116836
I0728 11:39:46.367139 65336 solver.cpp:252]     Train net output #0: loss = 0.116836 (* 1 = 0.116836 loss)
I0728 11:39:46.367158 65336 sgd_solver.cpp:106] Iteration 19940, lr = 0.001
I0728 11:41:06.780881 65336 solver.cpp:236] Iteration 19950, loss = 0.216373
I0728 11:41:06.781358 65336 solver.cpp:252]     Train net output #0: loss = 0.216373 (* 1 = 0.216373 loss)
I0728 11:41:06.781376 65336 sgd_solver.cpp:106] Iteration 19950, lr = 0.001
I0728 11:42:16.368515 65336 solver.cpp:236] Iteration 19960, loss = 0.194277
I0728 11:42:16.368790 65336 solver.cpp:252]     Train net output #0: loss = 0.194277 (* 1 = 0.194277 loss)
I0728 11:42:16.368834 65336 sgd_solver.cpp:106] Iteration 19960, lr = 0.001
I0728 11:43:18.489835 65336 solver.cpp:236] Iteration 19970, loss = 0.16363
I0728 11:43:18.489995 65336 solver.cpp:252]     Train net output #0: loss = 0.16363 (* 1 = 0.16363 loss)
I0728 11:43:18.490016 65336 sgd_solver.cpp:106] Iteration 19970, lr = 0.001
I0728 11:44:31.947932 65336 solver.cpp:236] Iteration 19980, loss = 0.179166
I0728 11:44:31.948123 65336 solver.cpp:252]     Train net output #0: loss = 0.179166 (* 1 = 0.179166 loss)
I0728 11:44:31.948151 65336 sgd_solver.cpp:106] Iteration 19980, lr = 0.001
I0728 11:45:36.936782 65336 solver.cpp:236] Iteration 19990, loss = 0.179219
I0728 11:45:36.936960 65336 solver.cpp:252]     Train net output #0: loss = 0.179219 (* 1 = 0.179219 loss)
I0728 11:45:36.936978 65336 sgd_solver.cpp:106] Iteration 19990, lr = 0.001
I0728 11:46:35.745324 65336 solver.cpp:461] Snapshotting to binary proto file models/gnet_iter_20000.caffemodel
I0728 11:46:36.030844 65336 sgd_solver.cpp:269] Snapshotting solver state to binary proto file models/gnet_iter_20000.solverstate
I0728 11:46:36.035298 65336 solver.cpp:340] Iteration 20000, Testing net (#0)
I0728 11:47:44.834766 65336 solver.cpp:408]     Test net output #0: accuracy = 0.928906
I0728 11:47:44.834944 65336 solver.cpp:408]     Test net output #1: loss = 0.168546 (* 1 = 0.168546 loss)
I0728 11:47:49.623765 65336 solver.cpp:236] Iteration 20000, loss = 0.16663
I0728 11:47:49.623838 65336 solver.cpp:252]     Train net output #0: loss = 0.16663 (* 1 = 0.16663 loss)
I0728 11:47:49.623857 65336 sgd_solver.cpp:106] Iteration 20000, lr = 0.0001
I0728 11:48:55.129503 65336 solver.cpp:236] Iteration 20010, loss = 0.00385572
I0728 11:48:55.129653 65336 solver.cpp:252]     Train net output #0: loss = 0.00385583 (* 1 = 0.00385583 loss)
I0728 11:48:55.129670 65336 sgd_solver.cpp:106] Iteration 20010, lr = 0.0001
I0728 11:49:54.707378 65336 solver.cpp:236] Iteration 20020, loss = 0.290083
I0728 11:49:54.707658 65336 solver.cpp:252]     Train net output #0: loss = 0.290083 (* 1 = 0.290083 loss)
I0728 11:49:54.707690 65336 sgd_solver.cpp:106] Iteration 20020, lr = 0.0001
I0728 11:50:47.201037 65336 solver.cpp:236] Iteration 20030, loss = 0.251398
I0728 11:50:47.201277 65336 solver.cpp:252]     Train net output #0: loss = 0.251398 (* 1 = 0.251398 loss)
I0728 11:50:47.201305 65336 sgd_solver.cpp:106] Iteration 20030, lr = 0.0001
I0728 11:51:56.746882 65336 solver.cpp:236] Iteration 20040, loss = 0.166502
I0728 11:51:56.747056 65336 solver.cpp:252]     Train net output #0: loss = 0.166502 (* 1 = 0.166502 loss)
I0728 11:51:56.747102 65336 sgd_solver.cpp:106] Iteration 20040, lr = 0.0001
I0728 11:53:07.723897 65336 solver.cpp:236] Iteration 20050, loss = 0.0621804
I0728 11:53:07.724061 65336 solver.cpp:252]     Train net output #0: loss = 0.0621805 (* 1 = 0.0621805 loss)
I0728 11:53:07.724083 65336 sgd_solver.cpp:106] Iteration 20050, lr = 0.0001
I0728 11:54:06.432679 65336 solver.cpp:236] Iteration 20060, loss = 0.207136
I0728 11:54:06.432838 65336 solver.cpp:252]     Train net output #0: loss = 0.207136 (* 1 = 0.207136 loss)
I0728 11:54:06.432863 65336 sgd_solver.cpp:106] Iteration 20060, lr = 0.0001
I0728 11:55:08.009481 65336 solver.cpp:236] Iteration 20070, loss = 0.00813875
I0728 11:55:08.009654 65336 solver.cpp:252]     Train net output #0: loss = 0.00813887 (* 1 = 0.00813887 loss)
I0728 11:55:08.009690 65336 sgd_solver.cpp:106] Iteration 20070, lr = 0.0001
I0728 11:56:02.056828 65336 solver.cpp:236] Iteration 20080, loss = 0.109089
I0728 11:56:02.056996 65336 solver.cpp:252]     Train net output #0: loss = 0.109089 (* 1 = 0.109089 loss)
I0728 11:56:02.057025 65336 sgd_solver.cpp:106] Iteration 20080, lr = 0.0001
I0728 11:56:55.115723 65336 solver.cpp:236] Iteration 20090, loss = 0.174424
I0728 11:56:55.118680 65336 solver.cpp:252]     Train net output #0: loss = 0.174424 (* 1 = 0.174424 loss)
I0728 11:56:55.118698 65336 sgd_solver.cpp:106] Iteration 20090, lr = 0.0001
I0728 11:58:03.104068 65336 solver.cpp:236] Iteration 20100, loss = 0.137081
I0728 11:58:03.104254 65336 solver.cpp:252]     Train net output #0: loss = 0.137081 (* 1 = 0.137081 loss)
I0728 11:58:03.104277 65336 sgd_solver.cpp:106] Iteration 20100, lr = 0.0001
I0728 11:59:16.553194 65336 solver.cpp:236] Iteration 20110, loss = 0.146268
I0728 11:59:16.553367 65336 solver.cpp:252]     Train net output #0: loss = 0.146268 (* 1 = 0.146268 loss)
I0728 11:59:16.553386 65336 sgd_solver.cpp:106] Iteration 20110, lr = 0.0001
I0728 12:00:24.038230 65336 solver.cpp:236] Iteration 20120, loss = 0.00768162
I0728 12:00:24.038409 65336 solver.cpp:252]     Train net output #0: loss = 0.00768175 (* 1 = 0.00768175 loss)
I0728 12:00:24.038429 65336 sgd_solver.cpp:106] Iteration 20120, lr = 0.0001
I0728 12:01:18.282218 65336 solver.cpp:236] Iteration 20130, loss = 0.163787
I0728 12:01:18.282439 65336 solver.cpp:252]     Train net output #0: loss = 0.163787 (* 1 = 0.163787 loss)
I0728 12:01:18.282465 65336 sgd_solver.cpp:106] Iteration 20130, lr = 0.0001
I0728 12:02:07.199772 65336 solver.cpp:236] Iteration 20140, loss = 0.115013
I0728 12:02:07.211017 65336 solver.cpp:252]     Train net output #0: loss = 0.115013 (* 1 = 0.115013 loss)
I0728 12:02:07.211046 65336 sgd_solver.cpp:106] Iteration 20140, lr = 0.0001
I0728 12:02:55.167567 65336 solver.cpp:236] Iteration 20150, loss = 0.132839
I0728 12:02:55.167742 65336 solver.cpp:252]     Train net output #0: loss = 0.132839 (* 1 = 0.132839 loss)
I0728 12:02:55.167762 65336 sgd_solver.cpp:106] Iteration 20150, lr = 0.0001
I0728 12:03:33.804101 65336 solver.cpp:236] Iteration 20160, loss = 0.476567
I0728 12:03:33.804314 65336 solver.cpp:252]     Train net output #0: loss = 0.476567 (* 1 = 0.476567 loss)
I0728 12:03:33.804335 65336 sgd_solver.cpp:106] Iteration 20160, lr = 0.0001
I0728 12:04:11.098036 65336 solver.cpp:236] Iteration 20170, loss = 0.148552
I0728 12:04:11.098245 65336 solver.cpp:252]     Train net output #0: loss = 0.148552 (* 1 = 0.148552 loss)
I0728 12:04:11.098268 65336 sgd_solver.cpp:106] Iteration 20170, lr = 0.0001
I0728 12:04:59.712000 65336 solver.cpp:236] Iteration 20180, loss = 0.223143
I0728 12:04:59.712309 65336 solver.cpp:252]     Train net output #0: loss = 0.223143 (* 1 = 0.223143 loss)
I0728 12:04:59.712330 65336 sgd_solver.cpp:106] Iteration 20180, lr = 0.0001
I0728 12:05:41.725472 65336 solver.cpp:236] Iteration 20190, loss = 0.156644
I0728 12:05:41.725706 65336 solver.cpp:252]     Train net output #0: loss = 0.156645 (* 1 = 0.156645 loss)
I0728 12:05:41.725725 65336 sgd_solver.cpp:106] Iteration 20190, lr = 0.0001
I0728 12:06:45.215843 65336 solver.cpp:236] Iteration 20200, loss = 0.224116
I0728 12:06:45.216051 65336 solver.cpp:252]     Train net output #0: loss = 0.224117 (* 1 = 0.224117 loss)
I0728 12:06:45.216076 65336 sgd_solver.cpp:106] Iteration 20200, lr = 0.0001
I0728 12:07:29.424275 65336 solver.cpp:236] Iteration 20210, loss = 0.0458347
I0728 12:07:29.424460 65336 solver.cpp:252]     Train net output #0: loss = 0.0458348 (* 1 = 0.0458348 loss)
I0728 12:07:29.424502 65336 sgd_solver.cpp:106] Iteration 20210, lr = 0.0001
I0728 12:07:56.409744 65336 solver.cpp:236] Iteration 20220, loss = 0.159267
I0728 12:07:56.409849 65336 solver.cpp:252]     Train net output #0: loss = 0.159267 (* 1 = 0.159267 loss)
I0728 12:07:56.409874 65336 sgd_solver.cpp:106] Iteration 20220, lr = 0.0001
I0728 12:08:36.313868 65336 solver.cpp:236] Iteration 20230, loss = 0.246578
I0728 12:08:36.314057 65336 solver.cpp:252]     Train net output #0: loss = 0.246578 (* 1 = 0.246578 loss)
I0728 12:08:36.314079 65336 sgd_solver.cpp:106] Iteration 20230, lr = 0.0001
I0728 12:09:00.153744 65336 solver.cpp:236] Iteration 20240, loss = 0.0234162
I0728 12:09:00.153820 65336 solver.cpp:252]     Train net output #0: loss = 0.0234164 (* 1 = 0.0234164 loss)
I0728 12:09:00.153842 65336 sgd_solver.cpp:106] Iteration 20240, lr = 0.0001
I0728 12:09:20.938974 65336 solver.cpp:236] Iteration 20250, loss = 0.255153
I0728 12:09:20.939270 65336 solver.cpp:252]     Train net output #0: loss = 0.255154 (* 1 = 0.255154 loss)
I0728 12:09:20.939290 65336 sgd_solver.cpp:106] Iteration 20250, lr = 0.0001
I0728 12:09:58.351634 65336 solver.cpp:236] Iteration 20260, loss = 0.126627
I0728 12:09:58.351801 65336 solver.cpp:252]     Train net output #0: loss = 0.126627 (* 1 = 0.126627 loss)
I0728 12:09:58.351819 65336 sgd_solver.cpp:106] Iteration 20260, lr = 0.0001
I0728 12:10:29.212299 65336 solver.cpp:236] Iteration 20270, loss = 0.0886619
I0728 12:10:29.212491 65336 solver.cpp:252]     Train net output #0: loss = 0.088662 (* 1 = 0.088662 loss)
I0728 12:10:29.212509 65336 sgd_solver.cpp:106] Iteration 20270, lr = 0.0001
I0728 12:11:12.685999 65336 solver.cpp:236] Iteration 20280, loss = 0.281851
I0728 12:11:12.686235 65336 solver.cpp:252]     Train net output #0: loss = 0.281851 (* 1 = 0.281851 loss)
I0728 12:11:12.686274 65336 sgd_solver.cpp:106] Iteration 20280, lr = 0.0001
I0728 12:11:42.124383 65336 solver.cpp:236] Iteration 20290, loss = 0.536902
I0728 12:11:42.124539 65336 solver.cpp:252]     Train net output #0: loss = 0.536902 (* 1 = 0.536902 loss)
I0728 12:11:42.124591 65336 sgd_solver.cpp:106] Iteration 20290, lr = 0.0001
I0728 12:12:07.965008 65336 solver.cpp:236] Iteration 20300, loss = 0.142775
I0728 12:12:07.965183 65336 solver.cpp:252]     Train net output #0: loss = 0.142775 (* 1 = 0.142775 loss)
I0728 12:12:07.965204 65336 sgd_solver.cpp:106] Iteration 20300, lr = 0.0001
I0728 12:12:49.808990 65336 solver.cpp:236] Iteration 20310, loss = 0.156182
I0728 12:12:49.809147 65336 solver.cpp:252]     Train net output #0: loss = 0.156182 (* 1 = 0.156182 loss)
I0728 12:12:49.809168 65336 sgd_solver.cpp:106] Iteration 20310, lr = 0.0001
I0728 12:13:22.386245 65336 solver.cpp:236] Iteration 20320, loss = 0.289392
I0728 12:13:22.386441 65336 solver.cpp:252]     Train net output #0: loss = 0.289392 (* 1 = 0.289392 loss)
I0728 12:13:22.386459 65336 sgd_solver.cpp:106] Iteration 20320, lr = 0.0001
I0728 12:14:16.668740 65336 solver.cpp:236] Iteration 20330, loss = 0.11453
I0728 12:14:16.668915 65336 solver.cpp:252]     Train net output #0: loss = 0.11453 (* 1 = 0.11453 loss)
I0728 12:14:16.668933 65336 sgd_solver.cpp:106] Iteration 20330, lr = 0.0001
I0728 12:14:57.172190 65336 solver.cpp:236] Iteration 20340, loss = 0.390817
I0728 12:14:57.195492 65336 solver.cpp:252]     Train net output #0: loss = 0.390817 (* 1 = 0.390817 loss)
I0728 12:14:57.195519 65336 sgd_solver.cpp:106] Iteration 20340, lr = 0.0001
I0728 12:15:27.193732 65336 solver.cpp:236] Iteration 20350, loss = 0.116409
I0728 12:15:27.193825 65336 solver.cpp:252]     Train net output #0: loss = 0.116409 (* 1 = 0.116409 loss)
I0728 12:15:27.193840 65336 sgd_solver.cpp:106] Iteration 20350, lr = 0.0001
I0728 12:15:57.006326 65336 solver.cpp:236] Iteration 20360, loss = 0.123622
I0728 12:15:57.006533 65336 solver.cpp:252]     Train net output #0: loss = 0.123622 (* 1 = 0.123622 loss)
I0728 12:15:57.006553 65336 sgd_solver.cpp:106] Iteration 20360, lr = 0.0001
I0728 12:16:23.126204 65336 solver.cpp:236] Iteration 20370, loss = 0.215367
I0728 12:16:23.126272 65336 solver.cpp:252]     Train net output #0: loss = 0.215367 (* 1 = 0.215367 loss)
I0728 12:16:23.126291 65336 sgd_solver.cpp:106] Iteration 20370, lr = 0.0001
I0728 12:16:57.800544 65336 solver.cpp:236] Iteration 20380, loss = 0.0829573
I0728 12:16:57.805897 65336 solver.cpp:252]     Train net output #0: loss = 0.0829574 (* 1 = 0.0829574 loss)
I0728 12:16:57.805924 65336 sgd_solver.cpp:106] Iteration 20380, lr = 0.0001
I0728 12:17:26.869704 65336 solver.cpp:236] Iteration 20390, loss = 0.0927587
I0728 12:17:26.869773 65336 solver.cpp:252]     Train net output #0: loss = 0.0927588 (* 1 = 0.0927588 loss)
I0728 12:17:26.869791 65336 sgd_solver.cpp:106] Iteration 20390, lr = 0.0001
I0728 12:18:18.771606 65336 solver.cpp:236] Iteration 20400, loss = 0.160356
I0728 12:18:18.772157 65336 solver.cpp:252]     Train net output #0: loss = 0.160356 (* 1 = 0.160356 loss)
I0728 12:18:18.772187 65336 sgd_solver.cpp:106] Iteration 20400, lr = 0.0001
I0728 12:19:23.970142 65336 solver.cpp:236] Iteration 20410, loss = 0.128369
I0728 12:19:23.970320 65336 solver.cpp:252]     Train net output #0: loss = 0.128369 (* 1 = 0.128369 loss)
I0728 12:19:23.970341 65336 sgd_solver.cpp:106] Iteration 20410, lr = 0.0001
I0728 12:20:09.658097 65336 solver.cpp:236] Iteration 20420, loss = 0.0650827
I0728 12:20:09.658270 65336 solver.cpp:252]     Train net output #0: loss = 0.0650828 (* 1 = 0.0650828 loss)
I0728 12:20:09.658293 65336 sgd_solver.cpp:106] Iteration 20420, lr = 0.0001
I0728 12:20:36.646852 65336 solver.cpp:236] Iteration 20430, loss = 0.186372
I0728 12:20:36.646924 65336 solver.cpp:252]     Train net output #0: loss = 0.186372 (* 1 = 0.186372 loss)
I0728 12:20:36.646940 65336 sgd_solver.cpp:106] Iteration 20430, lr = 0.0001
I0728 12:21:19.682672 65336 solver.cpp:236] Iteration 20440, loss = 0.325263
I0728 12:21:19.682828 65336 solver.cpp:252]     Train net output #0: loss = 0.325263 (* 1 = 0.325263 loss)
I0728 12:21:19.682848 65336 sgd_solver.cpp:106] Iteration 20440, lr = 0.0001
I0728 12:22:11.783542 65336 solver.cpp:236] Iteration 20450, loss = 0.000359759
I0728 12:22:11.783735 65336 solver.cpp:252]     Train net output #0: loss = 0.000359847 (* 1 = 0.000359847 loss)
I0728 12:22:11.783761 65336 sgd_solver.cpp:106] Iteration 20450, lr = 0.0001
I0728 12:22:41.962090 65336 solver.cpp:236] Iteration 20460, loss = 0.0511345
I0728 12:22:41.962369 65336 solver.cpp:252]     Train net output #0: loss = 0.0511346 (* 1 = 0.0511346 loss)
I0728 12:22:41.962398 65336 sgd_solver.cpp:106] Iteration 20460, lr = 0.0001
I0728 12:23:13.671838 65336 solver.cpp:236] Iteration 20470, loss = 0.16021
I0728 12:23:13.672044 65336 solver.cpp:252]     Train net output #0: loss = 0.160211 (* 1 = 0.160211 loss)
I0728 12:23:13.672062 65336 sgd_solver.cpp:106] Iteration 20470, lr = 0.0001
I0728 12:24:00.280740 65336 solver.cpp:236] Iteration 20480, loss = 0.17637
I0728 12:24:00.280954 65336 solver.cpp:252]     Train net output #0: loss = 0.17637 (* 1 = 0.17637 loss)
I0728 12:24:00.280990 65336 sgd_solver.cpp:106] Iteration 20480, lr = 0.0001
I0728 12:24:29.794441 65336 solver.cpp:236] Iteration 20490, loss = 0.205462
I0728 12:24:29.794512 65336 solver.cpp:252]     Train net output #0: loss = 0.205462 (* 1 = 0.205462 loss)
I0728 12:24:29.794528 65336 sgd_solver.cpp:106] Iteration 20490, lr = 0.0001
I0728 12:24:50.733639 65336 solver.cpp:340] Iteration 20500, Testing net (#0)
I0728 12:25:14.840641 65336 solver.cpp:408]     Test net output #0: accuracy = 0.947656
I0728 12:25:14.840713 65336 solver.cpp:408]     Test net output #1: loss = 0.137953 (* 1 = 0.137953 loss)
I0728 12:25:20.153167 65336 solver.cpp:236] Iteration 20500, loss = 0.331295
I0728 12:25:20.153234 65336 solver.cpp:252]     Train net output #0: loss = 0.331295 (* 1 = 0.331295 loss)
I0728 12:25:20.153251 65336 sgd_solver.cpp:106] Iteration 20500, lr = 0.0001
I0728 12:25:50.565328 65336 solver.cpp:236] Iteration 20510, loss = 0.08438
I0728 12:25:50.565646 65336 solver.cpp:252]     Train net output #0: loss = 0.0843801 (* 1 = 0.0843801 loss)
I0728 12:25:50.565665 65336 sgd_solver.cpp:106] Iteration 20510, lr = 0.0001
I0728 12:26:33.776614 65336 solver.cpp:236] Iteration 20520, loss = 0.092657
I0728 12:26:33.776823 65336 solver.cpp:252]     Train net output #0: loss = 0.0926571 (* 1 = 0.0926571 loss)
I0728 12:26:33.776851 65336 sgd_solver.cpp:106] Iteration 20520, lr = 0.0001
I0728 12:27:23.433787 65336 solver.cpp:236] Iteration 20530, loss = 0.178272
I0728 12:27:23.433961 65336 solver.cpp:252]     Train net output #0: loss = 0.178272 (* 1 = 0.178272 loss)
I0728 12:27:23.433979 65336 sgd_solver.cpp:106] Iteration 20530, lr = 0.0001
I0728 12:28:08.515723 65336 solver.cpp:236] Iteration 20540, loss = 0.0432903
I0728 12:28:08.515900 65336 solver.cpp:252]     Train net output #0: loss = 0.0432904 (* 1 = 0.0432904 loss)
I0728 12:28:08.515928 65336 sgd_solver.cpp:106] Iteration 20540, lr = 0.0001
I0728 12:29:10.435870 65336 solver.cpp:236] Iteration 20550, loss = 0.272978
I0728 12:29:10.436022 65336 solver.cpp:252]     Train net output #0: loss = 0.272978 (* 1 = 0.272978 loss)
I0728 12:29:10.436041 65336 sgd_solver.cpp:106] Iteration 20550, lr = 0.0001
I0728 12:30:08.878959 65336 solver.cpp:236] Iteration 20560, loss = 0.147708
I0728 12:30:08.879181 65336 solver.cpp:252]     Train net output #0: loss = 0.147708 (* 1 = 0.147708 loss)
I0728 12:30:08.879201 65336 sgd_solver.cpp:106] Iteration 20560, lr = 0.0001
I0728 12:30:47.056347 65336 solver.cpp:236] Iteration 20570, loss = 0.0885267
I0728 12:30:47.056488 65336 solver.cpp:252]     Train net output #0: loss = 0.0885268 (* 1 = 0.0885268 loss)
I0728 12:30:47.056505 65336 sgd_solver.cpp:106] Iteration 20570, lr = 0.0001
I0728 12:31:31.994796 65336 solver.cpp:236] Iteration 20580, loss = 0.131742
I0728 12:31:31.995102 65336 solver.cpp:252]     Train net output #0: loss = 0.131742 (* 1 = 0.131742 loss)
I0728 12:31:31.995147 65336 sgd_solver.cpp:106] Iteration 20580, lr = 0.0001
I0728 12:32:27.367511 65336 solver.cpp:236] Iteration 20590, loss = 0.301191
I0728 12:32:27.367657 65336 solver.cpp:252]     Train net output #0: loss = 0.301191 (* 1 = 0.301191 loss)
I0728 12:32:27.367682 65336 sgd_solver.cpp:106] Iteration 20590, lr = 0.0001
I0728 12:33:18.140908 65336 solver.cpp:236] Iteration 20600, loss = 0.229044
I0728 12:33:18.141114 65336 solver.cpp:252]     Train net output #0: loss = 0.229044 (* 1 = 0.229044 loss)
I0728 12:33:18.141151 65336 sgd_solver.cpp:106] Iteration 20600, lr = 0.0001
I0728 12:34:06.284672 65336 solver.cpp:236] Iteration 20610, loss = 0.161724
I0728 12:34:06.284869 65336 solver.cpp:252]     Train net output #0: loss = 0.161724 (* 1 = 0.161724 loss)
I0728 12:34:06.284888 65336 sgd_solver.cpp:106] Iteration 20610, lr = 0.0001
I0728 12:34:54.616690 65336 solver.cpp:236] Iteration 20620, loss = 0.254939
I0728 12:34:54.616858 65336 solver.cpp:252]     Train net output #0: loss = 0.254939 (* 1 = 0.254939 loss)
I0728 12:34:54.616876 65336 sgd_solver.cpp:106] Iteration 20620, lr = 0.0001
I0728 12:35:38.328176 65336 solver.cpp:236] Iteration 20630, loss = 0.340542
I0728 12:35:38.328361 65336 solver.cpp:252]     Train net output #0: loss = 0.340542 (* 1 = 0.340542 loss)
I0728 12:35:38.328380 65336 sgd_solver.cpp:106] Iteration 20630, lr = 0.0001
I0728 12:36:29.630565 65336 solver.cpp:236] Iteration 20640, loss = 0.0199045
I0728 12:36:29.630780 65336 solver.cpp:252]     Train net output #0: loss = 0.0199046 (* 1 = 0.0199046 loss)
I0728 12:36:29.630801 65336 sgd_solver.cpp:106] Iteration 20640, lr = 0.0001
I0728 12:37:32.869706 65336 solver.cpp:236] Iteration 20650, loss = 0.0730803
I0728 12:37:32.869868 65336 solver.cpp:252]     Train net output #0: loss = 0.0730804 (* 1 = 0.0730804 loss)
I0728 12:37:32.869886 65336 sgd_solver.cpp:106] Iteration 20650, lr = 0.0001
I0728 12:38:29.187330 65336 solver.cpp:236] Iteration 20660, loss = 0.207095
I0728 12:38:29.188591 65336 solver.cpp:252]     Train net output #0: loss = 0.207095 (* 1 = 0.207095 loss)
I0728 12:38:29.188617 65336 sgd_solver.cpp:106] Iteration 20660, lr = 0.0001
I0728 12:39:22.695688 65336 solver.cpp:236] Iteration 20670, loss = 0.370665
I0728 12:39:22.695835 65336 solver.cpp:252]     Train net output #0: loss = 0.370665 (* 1 = 0.370665 loss)
I0728 12:39:22.695854 65336 sgd_solver.cpp:106] Iteration 20670, lr = 0.0001
I0728 12:40:14.961098 65336 solver.cpp:236] Iteration 20680, loss = 0.230524
I0728 12:40:14.961244 65336 solver.cpp:252]     Train net output #0: loss = 0.230524 (* 1 = 0.230524 loss)
I0728 12:40:14.961263 65336 sgd_solver.cpp:106] Iteration 20680, lr = 0.0001
I0728 12:41:01.929613 65336 solver.cpp:236] Iteration 20690, loss = 0.143802
I0728 12:41:01.929765 65336 solver.cpp:252]     Train net output #0: loss = 0.143802 (* 1 = 0.143802 loss)
I0728 12:41:01.929783 65336 sgd_solver.cpp:106] Iteration 20690, lr = 0.0001
I0728 12:41:38.619283 65336 solver.cpp:236] Iteration 20700, loss = 0.0765248
I0728 12:41:38.619488 65336 solver.cpp:252]     Train net output #0: loss = 0.0765249 (* 1 = 0.0765249 loss)
I0728 12:41:38.619519 65336 sgd_solver.cpp:106] Iteration 20700, lr = 0.0001
I0728 12:42:34.404639 65336 solver.cpp:236] Iteration 20710, loss = 0.141202
I0728 12:42:34.404878 65336 solver.cpp:252]     Train net output #0: loss = 0.141202 (* 1 = 0.141202 loss)
I0728 12:42:34.404897 65336 sgd_solver.cpp:106] Iteration 20710, lr = 0.0001
I0728 12:43:31.326730 65336 solver.cpp:236] Iteration 20720, loss = 0.0181882
I0728 12:43:31.326939 65336 solver.cpp:252]     Train net output #0: loss = 0.0181884 (* 1 = 0.0181884 loss)
I0728 12:43:31.326959 65336 sgd_solver.cpp:106] Iteration 20720, lr = 0.0001
I0728 12:44:38.687541 65336 solver.cpp:236] Iteration 20730, loss = 0.303669
I0728 12:44:38.687707 65336 solver.cpp:252]     Train net output #0: loss = 0.303669 (* 1 = 0.303669 loss)
I0728 12:44:38.687724 65336 sgd_solver.cpp:106] Iteration 20730, lr = 0.0001
I0728 12:45:29.322677 65336 solver.cpp:236] Iteration 20740, loss = 0.100532
I0728 12:45:29.322882 65336 solver.cpp:252]     Train net output #0: loss = 0.100532 (* 1 = 0.100532 loss)
I0728 12:45:29.322902 65336 sgd_solver.cpp:106] Iteration 20740, lr = 0.0001
I0728 12:46:20.593308 65336 solver.cpp:236] Iteration 20750, loss = 0.0903661
I0728 12:46:20.593461 65336 solver.cpp:252]     Train net output #0: loss = 0.0903663 (* 1 = 0.0903663 loss)
I0728 12:46:20.593479 65336 sgd_solver.cpp:106] Iteration 20750, lr = 0.0001
I0728 12:47:09.234174 65336 solver.cpp:236] Iteration 20760, loss = 0.22607
I0728 12:47:09.234330 65336 solver.cpp:252]     Train net output #0: loss = 0.22607 (* 1 = 0.22607 loss)
I0728 12:47:09.234352 65336 sgd_solver.cpp:106] Iteration 20760, lr = 0.0001
I0728 12:48:20.044209 65336 solver.cpp:236] Iteration 20770, loss = 0.152721
I0728 12:48:20.044355 65336 solver.cpp:252]     Train net output #0: loss = 0.152722 (* 1 = 0.152722 loss)
I0728 12:48:20.044378 65336 sgd_solver.cpp:106] Iteration 20770, lr = 0.0001
I0728 12:48:29.648748 65336 solver.cpp:461] Snapshotting to binary proto file models/gnet_iter_20773.caffemodel
I0728 12:48:29.844172 65336 sgd_solver.cpp:269] Snapshotting solver state to binary proto file models/gnet_iter_20773.solverstate
I0728 12:48:29.848965 65336 solver.cpp:308] Optimization stopped early.
I0728 12:48:29.849014 65336 caffe.cpp:215] Optimization Done.
